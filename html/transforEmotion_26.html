<div class="container">

<table style="width: 100%;"><tr>
<td>transformer_scores</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Sentiment Analysis Scores</h2>

<h3>Description</h3>

<p>Uses sentiment analysis pipelines from <a href="https://huggingface.co">huggingface</a>
to compute probabilities that the text corresponds to the specified classes
</p>


<h3>Usage</h3>

<pre><code class="language-R">transformer_scores(
  text,
  classes,
  multiple_classes = FALSE,
  transformer = c("cross-encoder-roberta", "cross-encoder-distilroberta",
    "facebook-bart"),
  device = c("auto", "cpu", "cuda"),
  preprocess = FALSE,
  keep_in_env = TRUE,
  envir = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>text</code></td>
<td>
<p>Character vector or list.
Text in a vector or list data format</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>classes</code></td>
<td>
<p>Character vector.
Classes to score the text</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>multiple_classes</code></td>
<td>
<p>Boolean.
Whether the text can belong to multiple true classes.
Defaults to <code>FALSE</code>.
Set to <code>TRUE</code> to get scores with multiple classes</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>transformer</code></td>
<td>
<p>Character.
Specific zero-shot sentiment analysis transformer
to be used. Default options:
</p>

<dl>
<dt><code>"cross-encoder-roberta"</code></dt>
<dd>
<p>Uses <a href="https://huggingface.co/cross-encoder/nli-roberta-base">Cross-Encoder's Natural Language Interface RoBERTa Base</a>
zero-shot classification model trained on the
<a href="https://nlp.stanford.edu/projects/snli/">Stanford Natural Language Inference</a>
(SNLI) corpus and
<a href="https://huggingface.co/datasets/multi_nli">MultiNLI</a> datasets</p>
</dd>
<dt><code>"cross-encoder-distilroberta"</code></dt>
<dd>
<p>Uses <a href="https://huggingface.co/cross-encoder/nli-distilroberta-base">Cross-Encoder's Natural Language Interface DistilRoBERTa Base</a>
zero-shot classification model trained on the
<a href="https://nlp.stanford.edu/projects/snli/">Stanford Natural Language Inference</a>
(SNLI) corpus and
<a href="https://huggingface.co/datasets/multi_nli">MultiNLI</a> datasets. The DistilRoBERTa
is intended to be a smaller, more lightweight version of <code>"cross-encoder-roberta"</code>,
that sacrifices some accuracy for much faster speed (see
<a href="https://www.sbert.net/docs/cross_encoder/pretrained_models.html#nli">https://www.sbert.net/docs/cross_encoder/pretrained_models.html#nli</a>)</p>
</dd>
<dt><code>"facebook-bart"</code></dt>
<dd>
<p>Uses <a href="https://huggingface.co/facebook/bart-large-mnli">Facebook's BART Large</a>
zero-shot classification model trained on the
<a href="https://huggingface.co/datasets/multi_nli">Multi-Genre Natural Language
Inference</a> (MultiNLI) dataset</p>
</dd>
</dl>
<p>Defaults to <code>"cross-encoder-distilroberta"</code>
</p>
<p>Also allows any zero-shot classification models with a pipeline
from <a href="https://huggingface.co/models?pipeline_tag=zero-shot-classification">huggingface</a>
to be used by using the specified name (e.g., <code>"typeform/distilbert-base-uncased-mnli"</code>; see Examples)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>device</code></td>
<td>
<p>Character.
Whether to use CPU or GPU for inference.
Defaults to <code>"auto"</code> which will use
GPU over CPU (if CUDA-capable GPU is setup).
Set to <code>"cpu"</code> to perform over CPU</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>preprocess</code></td>
<td>
<p>Boolean.
Should basic preprocessing be applied?
Includes making lowercase, keeping only alphanumeric characters,
removing escape characters, removing repeated characters,
and removing white space.
Defaults to <code>FALSE</code>.
Transformers generally are OK without preprocessing and handle
many of these functions internally, so setting to <code>TRUE</code>
will not change performance much</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keep_in_env</code></td>
<td>
<p>Boolean.
Whether the classifier should be kept in your global environment.
Defaults to <code>TRUE</code>.
By keeping the classifier in your environment, you can skip
re-loading the classifier every time you run this function.
<code>TRUE</code> is recommended</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>envir</code></td>
<td>
<p>Numeric.
Environment for the classifier to be saved for repeated use.
Defaults to the global environment</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Returns probabilities for the text classes
</p>


<h3>Author(s)</h3>

<p>Alexander P. Christensen &lt;alexpaulchristensen@gmail.com&gt;
</p>


<h3>References</h3>

<p># BART <br>
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ... &amp; Zettlemoyer, L. (2019).
Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.
<em>arXiv preprint arXiv:1910.13461</em>.
</p>
<p># RoBERTa <br>
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... &amp; Stoyanov, V. (2019).
Roberta: A robustly optimized bert pretraining approach.
<em>arXiv preprint arXiv:1907.11692</em>.
</p>
<p># Zero-shot classification <br>
Yin, W., Hay, J., &amp; Roth, D. (2019).
Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach.
<em>arXiv preprint arXiv:1909.00161</em>.
</p>
<p># MultiNLI dataset <br>
Williams, A., Nangia, N., &amp; Bowman, S. R. (2017).
A broad-coverage challenge corpus for sentence understanding through inference.
<em>arXiv preprint arXiv:1704.05426</em>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Load data
data(neo_ipip_extraversion)

# Example text
text &lt;- neo_ipip_extraversion$friendliness[1:5]

## Not run: 
# Cross-Encoder DistilRoBERTa
transformer_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 )
)

# Facebook BART Large
transformer_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 transformer = "facebook-bart"
)

# Directly from huggingface: typeform/distilbert-base-uncased-mnli
transformer_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 transformer = "typeform/distilbert-base-uncased-mnli"
)

## End(Not run)

</code></pre>


</div>