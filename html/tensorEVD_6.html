<div class="container">

<table style="width: 100%;"><tr>
<td>Tensor EVD</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Tensor EVD</h2>

<h3>Description</h3>

<p>Fast eigen value decomposition (EVD) of the Hadamard product of two matrices
</p>


<h3>Usage</h3>

<pre><code class="language-R">tensorEVD(K1, K2, ID1, ID2, alpha = 1.0,
          EVD1 = NULL, EVD2 = NULL,
          d.min = .Machine$double.eps, 
          make.dimnames = FALSE, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>K1, K2</code></td>
<td>
<p>(numeric) Covariance structure matrices</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ID1</code></td>
<td>
<p>(character/integer) Vector of length <i>n</i> with either names or indices mapping from rows/columns of <code>K1</code> into the resulting tensor product</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ID2</code></td>
<td>
<p>(character/integer) Vector of length <i>n</i> with either names or indices mapping from rows/columns of <code>K2</code> into the resulting tensor product</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>(numeric) Proportion of variance of the tensor product to be explained by the tensor eigenvectors</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>EVD1</code></td>
<td>
<p>(list) (Optional) Eigenvectors and eigenvalues of <code>K1</code> as produced by the <code>eigen</code> function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>EVD2</code></td>
<td>
<p>(list) (Optional) Eigenvectors and eigenvalues of <code>K2</code> as produced by the <code>eigen</code> function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>d.min</code></td>
<td>
<p>(numeric) Tensor eigenvalue threshold. Default is a numeric zero. Only eigenvectors with eigenvalue passing this threshold are returned</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>make.dimnames</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> to whether add <code>rownames</code> and <code>colnames</code> attributes to the output</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p><code>TRUE</code> or <code>FALSE</code> to whether show progress</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Let the <i>n</i> × <i>n</i> matrix <b>K</b> to be the Hadamard product (aka element-wise or entry-wise product) involving two smaller matrices 
<b>K</b><sub>1</sub> and
<b>K</b><sub>2</sub> of dimensions 
<i>n</i><sub>1</sub> and <i>n</i><sub>2</sub>, respectively,
</p>
<p style="text-align:center"><b>K</b> = (<b>Z</b><sub>1</sub><b>K</b><sub>1</sub><b>Z'</b><sub>1</sub>) &amp;odot; (<b>Z</b><sub>2</sub><b>K</b><sub>2</sub><b>Z'</b><sub>2</sub>)</p>
<p>where
<b>Z</b><sub>1</sub> and
<b>Z</b><sub>2</sub> are incidence matrices mapping from rows (and columns) of the resulting Hadamard to rows (and columns) of <b>K</b><sub>1</sub> and
<b>K</b><sub>2</sub>, respectively. 
</p>
<p>Let the eigenvalue decomposition (EVD) of <b>K</b><sub>1</sub> and
<b>K</b><sub>2</sub> to be
<b>K</b><sub>1</sub> = <b>V</b><sub>1</sub><b>D</b><sub>1</sub><b>V'</b><sub>1</sub> and 
<b>K</b><sub>2</sub> = <b>V</b><sub>2</sub><b>D</b><sub>2</sub><b>V'</b><sub>2</sub>. 
Using properties of the Hadamard and Kronecker products, an EVD of the Hadamard product
<b>K</b> can be approximated using the EVD of 
<b>K</b><sub>1</sub> and 
<b>K</b><sub>2</sub> as
</p>
<p style="text-align:center"><b>K = V D V'</b></p>
<p>where <b>D</b> = <b>D</b><sub>1</sub>⊗<b>D</b><sub>2</sub> is a diagonal matrix containing
<i>N</i> = <i>n</i><sub>1</sub> × <i>n</i><sub>2</sub> tensor eigenvalues 
d<sub>1</sub> ≥ ... ≥ d<sub>N</sub> ≥ 0 and
<b>V</b> = (<b>Z</b><sub>1</sub>&amp;Star;<b>Z</b><sub>2</sub>)(<b>V</b><sub>1</sub>⊗<b>V</b><sub>2</sub>) = [<b>v</b><sub>1</sub>,...,<b>v</b><sub>N</sub>] is matrix containing <i>N</i> tensor eigenvectors
<b>v</b><sub>k</sub>; here the term 
<b>Z</b><sub>1</sub>&amp;Star;<b>Z</b><sub>2</sub> is the 
"face-splitting product" (aka "transposed Khatri–Rao product") of matrices 
<b>Z</b><sub>1</sub> and
<b>Z</b><sub>2</sub>.
</p>
<p>Each tensor eigenvector <i>k</i> is derived separately as a Hadamard product using the corresponding 
<i>i(k)</i> and <i>j(k)</i> eigenvectors 
<b>v</b><sub>1i(k)</sub> and
<b>v</b><sub>2j(k)</sub> from 
<b>V</b><sub>1</sub> and 
<b>V</b><sub>2</sub>, respectively, this is
</p>
<p style="text-align:center"><b>v</b><sub>k</sub> = (<b>Z</b><sub>1</sub><b>v</b><sub>1i(k)</sub>)&amp;odot;(<b>Z</b><sub>2</sub><b>v</b><sub>2j(k)</sub>)</p>
<p>The <code>tensorEVD</code> function derives each of these eigenvectors <b>v</b><sub>k</sub> by matrix indexing using integer vectors <code>ID1</code> and <code>ID2</code>. The entries of these vectors are the row (and column) number of 
<b>K</b><sub>1</sub> and 
<b>K</b><sub>2</sub> that are mapped at each row of <b>Z</b><sub>1</sub> and
<b>Z</b><sub>2</sub>, respectively.
</p>


<h3>Value</h3>

<p>Returns a list object that contains the elements:
</p>

<ul>
<li> <p><code>values</code>: (vector) resulting tensor eigenvalues.
</p>
</li>
<li> <p><code>vectors</code>: (matrix) resulting tensor eigenvectors.
</p>
</li>
<li> <p><code>totalVar</code>: (numeric) total variance of the tensor matrix product.
</p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R">  require(tensorEVD)
  set.seed(195021)
  
  # Generate matrices K1 and K2 of dimensions n1 and n2
  n1 = 10; n2 = 15
  K1 = crossprod(matrix(rnorm(n1*(n1+10)), ncol=n1))
  K2 = crossprod(matrix(rnorm(n2*(n2+10)), ncol=n2))

  # (a) Example 1. Full design (Kronecker product)
  ID1 = rep(seq(n1), each=n2)
  ID2 = rep(seq(n2), times=n1)

  # Direct EVD of the Hadamard product
  K = K1[ID1,ID1]*K2[ID2,ID2]
  EVD0 = eigen(K)

  # Tensor EVD using K1 and K2
  EVD = tensorEVD(K1, K2, ID1, ID2)

  # Eigenvectors and eigenvalues are numerically equal
  all.equal(EVD0$values, EVD$values)
  all.equal(abs(EVD0$vectors), abs(EVD$vectors)) 

  # (b) If a proportion of variance explained is specified, 
  # only the eigenvectors needed to explain such proportion are derived
  alpha = 0.95
  EVD = tensorEVD(K1, K2, ID1, ID2, alpha=alpha)
  dim(EVD$vectors)

  # For the direct EVD
  varexp = cumsum(EVD0$values/sum(EVD0$values))
  index = 1:which.min(abs(varexp-alpha))
  dim(EVD0$vectors[,index])

  # (c) Example 2. Incomplete design (Hadamard product)
  # Eigenvectors and eigenvalues are no longer equivalent
  n = n1*n2   # Sample size n
  ID1 = sample(seq(n1), n, replace=TRUE) # Randomly sample of ID1
  ID2 = sample(seq(n2), n, replace=TRUE) # Randomly sample of ID2

  K = K1[ID1,ID1]*K2[ID2,ID2]
  EVD0 = eigen(K)
  EVD = tensorEVD(K1, K2, ID1, ID2)

  all.equal(EVD0$values, EVD$values)
  all.equal(abs(EVD0$vectors), abs(EVD$vectors)) 
  
  # However, the sum of the eigenvalues is equal to the trace(K)
  c(sum(EVD0$values), sum(EVD$values), sum(diag(K)))

  # And provide the same approximation for K
  K01 = EVD0$vectors%*%diag(EVD0$values)%*%t(EVD0$vectors)
  K02 = EVD$vectors%*%diag(EVD$values)%*%t(EVD$vectors)
  c(all.equal(K,K01), all.equal(K,K02))

  # When n is different from N=n1xn2, both methods provide different 
  # number or eigenvectors/eigenvalues. The eigen function provides 
  # a number of eigenvectors equal to the minimum between n and N
  # for the tensorEVD, this number is always N

  # (d) Sample size n being half of n1 x n2
  n = n1*n2/2
  ID1 = sample(seq(n1), n, replace=TRUE)
  ID2 = sample(seq(n2), n, replace=TRUE)

  K = K1[ID1,ID1]*K2[ID2,ID2]
  EVD0 = eigen(K)
  EVD = tensorEVD(K1, K2, ID1, ID2)

  c(eigen=sum(EVD0$values&gt;1E-10), tensorEVD=sum(EVD$values&gt;1E-10))

  # (e) Sample size n being twice n1 x n2
  n = n1*n2*2
  ID1 = sample(seq(n1), n, replace=TRUE)
  ID2 = sample(seq(n2), n, replace=TRUE)

  K = K1[ID1,ID1]*K2[ID2,ID2]
  EVD0 = eigen(K)
  EVD = tensorEVD(K1, K2, ID1, ID2)

  c(eigen=sum(EVD0$values&gt;1E-10), tensorEVD=sum(EVD$values&gt;1E-10))
</code></pre>


</div>