<div class="container">

<table style="width: 100%;"><tr>
<td>nn_silu</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Applies the Sigmoid Linear Unit (SiLU) function, element-wise.
The SiLU function is also known as the swish function.</h2>

<h3>Description</h3>

<p>Applies the Sigmoid Linear Unit (SiLU) function, element-wise.
The SiLU function is also known as the swish function.
</p>


<h3>Usage</h3>

<pre><code class="language-R">nn_silu(inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: <code>FALSE</code></p>
</td>
</tr></table>
<h3>Details</h3>

<p>See <a href="https://arxiv.org/abs/1606.08415">Gaussian Error Linear Units (GELUs)</a>
where the SiLU (Sigmoid Linear Unit) was originally coined, and see
<a href="https://arxiv.org/abs/1702.03118">Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning</a>
and <a href="https://arxiv.org/abs/1710.05941v1">Swish: a Self-Gated Activation Function</a>
where the SiLU was experimented with later.
</p>


</div>