<div class="container">

<table style="width: 100%;"><tr>
<td>nn_rrelu</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>RReLU module</h2>

<h3>Description</h3>

<p>Applies the randomized leaky rectified liner unit function, element-wise,
as described in the paper:
</p>


<h3>Usage</h3>

<pre><code class="language-R">nn_rrelu(lower = 1/8, upper = 1/3, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>lower</code></td>
<td>
<p>lower bound of the uniform distribution. Default: <code class="reqn">\frac{1}{8}</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>upper</code></td>
<td>
<p>upper bound of the uniform distribution. Default: <code class="reqn">\frac{1}{3}</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>inplace</code></td>
<td>
<p>can optionally do the operation in-place. Default: <code>FALSE</code></p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code style="white-space: pre;">⁠Empirical Evaluation of Rectified Activations in Convolutional Network⁠</code>.
</p>
<p>The function is defined as:
</p>
<p style="text-align: center;"><code class="reqn">
\mbox{RReLU}(x) =
\left\{ \begin{array}{ll}
x &amp; \mbox{if } x \geq 0 \\
ax &amp; \mbox{ otherwise }
\end{array}
\right.
</code>
</p>

<p>where <code class="reqn">a</code> is randomly sampled from uniform distribution
<code class="reqn">\mathcal{U}(\mbox{lower}, \mbox{upper})</code>.
See: https://arxiv.org/pdf/1505.00853.pdf
</p>


<h3>Shape</h3>


<ul>
<li>
<p> Input: <code class="reqn">(N, *)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li>
<p> Output: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R">if (torch_is_installed()) {
m &lt;- nn_rrelu(0.1, 0.3)
input &lt;- torch_randn(2)
m(input)
}
</code></pre>


</div>