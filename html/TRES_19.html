<div class="container">

<table style="width: 100%;"><tr>
<td>square</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Square simulated data</h2>

<h3>Description</h3>

<p>Synthetic data generated from tensor predictor regression (TPR) model. Each response observation is univariate, and each predictor observation is a matrix.
</p>


<h3>Usage</h3>

<pre><code class="language-R">data("square")
</code></pre>


<h3>Format</h3>

<p>A list consisting of four components:
</p>

<dl>
<dt>x</dt>
<dd>
<p>A <code class="reqn">32 \times 32 \times 200</code> tensor, each matrix <code>x@data[,,i]</code> represents a predictor observation.</p>
</dd>
<dt>y</dt>
<dd>
<p>A <code class="reqn">1 \times 200</code> matrix, each entry represents a response observation.</p>
</dd>
<dt>coefficients</dt>
<dd>
<p>A <code class="reqn">32\times 32 \times 1</code> tensor with a square pattern.</p>
</dd>
<dt>Gamma</dt>
<dd>
<p>A list consisting of two <code class="reqn">32 \times 2</code> envelope basis.</p>
</dd>
</dl>
<h3>Details</h3>

<p>The dataset is generated from the tensor predictor regression (TPR) model:
</p>
<p style="text-align: center;"><code class="reqn">Y_i = B_{(m+1)}vec(X_i) + \epsilon_i, \quad i = 1,\ldots, n,</code>
</p>

<p>where <code class="reqn">n=200</code> and the regression coefficient <code class="reqn">B \in R^{32\times 32}</code> is a given image with rank 2, which has a square pattern. All the elements of the coefficient matrix <code class="reqn">B</code> are either 0.1 or 1. To make the model conform to the envelope structure, we construct the envelope basis <code class="reqn">\Gamma_k</code> and the covariance matrices <code class="reqn">\Sigma_k, k=1,2</code>, of predictor <code class="reqn">X</code> as following. With the singular value decomposition of <code class="reqn">B</code>, namely <code class="reqn">B = \Gamma_1 \Lambda \Gamma_2^T</code>, we choose the envelope basis as <code class="reqn">\Gamma_k \in R^{32 \times 2}, k=1,2</code>. Then the envelope dimensions are <code class="reqn">u_1 =  u_2 = 2</code>. We set matrices <code class="reqn">\Omega_k = I_2</code>  and  <code class="reqn">\Omega_{0k} = 0.01 I_{30}</code>, <code class="reqn">k=1,2</code>. Then we generate the covariance matrices <code class="reqn">\Sigma_k = \Gamma_k \Omega_k \Gamma_k^T + \Gamma_{0k}\Omega_{0k}\Gamma_{0k}^T</code>, followed by normalization with their Frobenius norms. The predictor <code class="reqn">X_i</code> is then generated from two-way tensor (matrix) normal distribution <code class="reqn">TN(0; \Sigma_1, \Sigma_2)</code>. And the error term <code class="reqn">\epsilon_i</code> is generated from standard normal distribution.
</p>


<h3>References</h3>

<p>Zhang, X. and Li, L., 2017. Tensor envelope partial least-squares regression. Technometrics, 59(4), pp.426-436.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Fit square dataset with the tensor predictor regression model
data("square")
x &lt;- square$x
y &lt;- square$y
# Model fitting with ordinary least square.
fit_std &lt;- TPR.fit(x, y, method="standard")
# Draw the coefficient plot.
plot(fit_std)

</code></pre>


</div>