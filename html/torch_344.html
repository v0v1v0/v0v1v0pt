<div class="container">

<table style="width: 100%;"><tr>
<td>optim_adagrad</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Adagrad optimizer</h2>

<h3>Description</h3>

<p>Proposed in <a href="https://jmlr.org/papers/v12/duchi11a.html">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a>
</p>


<h3>Usage</h3>

<pre><code class="language-R">optim_adagrad(
  params,
  lr = 0.01,
  lr_decay = 0,
  weight_decay = 0,
  initial_accumulator_value = 0,
  eps = 1e-10
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>params</code></td>
<td>
<p>(iterable): list of parameters to optimize or list parameter groups</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lr</code></td>
<td>
<p>(float, optional): learning rate (default: 1e-2)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lr_decay</code></td>
<td>
<p>(float, optional): learning rate decay (default: 0)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight_decay</code></td>
<td>
<p>(float, optional): weight decay (L2 penalty) (default: 0)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initial_accumulator_value</code></td>
<td>
<p>the initial value for the accumulator. (default: 0)
</p>
<p>Adagrad is an especially good optimizer for sparse data.
It individually modifies learning rate for every single parameter,
dividing the original learning rate value by sum of the squares of the gradients.
It causes that the rarely occurring features get greater learning rates.
The main downside of this method is the fact that learning rate may be
getting small too fast, so that at some point a model cannot learn anymore.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>(float, optional): term added to the denominator to improve
numerical stability (default: 1e-10)</p>
</td>
</tr>
</table>
<h3>Warning</h3>

<p>If you need to move a model to GPU via <code style="white-space: pre;">⁠$cuda()⁠</code>, please do so before
constructing optimizers for it. Parameters of a model after <code style="white-space: pre;">⁠$cuda()⁠</code>
will be different objects from those before the call. In general, you
should make sure that the objects pointed to by model parameters subject
to optimization remain the same over the whole lifecycle of optimizer
creation and usage.
</p>


<h3>Note</h3>

<p>Update rule:
</p>
<p style="text-align: center;"><code class="reqn">
\theta_{t+1} = \theta_{t} - \frac{\eta }{\sqrt{G_{t} + \epsilon}} \odot g_{t}
</code>
</p>

<p>The equation above and some remarks quoted
after <a href="https://web.archive.org/web/20220810011734/https://ruder.io/optimizing-gradient-descent/index.html"><em>An overview of gradient descent optimization algorithms</em></a>
by Sebastian Ruder.
</p>


</div>