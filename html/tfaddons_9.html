<div class="container">

<table style="width: 100%;"><tr>
<td>attention_bahdanau</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Bahdanau Attention</h2>

<h3>Description</h3>

<p>Implements Bahdanau-style (additive) attention
</p>


<h3>Usage</h3>

<pre><code class="language-R">attention_bahdanau(
  object,
  units,
  memory = NULL,
  memory_sequence_length = NULL,
  normalize = FALSE,
  probability_fn = "softmax",
  kernel_initializer = "glorot_uniform",
  dtype = NULL,
  name = "BahdanauAttention",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Model or layer object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>units</code></td>
<td>
<p>The depth of the query mechanism.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>memory</code></td>
<td>
<p>The memory to query; usually the output of an RNN encoder. This tensor
should be shaped [batch_size, max_time, ...].</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>memory_sequence_length</code></td>
<td>
<p>(optional): Sequence lengths for the batch entries in
memory. If provided, the memory tensor rows are masked with zeros for values past the
respective sequence lengths.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>normalize</code></td>
<td>
<p>boolean. Whether to normalize the energy term.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>probability_fn</code></td>
<td>
<p>(optional) string, the name of function to convert the attention
score to probabilities. The default is softmax which is tf.nn.softmax. Other options is hardmax,
which is hardmax() within this module. Any other value will result into validation
error. Default to use softmax.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_initializer</code></td>
<td>
<p>(optional), the name of the initializer for the attention kernel.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dtype</code></td>
<td>
<p>The data type for the query and memory layers of the attention mechanism.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>name</code></td>
<td>
<p>Name to use when creating ops.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>A list that contains other common arguments for layer creation.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This attention has two forms. The first is Bahdanau attention, as described in:
Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. "Neural Machine Translation by Jointly
Learning to Align and Translate." ICLR 2015. https://arxiv.org/abs/1409.0473 The second
is the normalized form. This form is inspired by the weight normalization article:
Tim Salimans, Diederik P. Kingma. "Weight Normalization: A Simple Reparameterization
to Accelerate Training of Deep Neural Networks." https://arxiv.org/abs/1602.07868
To enable the second form, construct the object with parameter 'normalize=TRUE'.
</p>


<h3>Value</h3>

<p>None
</p>


</div>