<div class="container">

<table style="width: 100%;"><tr>
<td>torch_amin</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Amin</h2>

<h3>Description</h3>

<p>Amin
</p>


<h3>Usage</h3>

<pre><code class="language-R">torch_amin(self, dim = list(), keepdim = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>self</code></td>
<td>
<p>(Tensor) the input tensor.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dim</code></td>
<td>
<p>(int or tuple of ints) the dimension or dimensions to reduce.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keepdim</code></td>
<td>
<p>(bool) whether the output tensor has <code>dim</code> retained or not.</p>
</td>
</tr>
</table>
<h3>amin(input, dim, keepdim=FALSE, *, out=None) -&gt; Tensor </h3>

<p>Returns the minimum value of each slice of the <code>input</code> tensor in the given
dimension(s) <code>dim</code>.
</p>


<h3>Note</h3>

<p>The difference between <code>max</code>/<code>min</code> and <code>amax</code>/<code>amin</code> is:
</p>

<ul>
<li> <p><code>amax</code>/<code>amin</code> supports reducing on multiple dimensions,
</p>
</li>
<li> <p><code>amax</code>/<code>amin</code> does not return indices,
</p>
</li>
<li> <p><code>amax</code>/<code>amin</code> evenly distributes gradient between equal values,
while <code>max(dim)</code>/<code>min(dim)</code> propagates gradient only to a single
index in the source tensor.
</p>
</li>
</ul>
<p>If <code>keepdim</code> is <code>TRUE</code>, the output tensors are of the same size as
<code>input</code> except in the dimension(s) <code>dim</code> where they are of size 1.
Otherwise, <code>dim</code>s are squeezed (see <code>torch_squeeze()</code>), resulting in
the output tensors having fewer dimensions than <code>input</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">if (torch_is_installed()) {

a &lt;- torch_randn(c(4, 4))
a
torch_amin(a, 1)
}
</code></pre>


</div>