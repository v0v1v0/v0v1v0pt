<div class="container">

<table style="width: 100%;"><tr>
<td>nn_softmax</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Softmax module</h2>

<h3>Description</h3>

<p>Applies the Softmax function to an n-dimensional input Tensor
rescaling them so that the elements of the n-dimensional output Tensor
lie in the range <code style="white-space: pre;">⁠[0,1]⁠</code> and sum to 1.
Softmax is defined as:
</p>


<h3>Usage</h3>

<pre><code class="language-R">nn_softmax(dim)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>dim</code></td>
<td>
<p>(int): A dimension along which Softmax will be computed (so every slice
along dim will sum to 1).</p>
</td>
</tr></table>
<h3>Details</h3>

<p style="text-align: center;"><code class="reqn">
  \mbox{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
</code>
</p>

<p>When the input Tensor is a sparse tensor then the unspecifed
values are treated as <code>-Inf</code>.
</p>


<h3>Value</h3>

<p>:
a Tensor of the same dimension and shape as the input with
values in the range <code style="white-space: pre;">⁠[0, 1]⁠</code>
</p>


<h3>Shape</h3>


<ul>
<li>
<p> Input: <code class="reqn">(*)</code> where <code>*</code> means, any number of additional
dimensions
</p>
</li>
<li>
<p> Output: <code class="reqn">(*)</code>, same shape as the input
</p>
</li>
</ul>
<h3>Note</h3>

<p>This module doesn't work directly with NLLLoss,
which expects the Log to be computed between the Softmax and itself.
Use <code>LogSoftmax</code> instead (it's faster and has better numerical properties).
</p>


<h3>Examples</h3>

<pre><code class="language-R">if (torch_is_installed()) {
m &lt;- nn_softmax(1)
input &lt;- torch_randn(2, 3)
output &lt;- m(input)
}
</code></pre>


</div>