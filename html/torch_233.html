<div class="container">

<table style="width: 100%;"><tr>
<td>nn_bce_with_logits_loss</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>BCE with logits loss</h2>

<h3>Description</h3>

<p>This loss combines a <code>Sigmoid</code> layer and the <code>BCELoss</code> in one single
class. This version is more numerically stable than using a plain <code>Sigmoid</code>
followed by a <code>BCELoss</code> as, by combining the operations into one layer,
we take advantage of the log-sum-exp trick for numerical stability.
</p>


<h3>Usage</h3>

<pre><code class="language-R">nn_bce_with_logits_loss(weight = NULL, reduction = "mean", pos_weight = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>weight</code></td>
<td>
<p>(Tensor, optional): a manual rescaling weight given to the loss
of each batch element. If given, has to be a Tensor of size <code>nbatch</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pos_weight</code></td>
<td>
<p>(Tensor, optional): a weight of positive examples.
Must be a vector with length equal to the number of classes.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The unreduced (i.e. with <code>reduction</code> set to <code>'none'</code>) loss can be described as:
</p>
<p style="text-align: center;"><code class="reqn">
  \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_n \left[ y_n \cdot \log \sigma(x_n)
                   + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],
</code>
</p>

<p>where <code class="reqn">N</code> is the batch size. If <code>reduction</code> is not <code>'none'</code>
(default <code>'mean'</code>), then
</p>
<p style="text-align: center;"><code class="reqn">
  \ell(x, y) = \begin{array}{ll}
\mbox{mean}(L), &amp; \mbox{if reduction} = \mbox{'mean';}\\
\mbox{sum}(L),  &amp; \mbox{if reduction} = \mbox{'sum'.}
\end{array}
</code>
</p>

<p>This is used for measuring the error of a reconstruction in for example
an auto-encoder. Note that the targets <code>t[i]</code> should be numbers
between 0 and 1.
It's possible to trade off recall and precision by adding weights to positive examples.
In the case of multi-label classification the loss can be described as:
</p>
<p style="text-align: center;"><code class="reqn">
\ell_c(x, y) = L_c = \{l_{1,c},\dots,l_{N,c}\}^\top, \quad
l_{n,c} = - w_{n,c} \left[ p_c y_{n,c} \cdot \log \sigma(x_{n,c})
+ (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],
</code>
</p>

<p>where <code class="reqn">c</code> is the class number (<code class="reqn">c &gt; 1</code> for multi-label binary
classification,
</p>
<p><code class="reqn">c = 1</code> for single-label binary classification),
<code class="reqn">n</code> is the number of the sample in the batch and
<code class="reqn">p_c</code> is the weight of the positive answer for the class <code class="reqn">c</code>.
<code class="reqn">p_c &gt; 1</code> increases the recall, <code class="reqn">p_c &lt; 1</code> increases the precision.
For example, if a dataset contains 100 positive and 300 negative examples of a single class,
then <code>pos_weight</code> for the class should be equal to <code class="reqn">\frac{300}{100}=3</code>.
The loss would act as if the dataset contains <code class="reqn">3\times 100=300</code> positive examples.
</p>


<h3>Shape</h3>


<ul>
<li>
<p> Input: <code class="reqn">(N, *)</code> where <code class="reqn">*</code> means, any number of additional dimensions
</p>
</li>
<li>
<p> Target: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li>
<li>
<p> Output: scalar. If <code>reduction</code> is <code>'none'</code>, then <code class="reqn">(N, *)</code>, same
shape as input.
</p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R">if (torch_is_installed()) {
loss &lt;- nn_bce_with_logits_loss()
input &lt;- torch_randn(3, requires_grad = TRUE)
target &lt;- torch_empty(3)$random_(1, 2)
output &lt;- loss(input, target)
output$backward()

target &lt;- torch_ones(10, 64, dtype = torch_float32()) # 64 classes, batch size = 10
output &lt;- torch_full(c(10, 64), 1.5) # A prediction (logit)
pos_weight &lt;- torch_ones(64) # All weights are equal to 1
criterion &lt;- nn_bce_with_logits_loss(pos_weight = pos_weight)
criterion(output, target) # -log(sigmoid(1.5))
}
</code></pre>


</div>