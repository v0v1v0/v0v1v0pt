<div class="container">

<table style="width: 100%;"><tr>
<td>attention_bahdanau_monotonic</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Bahdanau Monotonic Attention</h2>

<h3>Description</h3>

<p>Monotonic attention mechanism with Bahadanau-style energy function.
</p>


<h3>Usage</h3>

<pre><code class="language-R">attention_bahdanau_monotonic(
  object,
  units,
  memory = NULL,
  memory_sequence_length = NULL,
  normalize = FALSE,
  sigmoid_noise = 0,
  sigmoid_noise_seed = NULL,
  score_bias_init = 0,
  mode = "parallel",
  kernel_initializer = "glorot_uniform",
  dtype = NULL,
  name = "BahdanauMonotonicAttention",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Model or layer object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>units</code></td>
<td>
<p>The depth of the query mechanism.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>memory</code></td>
<td>
<p>The memory to query; usually the output of an RNN encoder. This tensor
should be shaped [batch_size, max_time, ...].</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>memory_sequence_length</code></td>
<td>
<p>(optional): Sequence lengths for the batch entries in memory.
If provided, the memory tensor rows are masked with zeros for values past the respective
sequence lengths.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>normalize</code></td>
<td>
<p>Python boolean. Whether to normalize the energy term.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigmoid_noise</code></td>
<td>
<p>Standard deviation of pre-sigmoid noise. See the docstring for
'_monotonic_probability_fn' for more information.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigmoid_noise_seed</code></td>
<td>
<p>(optional) Random seed for pre-sigmoid noise.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>score_bias_init</code></td>
<td>
<p>Initial value for score bias scalar. It's recommended to initialize
this to a negative value when the length of the memory is large.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mode</code></td>
<td>
<p>How to compute the attention distribution. Must be one of 'recursive',
'parallel', or 'hard'. See the docstring for tfa.seq2seq.monotonic_attention for more information.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_initializer</code></td>
<td>
<p>(optional), the name of the initializer for the attention kernel.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dtype</code></td>
<td>
<p>The data type for the query and memory layers of the attention mechanism.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>name</code></td>
<td>
<p>Name to use when creating ops.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>A list that contains other common arguments for layer creation.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This type of attention enforces a monotonic constraint on the attention
distributions; that is once the model attends to a given point in the memory it
can't attend to any prior points at subsequence output timesteps. It achieves this
by using the _monotonic_probability_fn instead of softmax to construct its attention
distributions. Since the attention scores are passed through a sigmoid, a learnable
scalar bias parameter is applied after the score function and before the sigmoid.
Otherwise, it is equivalent to BahdanauAttention. This approach is proposed in
</p>
<p>Colin Raffel, Minh-Thang Luong, Peter J. Liu, Ron J. Weiss, Douglas Eck,
"Online and Linear-Time Attention by Enforcing Monotonic Alignments."
ICML 2017. https://arxiv.org/abs/1704.00784
</p>


<h3>Value</h3>

<p>None
</p>


</div>