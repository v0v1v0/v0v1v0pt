<div class="container">

<table style="width: 100%;"><tr>
<td>nn_kl_div_loss</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Kullback-Leibler divergence loss</h2>

<h3>Description</h3>

<p>The Kullback-Leibler divergence loss measure
<a href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence</a>
is a useful distance measure for continuous distributions and is often
useful when performing direct regression over the space of (discretely sampled)
continuous output distributions.
</p>


<h3>Usage</h3>

<pre><code class="language-R">nn_kl_div_loss(reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'batchmean'</code> | <code>'sum'</code> | <code>'mean'</code>.
<code>'none'</code>: no reduction will be applied.
<code>'batchmean'</code>: the sum of the output will be divided by batchsize.
<code>'sum'</code>: the output will be summed.
<code>'mean'</code>: the output will be divided by the number of elements in the output.
Default: <code>'mean'</code></p>
</td>
</tr></table>
<h3>Details</h3>

<p>As with <code>nn_nll_loss()</code>, the <code>input</code> given is expected to contain
<em>log-probabilities</em> and is not restricted to a 2D Tensor.
</p>
<p>The targets are interpreted as <em>probabilities</em> by default, but could be considered
as <em>log-probabilities</em> with <code>log_target</code> set to <code>TRUE</code>.
</p>
<p>This criterion expects a <code>target</code> <code>Tensor</code> of the same size as the
<code>input</code> <code>Tensor</code>.
</p>
<p>The unreduced (i.e. with <code>reduction</code> set to <code>'none'</code>) loss can be described
as:
</p>
<p style="text-align: center;"><code class="reqn">
  l(x,y) = L = \{ l_1,\dots,l_N \}, \quad
l_n = y_n \cdot \left( \log y_n - x_n \right)
</code>
</p>

<p>where the index <code class="reqn">N</code> spans all dimensions of <code>input</code> and <code class="reqn">L</code> has the same
shape as <code>input</code>. If <code>reduction</code> is not <code>'none'</code> (default <code>'mean'</code>), then:
</p>
<p style="text-align: center;"><code class="reqn">
  \ell(x, y) = \begin{array}{ll}
\mbox{mean}(L), &amp; \mbox{if reduction} = \mbox{'mean';} \\
\mbox{sum}(L),  &amp; \mbox{if reduction} = \mbox{'sum'.}
\end{array}
</code>
</p>

<p>In default <code>reduction</code> mode <code>'mean'</code>, the losses are averaged for each minibatch
over observations <strong>as well as</strong> over dimensions. <code>'batchmean'</code> mode gives the
correct KL divergence where losses are averaged over batch dimension only.
<code>'mean'</code> mode's behavior will be changed to the same as <code>'batchmean'</code> in the next
major release.
</p>


<h3>Shape</h3>


<ul>
<li>
<p> Input: <code class="reqn">(N, *)</code> where <code class="reqn">*</code> means, any number of additional
dimensions
</p>
</li>
<li>
<p> Target: <code class="reqn">(N, *)</code>, same shape as the input
</p>
</li>
<li>
<p> Output: scalar by default. If <code>reduction</code> is <code>'none'</code>, then <code class="reqn">(N, *)</code>,
the same shape as the input
</p>
</li>
</ul>
<h3>Note</h3>

<p><code>reduction</code> = <code>'mean'</code> doesn't return the true kl divergence value,
please use <code>reduction</code> = <code>'batchmean'</code> which aligns with KL math
definition.
In the next major release, <code>'mean'</code> will be changed to be the same as
<code>'batchmean'</code>.
</p>


</div>