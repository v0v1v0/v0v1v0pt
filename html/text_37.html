<div class="container">

<table style="width: 100%;"><tr>
<td>textQA</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Question Answering. (experimental)</h2>

<h3>Description</h3>

<p>Question Answering. (experimental)
</p>


<h3>Usage</h3>

<pre><code class="language-R">textQA(
  question,
  context,
  model = "",
  device = "cpu",
  tokenizer_parallelism = FALSE,
  logging_level = "warning",
  return_incorrect_results = FALSE,
  top_k = 1L,
  doc_stride = 128L,
  max_answer_len = 15L,
  max_seq_len = 384L,
  max_question_len = 64L,
  handle_impossible_answer = FALSE,
  set_seed = 202208L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>question</code></td>
<td>
<p>(string)  A question</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>context</code></td>
<td>
<p>(string)  The context(s) where the model will look for the answer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>(string)  HuggingFace name of a pre-trained language model that have been fine-tuned
on a question answering task.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>device</code></td>
<td>
<p>(string)  Device to use: 'cpu', 'gpu', or 'gpu:k' where k is a specific device number</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tokenizer_parallelism</code></td>
<td>
<p>(boolean)  If TRUE this will turn on tokenizer parallelism.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>logging_level</code></td>
<td>
<p>(string)  Set the logging level.
Options (ordered from less logging to more logging): critical, error, warning, info, debug</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>return_incorrect_results</code></td>
<td>
<p>(boolean)  Stop returning some incorrectly formatted/structured results.
This setting does CANOT evaluate the actual results (whether or not they make sense, exist, etc.).
All it does is to ensure the returned results are formatted correctly (e.g., does the question-answering
dictionary contain the key "answer", is sentiments from textClassify containing the labels "positive"
and "negative").</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>top_k</code></td>
<td>
<p>(integer) (int)  Indicates number of possible answer span(s) to get from the model output.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>doc_stride</code></td>
<td>
<p>(integer)   If the context is too long to fit with the question for the model, it will be split
into overlapping chunks. This setting controls the overlap size.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_answer_len</code></td>
<td>
<p>(integer)  Max answer size to be extracted from the modelâ€™s output.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_seq_len</code></td>
<td>
<p>(integer)  The max total sentence length (context + question) in tokens of each chunk
passed to the model. If needed, the context is split in chunks (using doc_stride as overlap).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_question_len</code></td>
<td>
<p>(integer)   The max question length after tokenization. It will be truncated if needed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>handle_impossible_answer</code></td>
<td>
<p>(boolean)  Whether or not impossible is accepted as an answer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>set_seed</code></td>
<td>
<p>(Integer) Set seed.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Answers.
</p>


<h3>See Also</h3>

<p>see <code>textClassify</code>, <code>textGeneration</code>, <code>textNER</code>,
<code>textSum</code>, <code>textQA</code>, <code>textTranslate</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
#   qa_examples &lt;- textQA(question = "Which colour have trees?",
#     context = "Trees typically have leaves, are mostly green and like water.")

</code></pre>


</div>