<div class="container">

<table style="width: 100%;"><tr>
<td>layer_kl_divergence_add_loss</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Pass-through layer that adds a KL divergence penalty to the model loss</h2>

<h3>Description</h3>

<p>Pass-through layer that adds a KL divergence penalty to the model loss
</p>


<h3>Usage</h3>

<pre><code class="language-R">layer_kl_divergence_add_loss(
  object,
  distribution_b,
  use_exact_kl = FALSE,
  test_points_reduce_axis = NULL,
  test_points_fn = tf$convert_to_tensor,
  weight = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li>
<p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li>
<p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li>
<p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>distribution_b</code></td>
<td>
<p>Distribution instance corresponding to b as in  <code>KL[a, b]</code>.
The previous layer's output is presumed to be a Distribution instance and is a.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_exact_kl</code></td>
<td>
<p>Logical indicating if KL divergence should be
calculated exactly via <code>tfp$distributions$kl_divergence</code> or via Monte Carlo approximation.
Default value: FALSE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>test_points_reduce_axis</code></td>
<td>
<p>Integer vector or scalar representing dimensions
over which to reduce_mean while calculating the Monte Carlo approximation of the KL divergence.
As is with all tf$reduce_* ops, NULL means reduce over all dimensions;
() means reduce over none of them. Default value: () (i.e., no reduction).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>test_points_fn</code></td>
<td>
<p>A callable taking a <code>tfp$distributions$Distribution</code> instance and returning a tensor
used for random test points to approximate the KL divergence.
Default value: tf$convert_to_tensor.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight</code></td>
<td>
<p>Multiplier applied to the calculated KL divergence for each Keras batch member.
Default value: NULL (i.e., do not weight each batch member).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments passed to <code>args</code> of <code>keras::create_layer</code>.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>For an example how to use in a Keras model, see <code>layer_independent_normal()</code>.
</p>
<p>Other distribution_layers: 
<code>layer_categorical_mixture_of_one_hot_categorical()</code>,
<code>layer_distribution_lambda()</code>,
<code>layer_independent_bernoulli()</code>,
<code>layer_independent_logistic()</code>,
<code>layer_independent_normal()</code>,
<code>layer_independent_poisson()</code>,
<code>layer_kl_divergence_regularizer()</code>,
<code>layer_mixture_logistic()</code>,
<code>layer_mixture_normal()</code>,
<code>layer_mixture_same_family()</code>,
<code>layer_multivariate_normal_tri_l()</code>,
<code>layer_one_hot_categorical()</code>
</p>


</div>