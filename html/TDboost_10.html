<div class="container">

<table style="width: 100%;"><tr>
<td>TDboost</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>TDboost Tweedie Regression Modeling</h2>

<h3>Description</h3>

<p>Fits TDboost Tweedie Regression models.</p>


<h3>Usage</h3>

<pre><code class="language-R">TDboost(formula = formula(data),
    distribution = list(name="EDM",alpha=1.5),
    data = list(),
    weights,
    var.monotone = NULL,
    n.trees = 100,
    interaction.depth = 1,
    n.minobsinnode = 10,
    shrinkage = 0.001,
    bag.fraction = 0.5,
    train.fraction = 1.0,
    cv.folds=0,
    keep.data = TRUE,
    verbose = TRUE)

TDboost.fit(x,y,
        offset = NULL,
        misc = NULL,
        distribution = list(name="EDM",alpha=1.5),
        w = NULL,
        var.monotone = NULL,
        n.trees = 100,
        interaction.depth = 1,
        n.minobsinnode = 10,
        shrinkage = 0.001,
        bag.fraction = 0.5,
        train.fraction = 1.0,
        keep.data = TRUE,
        verbose = TRUE,
        var.names = NULL,
        response.name = NULL)

TDboost.more(object,
         n.new.trees = 100,
         data = NULL,
         weights = NULL,
         offset = NULL,
         verbose = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>a symbolic description of the model to be fit. The formula may 
include an offset term (e.g. y~offset(n)+x). If <code>keep.data=FALSE</code> in 
the initial call to <code>TDboost</code> then it is the user's responsibility to 
resupply the offset to <code>TDboost.more</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>distribution</code></td>
<td>
<p>a list with a component <code>name</code> specifying the distribution 
and any additional parameters needed. Tweedie regression is available and <code>distribution</code> must a list of the form 
<code>list(name="EDM",alpha=1.5)</code> where <code>alpha</code> is the index parameter that must be in (1,2]. 
When <code>alpha=2</code>, the distribution reduces to gamma.
The current version's Tweedie regression methods do 
not handle non-constant weights and will stop.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>an optional data frame containing the variables in the model. By
default the variables are taken from <code>environment(formula)</code>, typically 
the environment from which <code>TDboost</code> is called. If <code>keep.data=TRUE</code> in 
the initial call to <code>TDboost</code> then <code>TDboost</code> stores a copy with the 
object. If <code>keep.data=FALSE</code> then subsequent calls to 
<code>TDboost.more</code> must resupply the same dataset. It becomes the user's 
responsibility to resupply the same data at this point.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting process. 
Must be positive but do not need to be normalized. If <code>keep.data=FALSE</code> 
in the initial call to <code>TDboost</code> then it is the user's responsibility to 
resupply the weights to <code>TDboost.more</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>var.monotone</code></td>
<td>
<p>an optional vector, the same length as the number of
predictors, indicating which variables have a monotone increasing (+1),
decreasing (-1), or arbitrary (0) relationship with the outcome.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.trees</code></td>
<td>
<p>the total number of trees to fit. This is equivalent to the
number of iterations and the number of basis functions in the additive
expansion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cv.folds</code></td>
<td>
<p>Number of cross-validation folds to perform. If <code>cv.folds</code>&gt;1 then
<code>TDboost</code>, in addition to the usual fit, will perform a cross-validation, calculate
an estimate of generalization error returned in <code>cv.error</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>interaction.depth</code></td>
<td>
<p>The maximum depth of variable interactions. 1 implies
an additive model, 2 implies a model with up to 2-way interactions, etc.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.minobsinnode</code></td>
<td>
<p>minimum number of observations in the trees terminal
nodes. Note that this is the actual number of observations not the total
weight.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shrinkage</code></td>
<td>
<p>a shrinkage parameter applied to each tree in the expansion.
Also known as the learning rate or step-size reduction.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bag.fraction</code></td>
<td>
<p>the fraction of the training set observations randomly
selected to propose the next tree in the expansion. This introduces randomnesses
into the model fit. If <code>bag.fraction</code>&lt;1 then running the same model twice
will result in similar but different fits. <code>TDboost</code> uses the R random number
generator so <code>set.seed</code> can ensure that the model can be
reconstructed. Preferably, the user can save the returned
<code>TDboost.object</code> using <code>save</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>train.fraction</code></td>
<td>
<p>The first <code>train.fraction * nrows(data)</code>
observations are used to fit the <code>TDboost</code> and the remainder are used for
computing out-of-sample estimates of the loss function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keep.data</code></td>
<td>
<p>a logical variable indicating whether to keep the data and
an index of the data stored with the object. Keeping the data and index makes
subsequent calls to <code>TDboost.more</code> faster at the cost of storing an
extra copy of the dataset.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>a <code>TDboost</code> object created from an initial call to
<code>TDboost</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.new.trees</code></td>
<td>
<p>the number of additional trees to add to <code>object</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>If TRUE, TDboost will print out progress and performance indicators.
If this option is left unspecified for TDboost.more then it uses <code>verbose</code> from
<code>object</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x, y</code></td>
<td>
<p>For <code>TDboost.fit</code>: <code>x</code> is a data frame or data matrix containing the
predictor variables and <code>y</code> is the vector of outcomes. The number of rows
in <code>x</code> must be the same as the length of <code>y</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>offset</code></td>
<td>
<p>a vector of values for the offset</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>misc</code></td>
<td>
<p>For <code>TDboost.fit</code>: <code>misc</code> is an R object that is simply passed on to
the TDboost engine.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w</code></td>
<td>
<p>For <code>TDboost.fit</code>: <code>w</code> is a vector of weights of the same
length as the <code>y</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>var.names</code></td>
<td>
<p>For <code>TDboost.fit</code>: A vector of strings of length equal to the
number of columns of <code>x</code> containing the names of the predictor variables.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>response.name</code></td>
<td>
<p>For <code>TDboost.fit</code>: A character string label for the response
variable.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This package implements a regression tree based gradient boosting estimator for nonparametric multiple Tweedie regression. The code is a modified version of <code>gbm</code> library originally written by Greg Ridgeway.
</p>
<p>Boosting is the process of iteratively adding basis functions in a greedy
fashion so that each additional basis function further reduces the selected
loss function. This implementation closely follows Friedman's Gradient
Boosting Machine (Friedman, 2001).
</p>
<p>In addition to many of the features documented in the Gradient Boosting Machine,
<code>TDboost</code> offers additional features including the out-of-bag estimator for
the optimal number of iterations, the ability to store and manipulate the
resulting <code>TDboost</code> object.
</p>
<p><code>TDboost.fit</code> provides the link between R and the C++ TDboost engine. <code>TDboost</code>
is a front-end to <code>TDboost.fit</code> that uses the familiar R modeling formulas.
However, <code>model.frame</code> is very slow if there are many
predictor variables. For power-users with many variables use <code>TDboost.fit</code>.
For general practice <code>TDboost</code> is preferable.
</p>


<h3>Value</h3>

<p><code>TDboost</code>, <code>TDboost.fit</code>, and <code>TDboost.more</code> return a
<code>TDboost.object</code>.
</p>


<h3>Author(s)</h3>

<p>Yi Yang <a href="mailto:yi.yang6@mcgill.ca">yi.yang6@mcgill.ca</a>, Wei Qian <a href="mailto:wxqsma@rit.edu">wxqsma@rit.edu</a> and Hui Zou <a href="mailto:hzou@stat.umn.edu">hzou@stat.umn.edu</a></p>


<h3>References</h3>

<p>Yang, Y., Qian, W. and Zou, H. (2013), “A Boosted Tweedie Compound Poisson Model for Insurance Premium” Preprint.
</p>
<p>G. Ridgeway (1999). “The state of boosting,” <em>Computing Science and
Statistics</em> 31:172-181.
</p>
<p>J.H. Friedman (2001). “Greedy Function Approximation: A Gradient Boosting
Machine,” <em>Annals of Statistics</em> 29(5):1189-1232.
</p>
<p>J.H. Friedman (2002). “Stochastic Gradient Boosting,” <em>Computational Statistics
and Data Analysis</em> 38(4):367-378.
</p>


<h3>See Also</h3>

<p><code>TDboost.object</code>,
<code>TDboost.perf</code>,
<code>plot.TDboost</code>,
<code>predict.TDboost</code>,
<code>summary.TDboost</code>,
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(FHT)
# training on data1
TDboost1 &lt;- TDboost(Y~X1+X2+X3+X4+X5+X6,         # formula
    data=data1,                   # dataset
    var.monotone=c(0,0,0,0,0,0), # -1: monotone decrease,
                                 # +1: monotone increase,
                                 #  0: no monotone restrictions
    distribution=list(name="EDM",alpha=1.5),
                                 # specify Tweedie index parameter
    n.trees=3000,                # number of trees
    shrinkage=0.005,             # shrinkage or learning rate,
                                 # 0.001 to 0.1 usually work
    interaction.depth=3,         # 1: additive model, 2: two-way interactions, etc.
    bag.fraction = 0.5,          # subsampling fraction, 0.5 is probably best
    train.fraction = 0.5,        # fraction of data for training,
                                 # first train.fraction*N used for training
    n.minobsinnode = 10,         # minimum total weight needed in each node
    cv.folds = 5,                # do 5-fold cross-validation
    keep.data=TRUE,              # keep a copy of the dataset with the object
    verbose=TRUE)                # print out progress

# print out the optimal iteration number M
best.iter &lt;- TDboost.perf(TDboost1,method="test")
print(best.iter)

# check performance using 5-fold cross-validation
best.iter &lt;- TDboost.perf(TDboost1,method="cv")
print(best.iter)

# plot the performance
# plot variable influence
summary(TDboost1,n.trees=1)         # based on the first tree
summary(TDboost1,n.trees=best.iter) # based on the estimated best number of trees

# making prediction on data2
f.predict &lt;- predict.TDboost(TDboost1,data2,best.iter)

# least squares error
print(sum((data2$Y-f.predict)^2))

# create marginal plots
# plot variable X1 after "best" iterations
plot.TDboost(TDboost1,1,best.iter)
# contour plot of variables 1 and 3 after "best" iterations
plot.TDboost(TDboost1,c(1,3),best.iter)

# do another 20 iterations
TDboost2 &lt;- TDboost.more(TDboost1,20,
                 verbose=FALSE) # stop printing detailed progress

# fit a gamma model (when alpha = 2.0)
data2 &lt;- data1[data1$Y!=0,]
TDboost3 &lt;- TDboost(Y~X1+X2+X3+X4+X5+X6,         # formula
    data=data2,                   # dataset
    distribution=list(name="EDM",alpha=2.0),
    n.trees=3000, 				 # number of trees
    train.fraction = 0.5,        # fraction of data for training,
    verbose=TRUE)                # print out progress
best.iter2 &lt;- TDboost.perf(TDboost3,method="test")

</code></pre>


</div>