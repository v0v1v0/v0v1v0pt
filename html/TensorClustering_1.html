<div class="container">

<table style="width: 100%;"><tr>
<td>DEEM</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Doubly-enhanced EM algorithm</h2>

<h3>Description</h3>

<p>Doubly-enhanced EM algorithm for tensor clustering</p>


<h3>Usage</h3>

<pre><code class="language-R">DEEM(X, nclass, niter = 100, lambda = NULL, dfmax = n, pmax = nvars, pf = rep(1, nvars),
eps = 1e-04, maxit = 1e+05, sml = 1e-06, verbose = FALSE, ceps = 0.1,
initial = TRUE, vec_x = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Input tensor (or matrix) list of length <code class="reqn">n</code>, where <code class="reqn">n</code> is the number of observations. Each element of the list is a tensor or matrix. The order of tensor can be any positive integer not less than 2.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nclass</code></td>
<td>
<p>Number of clusters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>niter</code></td>
<td>
<p>Maximum iteration times for EM algorithm. Default value is 100.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>A user-specified <code>lambda</code> value. <code>lambda</code> is the weight of L1 penalty and a smaller <code>lambda</code> allows more variables to be nonzero</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dfmax</code></td>
<td>
<p>The maximum number of selected variables in the model. Default is the number of observations <code>n</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pmax</code></td>
<td>
<p>The maximum number of potential selected variables during iteration. In middle step, the algorithm can select at most <code>pmax</code> variables and then shrink part of them such that the number of final selected variables is less than <code>dfmax</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pf</code></td>
<td>
<p>Weight of lasso penalty. Default is a vector of value <code>1</code> and length <code>p</code>, representing L1 penalty of length <code class="reqn">p</code>. Can be modified to use adaptive lasso penalty.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>Convergence threshold for coordinate descent difference between iterations. Default value is <code>1e-04</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>Maximum iteration times for coordinate descent for all lambda. Default value is <code>1e+05</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sml</code></td>
<td>
<p>Threshold for ratio of loss function change after each iteration to old loss function value. Default value is <code>1e-06</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Indicates whether print out lambda during iteration or not. Default value is <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ceps</code></td>
<td>
<p>Convergence threshold for cluster mean difference between iterations. Default value is <code>1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initial</code></td>
<td>
<p>Whether to initialize algorithm with K-means clustering. Default value is <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vec_x</code></td>
<td>
<p>Vectorized tensor data. Default value is <code>NULL</code></p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The <code>DEEM</code> function implements the Doubly-Enhanced EM algorithm (DEEM) for tensor clustering. The observations <code class="reqn">\mathbf{X}_i</code> are assumed to be following the tensor normal mixture model (TNMM) with common covariances across different clusters:
</p>
<p style="text-align: center;"><code class="reqn">
\mathbf{X}_i\sim\sum_{k=1}^K\pi_k \mathrm{TN}(\bm{\mu}_k;\bm{\Sigma}_1,\ldots,\bm{\Sigma}_M),\quad i=1,\dots,n,
</code>
</p>

<p>where <code class="reqn">0&lt;\pi_k&lt;1</code> is the prior probability for <code class="reqn">\mathbf{X}</code> to be in the <code class="reqn">k</code>-th cluster such that <code class="reqn">\sum_{k=1}^{K}\pi_k=1</code>, <code class="reqn">\bm{\mu}_k</code> is the cluster mean of the <code class="reqn">k</code>-th cluster and <code class="reqn">\bm{\Sigma}_1,\ldots,\bm{\Sigma}_M)</code> are the common covariances across different clusters. Under the TNMM framework, the optimal clustering rule can be showed as
</p>
<p style="text-align: center;"><code class="reqn">
\widehat{Y}^{opt}=\arg\max_k\{\log\pi_k+\langle\mathbf{X}-(\bm{\mu}_1+\bm{\mu}_k)/2,\mathbf{B}_k\rangle\},
</code>
</p>

<p>where <code class="reqn">\mathbf{B}_k=[\![\bm{\mu}_k-\bm{\mu}_1;\bm{\Sigma}_1^{-1},\ldots,\bm{\Sigma}_M^{-1}]\!]</code>. In the enhanced E-step, <code>DEEM</code> imposes sparsity directly on the optimal clustering rule as a flexible alternative to popular low-rank assumptions on tensor coefficients <code class="reqn">\mathbf{B}_k</code> as 
</p>
<p style="text-align: center;"><code class="reqn">
\min_{\mathbf{B}_2,\dots,\mathbf{B}_K}\bigg[\sum_{k=2}^K(\langle\mathbf{B}_k,[\![\mathbf{B}_k,\widehat{\bm{\Sigma}}_1^{(t)},\ldots,\widehat{\bm{\Sigma}}_M^{(t)}]\!]\rangle-2\langle\mathbf{B}_k,\widehat{\bm{\mu}}_k^{(t)}-\widehat{\bm{\mu}}_1^{(t)}\rangle) +\lambda^{(t+1)}\sum_{\mathcal{J}}\sqrt{\sum_{k=2}^Kb_{k,\mathcal{J}}^2}\bigg],
</code>
</p>

<p>where <code class="reqn">\lambda^{(t+1)}</code> is a tuning parameter. In the enhanced M-step, <code>DEEM</code> employs a new estimator for the tensor correlation structure, which facilitates both the computation and the theoretical studies.</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>pi</code></td>
<td>
<p>A vector of estimated prior probabilities for clusters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu</code></td>
<td>
<p>A list of estimated cluster means.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigma</code></td>
<td>
<p>A list of estimated covariance matrices.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>A <code>n</code> by <code>nclass</code> matrix of estimated membership weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>A vector of estimated labels.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>Number of iterations until convergence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>Average zero elements in beta over iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>A matrix of vectorized <code>B_k</code>.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Kai Deng, Yuqing Pan, Xin Zhang and Qing Mai</p>


<h3>References</h3>

<p>Mai, Q., Zhang, X., Pan, Y. and Deng, K. (2021). A Doubly-Enhanced EM Algorithm for Model-Based Tensor Clustering. <em>Journal of the American Statistical Association</em>.</p>


<h3>See Also</h3>

<p><code>tune_lamb</code>, <code>tune_K</code></p>


<h3>Examples</h3>

<pre><code class="language-R">dimen = c(5,5,5)
nvars = prod(dimen)
K = 2
n = 100
sigma = array(list(),3)

sigma[[1]] = sigma[[2]] = sigma[[3]] = diag(5)

B2=array(0,dim=dimen)
B2[1:3,1,1]=2

y = c(rep(1,50),rep(2,50))
M = array(list(),K)
M[[1]] = array(0,dim=dimen)
M[[2]] = B2

vec_x=matrix(rnorm(n*prod(dimen)),ncol=n)
X=array(list(),n)
for (i in 1:n){
  X[[i]] = array(vec_x[,i],dim=dimen)
  X[[i]] = M[[y[i]]] + X[[i]]
}

myfit = DEEM(X, nclass=2, lambda=0.05)
</code></pre>


</div>