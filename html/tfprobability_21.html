<div class="container">

<table style="width: 100%;"><tr>
<td>layer_dense_variational</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Dense Variational Layer</h2>

<h3>Description</h3>

<p>This layer uses variational inference to fit a "surrogate" posterior to the
distribution over both the <code>kernel</code> matrix and the <code>bias</code> terms which are
otherwise used in a manner similar to <code>layer_dense()</code>.
This layer fits the "weights posterior" according to the following generative
process:
</p>
<div class="sourceCode none"><pre>[K, b] ~ Prior()
M = matmul(X, K) + b
Y ~ Likelihood(M)
</pre></div>


<h3>Usage</h3>

<pre><code class="language-R">layer_dense_variational(
  object,
  units,
  make_posterior_fn,
  make_prior_fn,
  kl_weight = NULL,
  kl_use_exact = FALSE,
  activation = NULL,
  use_bias = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li>
<p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li>
<p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li>
<p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>units</code></td>
<td>
<p>Positive integer, dimensionality of the output space.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>make_posterior_fn</code></td>
<td>
<p>function taking <code>tf$size(kernel)</code>,
<code>tf$size(bias)</code>, <code>dtype</code> and returns another callable which takes an
input and produces a <code>tfd$Distribution</code> instance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>make_prior_fn</code></td>
<td>
<p>function taking <code>tf$size(kernel)</code>, <code>tf$size(bias)</code>,
<code>dtype</code> and returns another callable which takes an input and produces a
<code>tfd$Distribution</code> instance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kl_weight</code></td>
<td>
<p>Amount by which to scale the KL divergence loss between prior
and posterior.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kl_use_exact</code></td>
<td>
<p>Logical indicating that the analytical KL divergence
should be used rather than a Monte Carlo approximation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>activation</code></td>
<td>
<p>An activation function.  See <code>keras::layer_dense</code>. Default: <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_bias</code></td>
<td>
<p>Whether or not the dense layers constructed in this layer
should have a bias term.  See <code>keras::layer_dense</code>.  Default: <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional keyword arguments passed to the <code>keras::layer_dense</code> constructed by this layer.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>Other layers: 
<code>layer_autoregressive()</code>,
<code>layer_conv_1d_flipout()</code>,
<code>layer_conv_1d_reparameterization()</code>,
<code>layer_conv_2d_flipout()</code>,
<code>layer_conv_2d_reparameterization()</code>,
<code>layer_conv_3d_flipout()</code>,
<code>layer_conv_3d_reparameterization()</code>,
<code>layer_dense_flipout()</code>,
<code>layer_dense_local_reparameterization()</code>,
<code>layer_dense_reparameterization()</code>,
<code>layer_variable()</code>
</p>


</div>