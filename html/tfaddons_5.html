<div class="container">

<table style="width: 100%;"><tr>
<td>activation_rrelu</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Rrelu</h2>

<h3>Description</h3>

<p>rrelu function.
</p>


<h3>Usage</h3>

<pre><code class="language-R">activation_rrelu(
  x,
  lower = 0.125,
  upper = 0.333333333333333,
  training = NULL,
  seed = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A 'Tensor'. Must be one of the following types: 'float16', 'float32', 'float64'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lower</code></td>
<td>
<p>'float', lower bound for random alpha.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>upper</code></td>
<td>
<p>'float', upper bound for random alpha.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>training</code></td>
<td>
<p>'bool', indicating whether the 'call' is meant for training or inference.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>'int', this sets the operation-level seed. Returns:</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Computes rrelu function:
'x if x &gt; 0 else random(lower, upper) * x' or
'x if x &gt; 0 else x * (lower + upper) / 2'
depending on whether training is enabled.
See [Empirical Evaluation of Rectified Activations in Convolutional Network](https://arxiv.org/abs/1505.00853).
</p>


<h3>Value</h3>

<p>A 'Tensor'. Has the same type as 'x'.
</p>


<h3>Computes rrelu function</h3>

<p>'x if x &gt; 0 else random(lower, upper) * x' or 'x if x &gt; 0 else x * (lower + upper) / 2' depending on
whether training is enabled.
</p>


</div>