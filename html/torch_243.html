<div class="container">

<table style="width: 100%;"><tr>
<td>nn_conv_transpose3d</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>ConvTranpose3D module</h2>

<h3>Description</h3>

<p>Applies a 3D transposed convolution operator over an input image composed of several input
planes.
</p>


<h3>Usage</h3>

<pre><code class="language-R">nn_conv_transpose3d(
  in_channels,
  out_channels,
  kernel_size,
  stride = 1,
  padding = 0,
  output_padding = 0,
  groups = 1,
  bias = TRUE,
  dilation = 1,
  padding_mode = "zeros"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>in_channels</code></td>
<td>
<p>(int): Number of channels in the input image</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>out_channels</code></td>
<td>
<p>(int): Number of channels produced by the convolution</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_size</code></td>
<td>
<p>(int or tuple): Size of the convolving kernel</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stride</code></td>
<td>
<p>(int or tuple, optional): Stride of the convolution. Default: 1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>padding</code></td>
<td>
<p>(int or tuple, optional): <code>dilation * (kernel_size - 1) - padding</code> zero-padding
will be added to both sides of each dimension in the input. Default: 0
output_padding (int or tuple, optional): Additional size added to one side
of each dimension in the output shape. Default: 0</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>output_padding</code></td>
<td>
<p>(int or tuple, optional): Additional size added to one side
of each dimension in the output shape. Default: 0</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>groups</code></td>
<td>
<p>(int, optional): Number of blocked connections from input channels to output channels. Default: 1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias</code></td>
<td>
<p>(bool, optional): If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dilation</code></td>
<td>
<p>(int or tuple, optional): Spacing between kernel elements. Default: 1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>padding_mode</code></td>
<td>
<p>(string, optional): <code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code></p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The transposed convolution operator multiplies each input value element-wise by a learnable kernel,
and sums over the outputs from all input feature planes.
</p>
<p>This module can be seen as the gradient of Conv3d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation).
</p>

<ul>
<li> <p><code>stride</code> controls the stride for the cross-correlation.
</p>
</li>
<li> <p><code>padding</code> controls the amount of implicit zero-paddings on both
sides for <code>dilation * (kernel_size - 1) - padding</code> number of points. See note
below for details.
</p>
</li>
<li> <p><code>output_padding</code> controls the additional size added to one side
of the output shape. See note below for details.
</p>
</li>
<li> <p><code>dilation</code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but this <code>link</code>_ has a nice visualization of what <code>dilation</code> does.
</p>
</li>
<li> <p><code>groups</code> controls the connections between inputs and outputs.
<code>in_channels</code> and <code>out_channels</code> must both be divisible by
<code>groups</code>. For example,
</p>

<ul>
<li>
<p> At groups=1, all inputs are convolved to all outputs.
</p>
</li>
<li>
<p> At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels,
and producing half the output channels, and both subsequently
concatenated.
</p>
</li>
<li>
<p> At groups= <code>in_channels</code>, each input channel is convolved with
its own set of filters (of size
<code class="reqn">\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor</code>).
</p>
</li>
</ul>
</li>
</ul>
<p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>output_padding</code>
can either be:
</p>

<ul>
<li>
<p> a single <code>int</code> – in which case the same value is used for the depth, height and width dimensions
</p>
</li>
<li>
<p> a <code>tuple</code> of three ints – in which case, the first <code>int</code> is used for the depth dimension,
the second <code>int</code> for the height dimension and the third <code>int</code> for the width dimension
</p>
</li>
</ul>
<h3>Shape</h3>


<ul>
<li>
<p> Input: <code class="reqn">(N, C_{in}, D_{in}, H_{in}, W_{in})</code>
</p>
</li>
<li>
<p> Output: <code class="reqn">(N, C_{out}, D_{out}, H_{out}, W_{out})</code> where
</p>
<p style="text-align: center;"><code class="reqn">
  D_{out} = (D_{in} - 1) \times \mbox{stride}[0] - 2 \times \mbox{padding}[0] + \mbox{dilation}[0]
\times (\mbox{kernel\_size}[0] - 1) + \mbox{output\_padding}[0] + 1
</code>
</p>

<p style="text-align: center;"><code class="reqn">
  H_{out} = (H_{in} - 1) \times \mbox{stride}[1] - 2 \times \mbox{padding}[1] + \mbox{dilation}[1]
\times (\mbox{kernel\_size}[1] - 1) + \mbox{output\_padding}[1] + 1
</code>
</p>

<p style="text-align: center;"><code class="reqn">
  W_{out} = (W_{in} - 1) \times \mbox{stride}[2] - 2 \times \mbox{padding}[2] + \mbox{dilation}[2]
\times (\mbox{kernel\_size}[2] - 1) + \mbox{output\_padding}[2] + 1
</code>
</p>

</li>
</ul>
<h3>Attributes</h3>


<ul>
<li>
<p> weight (Tensor): the learnable weights of the module of shape
<code class="reqn">(\mbox{in\_channels}, \frac{\mbox{out\_channels}}{\mbox{groups}},</code>
<code class="reqn">\mbox{kernel\_size[0]}, \mbox{kernel\_size[1]}, \mbox{kernel\_size[2]})</code>.
The values of these weights are sampled from
<code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
<code class="reqn">k = \frac{groups}{C_{\mbox{out}} * \prod_{i=0}^{2}\mbox{kernel\_size}[i]}</code>
</p>
</li>
<li>
<p> bias (Tensor):   the learnable bias of the module of shape (out_channels)
If <code>bias</code> is <code>True</code>, then the values of these weights are
sampled from <code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code> where
<code class="reqn">k = \frac{groups}{C_{\mbox{out}} * \prod_{i=0}^{2}\mbox{kernel\_size}[i]}</code>
</p>
</li>
</ul>
<h3>Note</h3>

<p>Depending of the size of your kernel, several (of the last)
columns of the input might be lost, because it is a valid <code>cross-correlation</code><em>,
and not a full <code>cross-correlation</code></em>.
It is up to the user to add proper padding.
</p>
<p>The <code>padding</code> argument effectively adds <code>dilation * (kernel_size - 1) - padding</code>
amount of zero padding to both sizes of the input. This is set so that
when a <code>~torch.nn.Conv3d</code> and a <code>~torch.nn.ConvTranspose3d</code>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code>stride &gt; 1</code>,
<code>~torch.nn.Conv3d</code> maps multiple input shapes to the same output
shape. <code>output_padding</code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code>output_padding</code> is only used to find output shape, but does
not actually add zero-padding to output.
</p>
<p>In some circumstances when using the CUDA backend with CuDNN, this operator
may select a nondeterministic algorithm to increase performance. If this is
undesirable, you can try to make the operation deterministic (potentially at
a performance cost) by setting <code>torch.backends.cudnn.deterministic = TRUE</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">if (torch_is_installed()) {
## Not run: 
# With square kernels and equal stride
m &lt;- nn_conv_transpose3d(16, 33, 3, stride = 2)
# non-square kernels and unequal stride and with padding
m &lt;- nn_conv_transpose3d(16, 33, c(3, 5, 2), stride = c(2, 1, 1), padding = c(0, 4, 2))
input &lt;- torch_randn(20, 16, 10, 50, 100)
output &lt;- m(input)

## End(Not run)
}
</code></pre>


</div>