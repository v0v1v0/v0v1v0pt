<div class="container">

<table style="width: 100%;"><tr>
<td>nlp_scores</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Natural Language Processing Scores</h2>

<h3>Description</h3>

<p>Natural Language Processing using word embeddings to compute
semantic similarities (cosine; see
<code>costring</code>) of text and specified classes
</p>


<h3>Usage</h3>

<pre><code class="language-R">nlp_scores(
  text,
  classes,
  semantic_space = c("baroni", "cbow", "cbow_ukwac", "en100", "glove", "tasa"),
  preprocess = TRUE,
  remove_stop = TRUE,
  keep_in_env = TRUE,
  envir = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>text</code></td>
<td>
<p>Character vector or list.
Text in a vector or list data format</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>classes</code></td>
<td>
<p>Character vector.
Classes to score the text</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>semantic_space</code></td>
<td>
<p>Character vector.
The semantic space used to compute the distances between words
(more than one allowed). Here's a list of the semantic spaces:
</p>

<dl>
<dt><code>"baroni"</code></dt>
<dd>
<p>Combination of British National Corpus, ukWaC corpus, and a 2009
Wikipedia dump. Space created using continuous bag of words algorithm
using a context window size of 11 words (5 left and right)
and 400 dimensions. Best word2vec model according to
Baroni, Dinu, &amp; Kruszewski (2014)</p>
</dd>
<dt><code>"cbow"</code></dt>
<dd>
<p>Combination of British National Corpus, ukWaC corpus, and a 2009
Wikipedia dump. Space created using continuous bag of words algorithm with
a context window size of 5 (2 left and right) and 300 dimensions</p>
</dd>
<dt><code>"cbow_ukwac"</code></dt>
<dd>
<p>ukWaC corpus with the continuous bag of words algorithm with
a context window size of 5 (2 left and right) and 400 dimensions</p>
</dd>
<dt><code>"en100"</code></dt>
<dd>
<p>Combination of British National Corpus, ukWaC corpus, and a 2009
Wikipedia dump. 100,000 most frequent words. Uses moving window model
with a size of 5 (2 to the left and right). Positive pointwise mutual
information and singular value decomposition was used to reduce the
space to 300 dimensions</p>
</dd>
<dt><code>"glove"</code></dt>
<dd>
<p><a href="https://dumps.wikimedia.org/">Wikipedia 2014 dump</a> and <a href="https://catalog.ldc.upenn.edu/LDC2011T07">Gigaword 5</a> with 400,000
words (300 dimensions). Uses co-occurrence of words in text
documents (uses cosine similarity)</p>
</dd>
<dt><code>"tasa"</code></dt>
<dd>
<p>Latent Semantic Analysis space from TASA corpus all (300 dimensions).Uses co-occurrence of words in text documents (uses cosine similarity)</p>
</dd>
</dl>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>preprocess</code></td>
<td>
<p>Boolean.
Should basic preprocessing be applied?
Includes making lowercase, keeping only alphanumeric characters,
removing escape characters, removing repeated characters,
and removing white space.
Defaults to <code>TRUE</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>remove_stop</code></td>
<td>
<p>Boolean.
Should <code>stop_words</code>
be removed?
Defaults to <code>TRUE</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keep_in_env</code></td>
<td>
<p>Boolean.
Whether the classifier should be kept in your global environment.
Defaults to <code>TRUE</code>.
By keeping the classifier in your environment, you can skip
re-loading the classifier every time you run this function.
<code>TRUE</code> is recommended</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>envir</code></td>
<td>
<p>Numeric.
Environment for the classifier to be saved for repeated use.
Defaults to the global environment</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Returns semantic distances for the text classes
</p>


<h3>Author(s)</h3>

<p>Alexander P. Christensen &lt;alexpaulchristensen@gmail.com&gt;
</p>


<h3>References</h3>

<p>Baroni, M., Dinu, G., &amp; Kruszewski, G. (2014).
Don't count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.
In <em>Proceedings of the 52nd annual meting of the association for computational linguistics</em> (pp. 238-247).
</p>
<p>Landauer, T.K., &amp; Dumais, S.T. (1997).
A solution to Plato's problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge.
<em>Psychological Review</em>, <em>104</em>, 211-240.
</p>
<p>Pennington, J., Socher, R., &amp; Manning, C. D. (2014).
GloVe: Global vectors for word representation.
In <em>Proceedings of the 2014 conference on empirical methods in natural language processing</em> (pp. 1532-1543).
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Load data
data(neo_ipip_extraversion)

# Example text 
text &lt;- neo_ipip_extraversion$friendliness[1:5]

## Not run: 
# GloVe
nlp_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 )
)

# Baroni
nlp_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 semantic_space = "baroni"
)
 
# CBOW
nlp_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 semantic_space = "cbow"
)

# CBOW + ukWaC
nlp_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 semantic_space = "cbow_ukwac"
)

# en100
nlp_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 semantic_space = "en100"
)

# tasa
nlp_scores(
 text = text,
 classes = c(
   "friendly", "gregarious", "assertive",
   "active", "excitement", "cheerful"
 ),
 semantic_space = "tasa"
)

## End(Not run)

</code></pre>


</div>