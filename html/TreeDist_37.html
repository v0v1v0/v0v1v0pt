<div class="container">

<table style="width: 100%;"><tr>
<td>KMeansPP</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>k-means++ clustering</h2>

<h3>Description</h3>

<p>k-means++ clustering (Arthur and Vassilvitskii 2007) improves the speed and
accuracy of standard <code>kmeans</code> clustering
(Hartigan and Wong 1979) by preferring initial cluster centres
that are far from others.
A scalable version of the algorithm has been proposed for larger data sets
(Bahmani et al. 2012), but is not implemented here.
</p>


<h3>Usage</h3>

<pre><code class="language-R">KMeansPP(x, k = 2, nstart = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Numeric matrix of data, or an object that can be coerced to such a
matrix (such as a numeric vector or a data frame with all numeric columns).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>Integer specifying the number of clusters, <em>k</em>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nstart</code></td>
<td>
<p>Positive integer specifying how many random sets should be
chosen</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional arguments passed to <code>kmeans</code></p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p><a href="https://orcid.org/0000-0001-5660-1727">Martin R. Smith</a>
(<a href="mailto:martin.smith@durham.ac.uk">martin.smith@durham.ac.uk</a>)
</p>


<h3>References</h3>

<p>Arthur D, Vassilvitskii S (2007).
“K-Means++: The Advantages of Careful Seeding.”
In <em>Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms</em>,  SODA '07, 1027–1035.<br><br> Bahmani B, Moseley B, Vattani A, Kumar R, Vassilvitskii S (2012).
“Scalable K-Means++.”
<em>arXiv</em>.
<a href="https://doi.org/10.48550/arXiv.1203.6402">doi:10.48550/arXiv.1203.6402</a>, 1203.6402.<br><br> Hartigan JA, Wong MA (1979).
“Algorithm AS 136: a <em>K</em>-means clustering algorithm.”
<em>Journal of the Royal Statistical Society. Series C (Applied Statistics)</em>, <b>28</b>(1), 100–108.
<a href="https://doi.org/10.2307/2346830">doi:10.2307/2346830</a>.
</p>


<h3>See Also</h3>

<p><code>kmeans</code>
</p>
<p>Other cluster functions: 
<code>cluster-statistics</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Generate random points
set.seed(1)
x &lt;- cbind(c(rnorm(10, -5), rnorm(5, 1), rnorm(10, 6)),
           c(rnorm(5, 0), rnorm(15, 4), rnorm(5, 0)))

# Conventional k-means may perform poorly
klusters &lt;- kmeans(x, cent = 5)
plot(x, col = klusters$cluster, pch = rep(15:19, each = 5))

# Here, k-means++ recovers a better clustering
plusters &lt;- KMeansPP(x, k = 5)
plot(x, col = plusters$cluster, pch = rep(15:19, each = 5))
</code></pre>


</div>