<div class="container">

<table style="width: 100%;"><tr>
<td>loss_contrastive</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Contrastive loss</h2>

<h3>Description</h3>

<p>Computes the contrastive loss between 'y_true' and 'y_pred'.
</p>


<h3>Usage</h3>

<pre><code class="language-R">loss_contrastive(
  margin = 1,
  reduction = tf$keras$losses$Reduction$SUM_OVER_BATCH_SIZE,
  name = "contrasitve_loss"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>margin</code></td>
<td>
<p>Float, margin term in the loss definition. Default value is 1.0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reduction</code></td>
<td>
<p>(Optional) Type of tf$keras$losses$Reduction to apply.
Default value is SUM_OVER_BATCH_SIZE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>name</code></td>
<td>
<p>(Optional) name for the loss.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This loss encourages the embedding to be close to each other for
the samples of the same label and the embedding to be far apart at least
by the margin constant for the samples of different labels.
The euclidean distances 'y_pred' between two embedding matrices
'a' and 'b' with shape [batch_size, hidden_size] can be computed
as follows: “'python
# y_pred = '\sqrt' ('\sum_i' (a[:, i] - b[:, i])^2)
y_pred = tf$linalg.norm(a - b, axis=1)
“' See: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf
</p>


<h3>Value</h3>

<p>contrastive_loss: 1-D float 'Tensor' with shape [batch_size].
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## Not run: 
keras_model_sequential() %&gt;%
  layer_dense(4, input_shape = c(784)) %&gt;%
  compile(
    optimizer = 'sgd',
    loss=loss_contrastive(),
    metrics='accuracy'
  )

## End(Not run)

</code></pre>


</div>