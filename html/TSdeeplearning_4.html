<div class="container">

<table style="width: 100%;"><tr>
<td>RNN_ts</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Recurrent neural network Model
</h2>

<h3>Description</h3>

<p>The RNN function computes forecasted value with different forecasting evaluation criteria for recurrent neural network model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">RNN_ts(xt, xtlag = 4, uRNN = 2, Drate = 0, nEpochs = 10,
Loss = "mse", AccMetrics = "mae",ActFn = "tanh",
Split = 0.8, Valid = 0.1)</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>xt</code></td>
<td>

<p>Input univariate time series (ts) data.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xtlag</code></td>
<td>

<p>Lag of time series data.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>uRNN</code></td>
<td>

<p>Number of unit in RNN layer.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Drate</code></td>
<td>

<p>Dropout rate.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nEpochs</code></td>
<td>

<p>Number of epochs.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Loss</code></td>
<td>

<p>Loss function.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>AccMetrics</code></td>
<td>

<p>Metrics.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ActFn</code></td>
<td>

<p>Activation function.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Split</code></td>
<td>

<p>Index of the split point and separates the data into the training and testing datasets.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Valid</code></td>
<td>

<p>Validation set.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Recurrent neural networks (RNNs) (Rumelhart 1986) add the explicit handling of order between observations when learning a mapping function from inputs to outputs. RNNs actually process single elements of any input sequence at a particular time, and maintain a ‘state vector’ in their hidden units. Nevertheless, when the interval of data dependencies increases, the standard RNNs tend to suffer increasingly heavily from the problem of either vanishing gradient or exploding gradient (Bengio et al. 1994; Lin et al. 1996).
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>TrainFittedValue </code></td>
<td>
<p>Training Fitted value for given time series data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TestPredictedValue</code></td>
<td>
<p>Final forecasted value of the RNN model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fcast_criteria </code></td>
<td>
<p>Different Forecasting evaluation criteria for RNN model.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Bengio et al. 1994; Lin Sagheer A, Kotb M (2019) Time series forecasting of petroleum production using deep LSTM recurrent networks. Neurocomputing 323: 203–213.
</p>
<p>Rumelhart DE (1986) Learning internal representations by error propagation. In: Parallel distributed processing: Explorations in the microstructure of cognition. pp 318–362.
</p>
<p>Jha, G. K. and Sinha, K. (2014). Time-delay neural networks for time series prediction: An application to the monthly wholesale price of oilseeds in India. Neural Computing and Applications, 24(3–4), 563–571.
Jaiswal, R., Jha, G. K., Kumar, R. R. and Choudhary, K. (2022). Deep long short-term memory based model for agricultural price forecasting. Neural Computing and Applications, 34(6), 4661–4676.
</p>


<h3>See Also</h3>

<p>LSTM, GRU
</p>


<h3>Examples</h3>

<pre><code class="language-R">
data("Data_Maize")
RNN_ts(Data_Maize)

</code></pre>


</div>