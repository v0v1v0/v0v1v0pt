<div class="container">

<table style="width: 100%;"><tr>
<td>nnf_log_softmax</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Log_softmax</h2>

<h3>Description</h3>

<p>Applies a softmax followed by a logarithm.
</p>


<h3>Usage</h3>

<pre><code class="language-R">nnf_log_softmax(input, dim = NULL, dtype = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>input</code></td>
<td>
<p>(Tensor) input</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dim</code></td>
<td>
<p>(int) A dimension along which log_softmax will be computed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.
If specified, the input tensor is casted to <code>dtype</code> before the operation
is performed. This is useful for preventing data type overflows.
Default: <code>NULL</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>While mathematically equivalent to log(softmax(x)), doing these two
operations separately is slower, and numerically unstable. This function
uses an alternative formulation to compute the output and gradient correctly.
</p>


</div>