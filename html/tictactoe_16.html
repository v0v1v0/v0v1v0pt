<div class="container">

<table style="width: 100%;"><tr>
<td>ttt_qlearn</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Q-Learning for Training Tic-Tac-Toe AI</h2>

<h3>Description</h3>

<p>Train a tic-tac-toe AI through Q-learning
</p>


<h3>Usage</h3>

<pre><code class="language-R">ttt_qlearn(player, N = 1000L, epsilon = 0.1, alpha = 0.8, gamma = 0.99,
  simulate = TRUE, sim_every = 250L, N_sim = 1000L, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>player</code></td>
<td>
<p>AI player to train</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>N</code></td>
<td>
<p>number of episode, i.e. training games</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>fraction of random exploration move</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>learning rate</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>discount factor</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>simulate</code></td>
<td>
<p>if true, conduct simulation during training</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sim_every</code></td>
<td>
<p>conduct simulation after this many training games</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>N_sim</code></td>
<td>
<p>number of simulation games</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>if true, progress report is shown</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function implements Q-learning to train a tic-tac-toe AI player.
It is designed to train one AI player, which plays against itself to update its
value and policy functions.
</p>
<p>The employed algorithm is Q-learning with epsilon greedy.
For each state <code class="reqn">s</code>, the player updates its value evaluation by
</p>
<p style="text-align: center;"><code class="reqn">V(s) = (1-\alpha) V(s) + \alpha \gamma max_s' V(s')</code>
</p>

<p>if it is the first player's turn.  If it is the other player's turn, replace
<code class="reqn">max</code> by <code class="reqn">min</code>.
Note that <code class="reqn">s'</code> spans all possible states you can reach from <code class="reqn">s</code>.
The policy function is also updated analogously, that is, the set of
actions to reach <code class="reqn">s'</code> that maximizes <code class="reqn">V(s')</code>.
The parameter <code class="reqn">\alpha</code> controls the learning rate, and <code class="reqn">gamma</code> is
the discount factor (earlier win is better than later).
</p>
<p>Then the player chooses the next action by <code class="reqn">\epsilon</code>-greedy method;
Follow its policy with probability <code class="reqn">1-\epsilon</code>, and choose random
action with probability <code class="reqn">\epsilon</code>.  <code class="reqn">\epsilon</code> controls
the ratio of explorative moves.
</p>
<p>At the end of a game, the player sets the value of the final state either to
100 (if the first player wins), -100 (if the second player wins), or
0 (if draw).
</p>
<p>This learning process is repeated for <code>N</code> training games.
When <code>simulate</code> is set true, simulation is conducted after
<code>sim_every</code> training games.
This would be usefule for observing the progress of training.
In general, as the AI gets smarter, the game tends to result in draw more.
</p>
<p>See Sutton and Barto (1998) for more about the Q-learning.
</p>


<h3>Value</h3>

<p><code>data.frame</code> of simulation outcomes, if any
</p>


<h3>References</h3>

<p>Sutton, Richard S and Barto, Andrew G. Reinforcement Learning: An Introduction. The MIT Press (1998)
</p>


<h3>Examples</h3>

<pre><code class="language-R">p &lt;- ttt_ai()
o &lt;- ttt_qlearn(p, N = 200)
</code></pre>


</div>