<div class="container">

<table style="width: 100%;"><tr>
<td>layer_multi_head_attention</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Keras-based multi head attention layer</h2>

<h3>Description</h3>

<p>MultiHead Attention layer.
</p>


<h3>Usage</h3>

<pre><code class="language-R">layer_multi_head_attention(
  object,
  head_size,
  num_heads,
  output_size = NULL,
  dropout = 0,
  use_projection_bias = TRUE,
  return_attn_coef = FALSE,
  kernel_initializer = "glorot_uniform",
  kernel_regularizer = NULL,
  kernel_constraint = NULL,
  bias_initializer = "zeros",
  bias_regularizer = NULL,
  bias_constraint = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Model or layer object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>head_size</code></td>
<td>
<p>int, dimensionality of the 'query', 'key' and 'value' tensors after the linear transformation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_heads</code></td>
<td>
<p>int, number of attention heads.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>output_size</code></td>
<td>
<p>int, dimensionality of the output space, if 'NULL' then the input dimension of 'value' or 'key' will be used, default 'NULL'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dropout</code></td>
<td>
<p>float, 'rate' parameter for the dropout layer that is applied to attention after softmax, default '0'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_projection_bias</code></td>
<td>
<p>bool, whether to use a bias term after the linear output projection.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>return_attn_coef</code></td>
<td>
<p>bool, if 'TRUE', return the attention coefficients as an additional output argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_initializer</code></td>
<td>
<p>initializer, initializer for the kernel weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_regularizer</code></td>
<td>
<p>regularizer, regularizer for the kernel weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_constraint</code></td>
<td>
<p>constraint, constraint for the kernel weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias_initializer</code></td>
<td>
<p>initializer, initializer for the bias weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias_regularizer</code></td>
<td>
<p>regularizer, regularizer for the bias weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias_constraint</code></td>
<td>
<p>constraint, constraint for the bias weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional parameters to pass</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Defines the MultiHead Attention operation as defined in
[Attention Is All You Need](https://arxiv.org/abs/1706.03762) which takes
in a 'query', 'key' and 'value' tensors returns the dot-product attention
between them.
</p>


<h3>Value</h3>

<p>A tensor
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## Not run: 

mha = layer_multi_head_attention(head_size=128, num_heads=128)
query = tf$random$uniform(list(32L, 20L, 200L)) # (batch_size, query_elements, query_depth)
key = tf$random$uniform(list(32L, 15L, 300L)) # (batch_size, key_elements, key_depth)
value = tf$random$uniform(list(32L, 15L, 400L)) # (batch_size, key_elements, value_depth)
attention = mha(list(query, key, value)) # (batch_size, query_elements, value_depth)

# If `value` is not given then internally `value = key` will be used:
mha = layer_multi_head_attention(head_size=128, num_heads=128)
query = tf$random$uniform(list(32L, 20L, 200L)) # (batch_size, query_elements, query_depth)
key = tf$random$uniform(list(32L, 15L, 300L)) # (batch_size, key_elements, key_depth)
attention = mha(list(query, key)) # (batch_size, query_elements, value_depth)


## End(Not run)

</code></pre>


</div>