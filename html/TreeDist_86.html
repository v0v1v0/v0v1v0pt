<div class="container">

<table style="width: 100%;"><tr>
<td>SplitEntropy</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Entropy of two splits</h2>

<h3>Description</h3>

<p>Calculate the entropy, joint entropy, entropy distance and information
content of two splits, treating each split as a division of <em>n</em> leaves into
two groups.
Further details are available in a
<a href="https://ms609.github.io/TreeDist/articles/information.html">vignette</a>,
MacKay (2003) and
Meila (2007).
</p>


<h3>Usage</h3>

<pre><code class="language-R">SplitEntropy(split1, split2 = split1)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>split1, split2</code></td>
<td>
<p>Logical vectors listing leaves in a consistent order,
identifying each leaf as a member of the ingroup (<code>TRUE</code>) or outgroup
(<code>FALSE</code>) of the split in question.</p>
</td>
</tr></table>
<h3>Value</h3>

<p>A numeric vector listing, in bits:
</p>

<ul>
<li> <p><code>H1</code> The entropy of split 1;
</p>
</li>
<li> <p><code>H2</code> The entropy of split 2;
</p>
</li>
<li> <p><code>H12</code> The joint entropy of both splits;
</p>
</li>
<li> <p><code>I</code> The mutual information of the splits;
</p>
</li>
<li> <p><code>Hd</code> The entropy distance (variation of information) of the splits.
</p>
</li>
</ul>
<h3>Author(s)</h3>

<p><a href="https://orcid.org/0000-0001-5660-1727">Martin R. Smith</a>
(<a href="mailto:martin.smith@durham.ac.uk">martin.smith@durham.ac.uk</a>)
</p>


<h3>References</h3>

<p>MacKay DJC (2003).
<em>Information Theory, Inference, and Learning Algorithms</em>.
Cambridge University Press, Cambridge.
<a href="https://www.inference.org.uk/itprnn/book.pdf">https://www.inference.org.uk/itprnn/book.pdf</a>.<br><br> Meila M (2007).
“Comparing clusterings—an information based distance.”
<em>Journal of Multivariate Analysis</em>, <b>98</b>(5), 873–895.
<a href="https://doi.org/10.1016/j.jmva.2006.11.013">doi:10.1016/j.jmva.2006.11.013</a>.
</p>


<h3>See Also</h3>

<p>Other information functions: 
<code>SplitSharedInformation()</code>,
<code>TreeInfo</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">A &lt;- TRUE
B &lt;- FALSE
SplitEntropy(c(A, A, A, B, B, B), c(A, A, B, B, B, B))
</code></pre>


</div>