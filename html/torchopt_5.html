<div class="container">

<table style="width: 100%;"><tr>
<td>optim_adamw</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>AdamW optimizer</h2>

<h3>Description</h3>

<p>R implementation of the AdamW optimizer proposed
by Loshchilov &amp; Hutter (2019). We used the pytorch implementation
developed by Collin Donahue-Oponski available at:
https://gist.github.com/colllin/0b146b154c4351f9a40f741a28bff1e3
</p>
<p>From the abstract by the paper by Loshchilov &amp; Hutter (2019):
L2 regularization and weight decay regularization are equivalent for standard
stochastic gradient descent (when rescaled by the learning rate),
but as we demonstrate this is not the case for adaptive gradient algorithms,
such as Adam. While common implementations of these algorithms
employ L2 regularization (often calling it “weight decay”
in what may be misleading due to the inequivalence we expose),
we propose a simple modification to recover the original formulation of
weight decay regularization by decoupling the weight decay from the optimization
steps taken w.r.t. the loss function
</p>


<h3>Usage</h3>

<pre><code class="language-R">optim_adamw(
  params,
  lr = 0.01,
  betas = c(0.9, 0.999),
  eps = 1e-08,
  weight_decay = 1e-06
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>params</code></td>
<td>
<p>List of parameters to optimize.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lr</code></td>
<td>
<p>Learning rate (default: 1e-3)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>betas</code></td>
<td>
<p>Coefficients computing running averages of gradient
and its square (default: (0.9, 0.999))</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>Term added to the denominator to improve numerical
stability (default: 1e-8)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight_decay</code></td>
<td>
<p>Weight decay (L2 penalty) (default: 1e-6)</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A torch optimizer object implementing the <code>step</code> method.
</p>


<h3>Author(s)</h3>

<p>Gilberto Camara, <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a>
</p>
<p>Rolf Simoes, <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a>
</p>
<p>Felipe Souza, <a href="mailto:lipecaso@gmail.com">lipecaso@gmail.com</a>
</p>
<p>Alber Sanchez, <a href="mailto:alber.ipia@inpe.br">alber.ipia@inpe.br</a>
</p>


<h3>References</h3>

<p>Ilya Loshchilov, Frank Hutter,
"Decoupled Weight Decay Regularization",
International Conference on Learning Representations (ICLR) 2019.
https://arxiv.org/abs/1711.05101
</p>


<h3>Examples</h3>

<pre><code class="language-R">if (torch::torch_is_installed()) {
# function to demonstrate optimization
beale &lt;- function(x, y) {
    log((1.5 - x + x * y)^2 + (2.25 - x - x * y^2)^2 + (2.625 - x + x * y^3)^2)
 }
# define optimizer
optim &lt;- torchopt::optim_adamw
# define hyperparams
opt_hparams &lt;- list(lr = 0.01)

# starting point
x0 &lt;- 3
y0 &lt;- 3
# create tensor
x &lt;- torch::torch_tensor(x0, requires_grad = TRUE)
y &lt;- torch::torch_tensor(y0, requires_grad = TRUE)
# instantiate optimizer
optim &lt;- do.call(optim, c(list(params = list(x, y)), opt_hparams))
# run optimizer
steps &lt;- 400
x_steps &lt;- numeric(steps)
y_steps &lt;- numeric(steps)
for (i in seq_len(steps)) {
    x_steps[i] &lt;- as.numeric(x)
    y_steps[i] &lt;- as.numeric(y)
    optim$zero_grad()
    z &lt;- beale(x, y)
    z$backward()
    optim$step()
}
print(paste0("starting value = ", beale(x0, y0)))
print(paste0("final value = ", beale(x_steps[steps], y_steps[steps])))
}
</code></pre>


</div>