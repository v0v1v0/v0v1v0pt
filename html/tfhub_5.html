<div class="container">

<table style="width: 100%;"><tr>
<td>hub_sparse_text_embedding_column</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Module to construct dense representations from sparse text features.</h2>

<h3>Description</h3>

<p>The input to this feature column is a batch of multiple strings with
arbitrary size, assuming the input is a SparseTensor.
</p>


<h3>Usage</h3>

<pre><code class="language-R">hub_sparse_text_embedding_column(
  key,
  module_spec,
  combiner,
  default_value,
  trainable = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>key</code></td>
<td>
<p>A string or [feature_column](https://tensorflow.rstudio.com/tfestimators/articles/feature_columns.html)
identifying the text feature.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>module_spec</code></td>
<td>
<p>A string handle or a _ModuleSpec identifying the module.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>combiner</code></td>
<td>
<p>a string specifying reducing op for embeddings in the same Example.
Currently, 'mean', 'sqrtn', 'sum' are supported. Using 'combiner = NULL' is
undefined.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>default_value</code></td>
<td>
<p>default value for Examples where the text feature is empty.
Note, it's recommended to have default_value consistent OOV tokens, in case
there was special handling of OOV in the text module. If 'NULL', the text
feature is assumed be non-empty for each Example.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trainable</code></td>
<td>
<p>Whether or not the Module is trainable. 'FALSE' by default,
meaning the pre-trained weights are frozen. This is different from the ordinary
'tf.feature_column.embedding_column()', but that one is intended for training
from scratch.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This type of feature column is typically suited for modules that operate
on pre-tokenized text to produce token level embeddings which are combined
with the combiner into a text embedding. The combiner always treats the tokens
as a bag of words rather than a sequence.
</p>
<p>The output (i.e., transformed input layer) is a DenseTensor, with
shape [batch_size, num_embedding_dim].
</p>


</div>