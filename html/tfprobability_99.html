<div class="container">

<table style="width: 100%;"><tr>
<td>tfb_batch_normalization</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Computes<code>Y = g(X)</code> s.t. <code>X = g^-1(Y) = (Y - mean(Y)) / std(Y)</code>
</h2>

<h3>Description</h3>

<p>Applies Batch Normalization (Ioffe and Szegedy, 2015) to samples from a
data distribution. This can be used to stabilize training of normalizing
flows (Papamakarios et al., 2016; Dinh et al., 2017)
</p>


<h3>Usage</h3>

<pre><code class="language-R">tfb_batch_normalization(
  batchnorm_layer = NULL,
  training = TRUE,
  validate_args = FALSE,
  name = "batch_normalization"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>batchnorm_layer</code></td>
<td>
<p><code>tf$layers$BatchNormalization</code> layer object. If NULL, defaults to
<code>tf$layers$BatchNormalization(gamma_constraint=tf$nn$relu(x) + 1e-6)</code>.
This ensures positivity of the scale variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>training</code></td>
<td>
<p>If TRUE, updates running-average statistics during call to inverse().</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>validate_args</code></td>
<td>
<p>Logical, default FALSE. Whether to validate input with asserts. If validate_args is
FALSE, and the inputs are invalid, correct behavior is not guaranteed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>When training Deep Neural Networks (DNNs), it is common practice to
normalize or whiten features by shifting them to have zero mean and
scaling them to have unit variance.
</p>
<p>The <code>inverse()</code> method of the BatchNormalization bijector, which is used in
the log-likelihood computation of data samples, implements the normalization
procedure (shift-and-scale) using the mean and standard deviation of the
current minibatch.
</p>
<p>Conversely, the <code>forward()</code> method of the bijector de-normalizes samples (e.g.
<code>X*std(Y) + mean(Y)</code> with the running-average mean and standard deviation
computed at training-time. De-normalization is useful for sampling.
</p>
<p>During training time, BatchNormalization.inverse and BatchNormalization.forward are not
guaranteed to be inverses of each other because <code>inverse(y)</code> uses statistics of the current minibatch,
while <code>forward(x)</code> uses running-average statistics accumulated from training.
In other words, <code>tfb_batch_normalization()$inverse(tfb_batch_normalization()$forward(...))</code> and
<code>tfb_batch_normalization()$forward(tfb_batch_normalization()$inverse(...))</code> will be identical when
training=FALSE but may be different when training=TRUE.
</p>


<h3>Value</h3>

<p>a bijector instance.
</p>


<h3>References</h3>


<ul>
<li> <p><a href="https://arxiv.org/abs/1502.03167">Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In <em>International Conference on Machine Learning</em>, 2015.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1605.08803">Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density Estimation using Real NVP. In <em>International Conference on Learning Representations</em>, 2017.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1705.07057">George Papamakarios, Theo Pavlakou, and Iain Murray. Masked Autoregressive Flow for Density Estimation. In <em>Neural Information Processing Systems</em>, 2017.</a>
</p>
</li>
</ul>
<h3>See Also</h3>

<p>For usage examples see <code>tfb_forward()</code>, <code>tfb_inverse()</code>, <code>tfb_inverse_log_det_jacobian()</code>.
</p>
<p>Other bijectors: 
<code>tfb_absolute_value()</code>,
<code>tfb_affine_linear_operator()</code>,
<code>tfb_affine_scalar()</code>,
<code>tfb_affine()</code>,
<code>tfb_ascending()</code>,
<code>tfb_blockwise()</code>,
<code>tfb_chain()</code>,
<code>tfb_cholesky_outer_product()</code>,
<code>tfb_cholesky_to_inv_cholesky()</code>,
<code>tfb_correlation_cholesky()</code>,
<code>tfb_cumsum()</code>,
<code>tfb_discrete_cosine_transform()</code>,
<code>tfb_expm1()</code>,
<code>tfb_exp()</code>,
<code>tfb_ffjord()</code>,
<code>tfb_fill_scale_tri_l()</code>,
<code>tfb_fill_triangular()</code>,
<code>tfb_glow()</code>,
<code>tfb_gompertz_cdf()</code>,
<code>tfb_gumbel_cdf()</code>,
<code>tfb_gumbel()</code>,
<code>tfb_identity()</code>,
<code>tfb_inline()</code>,
<code>tfb_invert()</code>,
<code>tfb_iterated_sigmoid_centered()</code>,
<code>tfb_kumaraswamy_cdf()</code>,
<code>tfb_kumaraswamy()</code>,
<code>tfb_lambert_w_tail()</code>,
<code>tfb_masked_autoregressive_default_template()</code>,
<code>tfb_masked_autoregressive_flow()</code>,
<code>tfb_masked_dense()</code>,
<code>tfb_matrix_inverse_tri_l()</code>,
<code>tfb_matvec_lu()</code>,
<code>tfb_normal_cdf()</code>,
<code>tfb_ordered()</code>,
<code>tfb_pad()</code>,
<code>tfb_permute()</code>,
<code>tfb_power_transform()</code>,
<code>tfb_rational_quadratic_spline()</code>,
<code>tfb_rayleigh_cdf()</code>,
<code>tfb_real_nvp_default_template()</code>,
<code>tfb_real_nvp()</code>,
<code>tfb_reciprocal()</code>,
<code>tfb_reshape()</code>,
<code>tfb_scale_matvec_diag()</code>,
<code>tfb_scale_matvec_linear_operator()</code>,
<code>tfb_scale_matvec_lu()</code>,
<code>tfb_scale_matvec_tri_l()</code>,
<code>tfb_scale_tri_l()</code>,
<code>tfb_scale()</code>,
<code>tfb_shifted_gompertz_cdf()</code>,
<code>tfb_shift()</code>,
<code>tfb_sigmoid()</code>,
<code>tfb_sinh_arcsinh()</code>,
<code>tfb_sinh()</code>,
<code>tfb_softmax_centered()</code>,
<code>tfb_softplus()</code>,
<code>tfb_softsign()</code>,
<code>tfb_split()</code>,
<code>tfb_square()</code>,
<code>tfb_tanh()</code>,
<code>tfb_transform_diagonal()</code>,
<code>tfb_transpose()</code>,
<code>tfb_weibull_cdf()</code>,
<code>tfb_weibull()</code>
</p>


</div>