<div class="container">

<table style="width: 100%;"><tr>
<td>optimizer_decay_adamw</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Optimizer that implements the Adam algorithm with weight decay</h2>

<h3>Description</h3>

<p>This is an implementation of the AdamW optimizer described in "Decoupled Weight Decay Regularization"
by Loshchilov &amp; Hutter (https://arxiv.org/abs/1711.05101) ([pdf])(https://arxiv.org/pdf/1711.05101.pdf). It computes
the update step of tf.keras.optimizers.Adam and additionally decays the variable. Note that this is different
from adding L2 regularization on the variables to the loss: it regularizes variables with large gradients more than
L2 regularization would, which was shown to yield better training loss and generalization error in the paper above.
</p>


<h3>Usage</h3>

<pre><code class="language-R">optimizer_decay_adamw(
  weight_decay,
  learning_rate = 0.001,
  beta_1 = 0.9,
  beta_2 = 0.999,
  epsilon = 1e-07,
  amsgrad = FALSE,
  name = "AdamW",
  clipnorm = NULL,
  clipvalue = NULL,
  decay = NULL,
  lr = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>weight_decay</code></td>
<td>
<p>A Tensor or a floating point value. The weight decay.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learning_rate</code></td>
<td>
<p>A Tensor or a floating point value. The learning rate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta_1</code></td>
<td>
<p>A float value or a constant float tensor. The exponential decay rate for the 1st moment estimates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta_2</code></td>
<td>
<p>A float value or a constant float tensor. The exponential decay rate for the 2nd moment estimates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>A small constant for numerical stability. This epsilon is "epsilon hat" in
the Kingma and Ba paper (in the formula just before Section 2.1),
not the epsilon in Algorithm 1 of the paper.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>amsgrad</code></td>
<td>
<p>boolean. Whether to apply AMSGrad variant of this algorithm from the paper
"On the Convergence of Adam and beyond".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>name</code></td>
<td>
<p>Optional name for the operations created when applying</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>clipnorm</code></td>
<td>
<p>is clip gradients by norm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>clipvalue</code></td>
<td>
<p>is clip gradients by value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>decay</code></td>
<td>
<p>is included for backward compatibility to allow time inverse decay of learning rate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lr</code></td>
<td>
<p>is included for backward compatibility, recommended to use learning_rate instead.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Optimizer for use with 'keras::compile()'
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## Not run: 

step = tf$Variable(0L, trainable = FALSE)
schedule = tf$optimizers$schedules$PiecewiseConstantDecay(list(c(10000, 15000)),
list(c(1e-0, 1e-1, 1e-2)))
lr = 1e-1 * schedule(step)
wd = lambda: 1e-4 * schedule(step)


## End(Not run)

</code></pre>


</div>