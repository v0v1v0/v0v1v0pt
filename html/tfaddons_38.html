<div class="container">

<table style="width: 100%;"><tr>
<td>extend_with_decoupled_weight_decay</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Factory function returning an optimizer class with decoupled weight decay</h2>

<h3>Description</h3>

<p>Factory function returning an optimizer class with decoupled weight decay
</p>


<h3>Usage</h3>

<pre><code class="language-R">extend_with_decoupled_weight_decay(base_optimizer)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>base_optimizer</code></td>
<td>
<p>An optimizer class that inherits from tf$optimizers$Optimizer.</p>
</td>
</tr></table>
<h3>Details</h3>

<p>The API of the new optimizer class slightly differs from the API of the base optimizer:
</p>
<p>- The first argument to the constructor is the weight decay rate.
- minimize and apply_gradients accept the optional keyword argument decay_var_list,
which specifies the variables that should be decayed. If NULLs, all variables that are optimized are decayed.
</p>


<h3>Value</h3>

<p>A new optimizer class that inherits from DecoupledWeightDecayExtension and base_optimizer.
</p>


<h3>Note</h3>

<p>Note: this extension decays weights BEFORE applying the update based
on the gradient, i.e. this extension only has the desired behaviour for
optimizers which do not depend on the value of 'var' in the update step!
Note: when applying a decay to the learning rate, be sure to manually apply
the decay to the 'weight_decay' as well.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## Not run: 

### MyAdamW is a new class
MyAdamW = extend_with_decoupled_weight_decay(tf$keras$optimizers$Adam)
### Create a MyAdamW object
optimizer = MyAdamW(weight_decay = 0.001, learning_rate = 0.001)
#### update var1, var2 but only decay var1
optimizer$minimize(loss, var_list = list(var1, var2), decay_variables = list(var1))


## End(Not run)

</code></pre>


</div>