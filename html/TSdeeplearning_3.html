<div class="container">

<table style="width: 100%;"><tr>
<td>LST_ts</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Long- Short Term Memory Model
</h2>

<h3>Description</h3>

<p>The LSTM function computes forecasted value with different forecasting evaluation criteria for long- short term memory  model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">LSTM_ts(xt, xtlag = 4, uLSTM = 2, Drate = 0, nEpochs = 10,
Loss = "mse", AccMetrics = "mae",ActFn = "tanh",
Split = 0.8, Valid = 0.1)</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>xt</code></td>
<td>

<p>Input univariate time series (ts) data.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xtlag</code></td>
<td>

<p>Lag of time series data.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>uLSTM</code></td>
<td>

<p>Number of unit in LSTM layer.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Drate</code></td>
<td>

<p>Dropout rate.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nEpochs</code></td>
<td>

<p>Number of epochs.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Loss</code></td>
<td>

<p>Loss function.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>AccMetrics</code></td>
<td>

<p>Metrics.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ActFn</code></td>
<td>

<p>Activation function.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Split</code></td>
<td>

<p>Index of the split point and separates the data into the training and testing datasets.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Valid</code></td>
<td>

<p>Validation set.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) based RNN is designed to overcome the vanishing gradients problem while dealing with long term dependencies. In contrast to standard RNN, LSTM has this peculiar and unique inbuilt ability by maintaining a memory cell to determine which unimportant features should be forgotten and which important features should be remembered during the learning process (Jaiswal et al., 2022). An LSTM model analyses and captures both short-term and long-term temporal dependencies of a complex time series effectively due to its architecture of recurrent neural network and the memory function used in the hidden nodes.
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>TrainFittedValue </code></td>
<td>
<p>Training Fitted value for given time series data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TestPredictedValue</code></td>
<td>
<p>Final forecasted value of the LSTM model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fcast_criteria </code></td>
<td>
<p>Different Forecasting evaluation criteria for LSTM model.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Cho, K., Van MerriÃ«nboer, B., Bahdanau, D. and Bengio, Y. (2014). On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.
</p>


<h3>See Also</h3>

<p>GRU, RNN
</p>


<h3>Examples</h3>

<pre><code class="language-R">
data("Data_Maize")
LSTM_ts(Data_Maize)

</code></pre>


</div>