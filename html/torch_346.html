<div class="container">

<table style="width: 100%;"><tr>
<td>optim_adamw</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Implements AdamW algorithm</h2>

<h3>Description</h3>

<p>For further details regarding the algorithm we refer to
<a href="https://arxiv.org/abs/1711.05101">Decoupled Weight Decay Regularization</a>
</p>


<h3>Usage</h3>

<pre><code class="language-R">optim_adamw(
  params,
  lr = 0.001,
  betas = c(0.9, 0.999),
  eps = 1e-08,
  weight_decay = 0.01,
  amsgrad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>params</code></td>
<td>
<p>(iterable): iterable of parameters to optimize or dicts defining
parameter groups</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lr</code></td>
<td>
<p>(float, optional): learning rate (default: 1e-3)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>betas</code></td>
<td>
<p>(<code>Tuple[float, float]</code>, optional): coefficients used for computing
running averages of gradient and its square (default: (0.9, 0.999))</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>(float, optional): term added to the denominator to improve
numerical stability (default: 1e-8)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight_decay</code></td>
<td>
<p>(float, optional): weight decay (L2 penalty) (default: 0)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>amsgrad</code></td>
<td>
<p>(boolean, optional): whether to use the AMSGrad variant of this
algorithm from the paper <a href="https://openreview.net/forum?id=ryQu7f-RZ">On the Convergence of Adam and Beyond</a>
(default: FALSE)</p>
</td>
</tr>
</table>
</div>