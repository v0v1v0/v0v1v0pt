<div class="container">

<table style="width: 100%;"><tr>
<td>nn_dropout</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Dropout module</h2>

<h3>Description</h3>

<p>During training, randomly zeroes some of the elements of the input
tensor with probability <code>p</code> using samples from a Bernoulli
distribution. Each channel will be zeroed out independently on every forward
call.
</p>


<h3>Usage</h3>

<pre><code class="language-R">nn_dropout(p = 0.5, inplace = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>p</code></td>
<td>
<p>probability of an element to be zeroed. Default: 0.5</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>inplace</code></td>
<td>
<p>If set to <code>TRUE</code>, will do this operation in-place. Default: <code>FALSE</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This has proven to be an effective technique for regularization and
preventing the co-adaptation of neurons as described in the paper
<a href="https://arxiv.org/abs/1207.0580">Improving neural networks by preventing co-adaptation of feature detectors</a>.
</p>
<p>Furthermore, the outputs are scaled by a factor of :math:<code style="white-space: pre;">⁠\frac{1}{1-p}⁠</code> during
training. This means that during evaluation the module simply computes an
identity function.
</p>


<h3>Shape</h3>


<ul>
<li>
<p> Input: <code class="reqn">(*)</code>. Input can be of any shape
</p>
</li>
<li>
<p> Output: <code class="reqn">(*)</code>. Output is of the same shape as input
</p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R">if (torch_is_installed()) {
m &lt;- nn_dropout(p = 0.2)
input &lt;- torch_randn(20, 16)
output &lt;- m(input)
}
</code></pre>


</div>