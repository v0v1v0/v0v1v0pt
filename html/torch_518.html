<div class="container">

<table style="width: 100%;"><tr>
<td>torch_hamming_window</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Hamming_window</h2>

<h3>Description</h3>

<p>Hamming_window
</p>


<h3>Usage</h3>

<pre><code class="language-R">torch_hamming_window(
  window_length,
  periodic = TRUE,
  alpha = 0.54,
  beta = 0.46,
  dtype = NULL,
  layout = NULL,
  device = NULL,
  requires_grad = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>window_length</code></td>
<td>
<p>(int) the size of returned window</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>periodic</code></td>
<td>
<p>(bool, optional) If TRUE, returns a window to be used as periodic        function. If False, return a symmetric window.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>(float, optional) The coefficient <code class="reqn">\alpha</code> in the equation above</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>(float, optional) The coefficient <code class="reqn">\beta</code> in the equation above</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dtype</code></td>
<td>
<p>(<code>torch.dtype</code>, optional) the desired data type of returned tensor.        Default: if <code>NULL</code>, uses a global default (see <code>torch_set_default_tensor_type</code>). Only floating point types are supported.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>layout</code></td>
<td>
<p>(<code>torch.layout</code>, optional) the desired layout of returned window tensor. Only          <code>torch_strided</code> (dense layout) is supported.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>device</code></td>
<td>
<p>(<code>torch.device</code>, optional) the desired device of returned tensor.        Default: if <code>NULL</code>, uses the current device for the default tensor type        (see <code>torch_set_default_tensor_type</code>). <code>device</code> will be the CPU        for CPU tensor types and the current CUDA device for CUDA tensor types.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>requires_grad</code></td>
<td>
<p>(bool, optional) If autograd should record operations on the        returned tensor. Default: <code>FALSE</code>.</p>
</td>
</tr>
</table>
<h3>hamming_window(window_length, periodic=TRUE, alpha=0.54, beta=0.46, dtype=NULL, layout=torch.strided, device=NULL, requires_grad=False) -&gt; Tensor </h3>

<p>Hamming window function.
</p>
<p style="text-align: center;"><code class="reqn">
    w[n] = \alpha - \beta\ \cos \left( \frac{2 \pi n}{N - 1} \right),
</code>
</p>

<p>where <code class="reqn">N</code> is the full window size.
</p>
<p>The input <code>window_length</code> is a positive integer controlling the
returned window size. <code>periodic</code> flag determines whether the returned
window trims off the last duplicate value from the symmetric window and is
ready to be used as a periodic window with functions like
<code>torch_stft</code>. Therefore, if <code>periodic</code> is true, the <code class="reqn">N</code> in
above formula is in fact <code class="reqn">\mbox{window\_length} + 1</code>. Also, we always have
<code>torch_hamming_window(L, periodic=TRUE)</code> equal to
<code style="white-space: pre;">⁠torch_hamming_window(L + 1, periodic=False)[:-1])⁠</code>.
</p>


<h3>Note</h3>

<div class="sourceCode"><pre>If `window_length` \eqn{=1}, the returned window contains a single value 1.
</pre></div>
<div class="sourceCode"><pre>This is a generalized version of `torch_hann_window`.
</pre></div>


</div>