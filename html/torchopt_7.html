<div class="container">

<table style="width: 100%;"><tr>
<td>optim_nadam</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Nadam optimizer</h2>

<h3>Description</h3>

<p>R implementation of the Nadam optimizer proposed
by Dazat (2016).
</p>
<p>From the abstract by the paper by Dozat (2016):
This work aims to improve upon the recently proposed and
rapidly popularized optimization algorithm Adam (Kingma &amp; Ba, 2014).
Adam has two main components—a momentum component and an adaptive
learning rate component. However, regular momentum can be shown conceptually
and empirically to be inferior to a similar algorithm known as
Nesterov’s accelerated gradient (NAG).
</p>


<h3>Usage</h3>

<pre><code class="language-R">optim_nadam(
  params,
  lr = 0.002,
  betas = c(0.9, 0.999),
  eps = 1e-08,
  weight_decay = 0,
  momentum_decay = 0.004
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>params</code></td>
<td>
<p>List of parameters to optimize.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lr</code></td>
<td>
<p>Learning rate (default: 1e-3)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>betas</code></td>
<td>
<p>Coefficients computing running averages of gradient
and its square (default: (0.9, 0.999)).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>Term added to the denominator to improve numerical
stability (default: 1e-8).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight_decay</code></td>
<td>
<p>Weight decay (L2 penalty) (default: 0).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>momentum_decay</code></td>
<td>
<p>Momentum_decay (default: 4e-3).</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A torch optimizer object implementing the <code>step</code> method.
</p>


<h3>Author(s)</h3>

<p>Gilberto Camara, <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a>
</p>
<p>Rolf Simoes, <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a>
</p>
<p>Felipe Souza, <a href="mailto:lipecaso@gmail.com">lipecaso@gmail.com</a>
</p>
<p>Alber Sanchez, <a href="mailto:alber.ipia@inpe.br">alber.ipia@inpe.br</a>
</p>


<h3>References</h3>

<p>Timothy Dozat,
"Incorporating Nesterov Momentum into Adam",
International Conference on Learning Representations (ICLR) 2016.
https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf
</p>


<h3>Examples</h3>

<pre><code class="language-R">if (torch::torch_is_installed()) {
# function to demonstrate optimization
beale &lt;- function(x, y) {
    log((1.5 - x + x * y)^2 + (2.25 - x - x * y^2)^2 + (2.625 - x + x * y^3)^2)
 }
# define optimizer
optim &lt;- torchopt::optim_nadam
# define hyperparams
opt_hparams &lt;- list(lr = 0.01)

# starting point
x0 &lt;- 3
y0 &lt;- 3
# create tensor
x &lt;- torch::torch_tensor(x0, requires_grad = TRUE)
y &lt;- torch::torch_tensor(y0, requires_grad = TRUE)
# instantiate optimizer
optim &lt;- do.call(optim, c(list(params = list(x, y)), opt_hparams))
# run optimizer
steps &lt;- 400
x_steps &lt;- numeric(steps)
y_steps &lt;- numeric(steps)
for (i in seq_len(steps)) {
    x_steps[i] &lt;- as.numeric(x)
    y_steps[i] &lt;- as.numeric(y)
    optim$zero_grad()
    z &lt;- beale(x, y)
    z$backward()
    optim$step()
}
print(paste0("starting value = ", beale(x0, y0)))
print(paste0("final value = ", beale(x_steps[steps], y_steps[steps])))
}
</code></pre>


</div>