<div class="container">

<table style="width: 100%;"><tr>
<td>tucker</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Tucker Decomposition</h2>

<h3>Description</h3>

<p>The Tucker decomposition of a tensor. Approximates a K-Tensor using a n-mode product of a core tensor (with modes specified by <code>ranks</code>) with orthogonal factor matrices. If there is no truncation in all the modes (i.e. <code>ranks = tnsr@modes</code>), then this is the same as the HOSVD, <code>hosvd</code>. This is an iterative algorithm, with two possible stopping conditions: either relative error in Frobenius norm has gotten below <code>tol</code>, or the <code>max_iter</code> number of iterations has been reached. For more details on the Tucker decomposition, consult Kolda and Bader (2009).
</p>


<h3>Usage</h3>

<pre><code class="language-R">tucker(tnsr, ranks = NULL, max_iter = 25, tol = 1e-05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>tnsr</code></td>
<td>
<p>Tensor with K modes</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ranks</code></td>
<td>
<p>a vector of the modes of the output core Tensor</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter</code></td>
<td>
<p>maximum number of iterations if error stays above <code>tol</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>relative Frobenius norm error tolerance</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Uses the Alternating Least Squares (ALS) estimation procedure also known as Higher-Order Orthogonal Iteration (HOOI). Intialized using a (Truncated-)HOSVD. A progress bar is included to help monitor operations on large tensors.
</p>


<h3>Value</h3>

<p>a list containing the following:</p>

<dl>
<dt><code>Z</code></dt>
<dd>
<p>the core tensor, with modes specified by <code>ranks</code></p>
</dd>
<dt><code>U</code></dt>
<dd>
<p>a list of orthgonal factor matrices - one for each mode, with the number of columns of the matrices given by <code>ranks</code></p>
</dd>
<dt><code>conv</code></dt>
<dd>
<p>whether or not <code>resid</code> &lt; <code>tol</code> by the last iteration</p>
</dd>
<dt><code>est</code></dt>
<dd>
<p>estimate of <code>tnsr</code> after compression</p>
</dd>
<dt><code>norm_percent</code></dt>
<dd>
<p>the percent of Frobenius norm explained by the approximation</p>
</dd>
<dt><code>fnorm_resid</code></dt>
<dd>
<p>the Frobenius norm of the error <code>fnorm(est-tnsr)</code></p>
</dd>
<dt><code>all_resids</code></dt>
<dd>
<p>vector containing the Frobenius norm of error for all the iterations</p>
</dd>
</dl>
<h3>Note</h3>

<p>The length of <code>ranks</code> must match <code>tnsr@num_modes</code>.
</p>


<h3>References</h3>

<p>T. Kolda, B. Bader, "Tensor decomposition and applications". SIAM Applied Mathematics and Applications 2009, Vol. 51, No. 3 (September 2009), pp. 455-500. URL: https://www.jstor.org/stable/25662308
</p>


<h3>See Also</h3>

<p><code>hosvd</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">tnsr &lt;- rand_tensor(c(4,4,4,4))
tuckerD &lt;- tucker(tnsr,ranks=c(2,2,2,2))
tuckerD$conv 
tuckerD$norm_percent
plot(tuckerD$all_resids)
</code></pre>


</div>