<div class="container">

<table style="width: 100%;"><tr>
<td>tensorA-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>The tensorA package for tensor arithmetic</h2>

<h3>Description</h3>

<p>tensorA stands for "tensor arithmetic". A tensor is a mathematical
generalization of vector and matrix with
many applications in physics, geometry and in the statistics of
vectors valued data. However the package is also useful in any case,
where computations on sequences of matrices, vectors or even tensors
is involved. 
</p>


<h3>Details</h3>


<table>
<tr>
<td style="text-align: left;">
    Package: </td>
<td style="text-align: left;"> tensorA</td>
</tr>
<tr>
<td style="text-align: left;">
    Type: </td>
<td style="text-align: left;"> Package</td>
</tr>
<tr>
<td style="text-align: left;">
    Version: </td>
<td style="text-align: left;"> 0.1</td>
</tr>
<tr>
<td style="text-align: left;">
    Date: </td>
<td style="text-align: left;"> 2006-06-08</td>
</tr>
<tr>
<td style="text-align: left;">
    License: </td>
<td style="text-align: left;"> GPL Version 2 or newer</td>
</tr>
<tr>
<td style="text-align: left;">
  </td>
</tr>
</table>
<p>The tensorA package is made to allow programming for tensors in R on
the same level of abstraction as we know from matrices. It
provides many of the
mathematical operations common in tensor arithmetics including the
whole tensor calculus of covariate and contravariate indices, naming
of indices, sequence of indices, decompositions of tensors, Einstein
and Riemann summing conventions and vectorized computations on datasets
of tensors just like the well vectorization of numbers in R. It
provides tools to write tensor formulae very close to there paper form
and to handle tensors of arbitrary level with simple programs.
<br>
The whole documentation of the package is best read in
pdf or dvi format since it contains complicated mathematical formulae
with multi-indices.
<br></p>
<p>Simply speaking a tensor (see <code>to.tensor</code>) is just a
multidimensional array
<code>A[,,]</code>. The number
of indices (i.e. <code>length(dim(A))</code> is called the level of the
tensor (see <code>level.tensor</code>). A tensor is mathematically it
is denoted
by a core symbol (e.g. A) with multiple indices:e.g.
</p>
<p style="text-align: center;"><code class="reqn">A_{ijk}</code>
</p>

<p>The indices <code class="reqn">i,j,k</code> can be seen as names for the dimensions
and as integer numbers giving the respective index into the array.
However the tensor is an algebraical object with many algebraical
operations defined on it, which are also of relevancy for programming,
e.g. in the parallel treatment of multiple linear equation systems.
</p>
<p>To understand the package we need to understand tensors including
their mathematical origin, the corresponding calculus, notation and
basic
operations.
<br>
One mathematical interpretation of a tensor, which is most relevant
for physics, that of a multi-linear
form of <code class="reqn">level(A)</code> vectors, i.e. a function of <code class="reqn">level(A)</code> many
vectors
to the real or complex numbers, which is linear with respect to each
of its arguments. E.g. the two vectors "plane face direction" and "force
direction" are mapped to the actual force by the stress tensor.
<br>
Row vectors are a special case of that and likewise column vectors as
linear forms for row vectors. Matrices are bilinear forms of a row
vector and a column vector. Thus Vectors and Matrices are examples of
tensors of level 1 and 2. 
</p>
<p>Another interpretation of a tensor is the that of a linear mapping,
quite like a matrix, but from a tensor space (e.g. the space of
matrices or vectors seen as tensor) to another tensor space
(e.g. again a space of matrices). An
example for that is the Hook elasticity tensor mapping the strain
tensor (i.e. a
matrix describing the local deformation) to the stress tensor (i.e. a
matrix describing the local forces). The Hook tensor is a tensor of
level 4. Statistically relevant tensors of level 4 are
e.g. covariances of matrices mapping two linear forms (i.e. 2 level 2
tensors) on observed
matrices to there covariance.  The mapping is performed with the
tensor product, which is not unlike a matrix product, however more
general. Let denote <code class="reqn">A</code> a matrix and <code class="reqn">v</code> a vector, we
would write <code class="reqn">r=Ab</code> for the matrix product and <code>r &lt;- A%*%b</code> in R,
which is defined as:
</p>
<p style="text-align: center;"><code class="reqn">r_i = \sum_{j=1}^{j_{\max}} A_{ij}b_j </code>
</p>

<p>We know that we have to use the \(j\)-dimension in the summing, since
the matrix multiplication rule says "row times column". 
Since a tensor can have more than two indices there is no row or
column specified and we need to specify the inner product
differently. To do this in
the Einstein-Notation writing the tensor always with indices
<code class="reqn">r_i=A_{ij}b_j</code> and according to the Einstein summing rule the
entries of \(r_i\) are given by an implicit sum over all indices which
show
up twice in this notation:
</p>
<p style="text-align: center;"><code class="reqn"> r_i=\sum_{j=1}^{j_{\max}} A_{ij}b_j </code>
</p>

<p>This notation allows for a multitude of other products:
<code class="reqn"> A_{ij}b_i=t(A)b </code>, <code class="reqn"> A_{ij}b_k=outer(A,b)
  </code> ,
<code class="reqn"> A_{ii}b_j=trace(A)b </code> with equal
simplicity and without any
additional functions. 
More complicated products involving more than tensors of level two
can not even be formulated in pure matrix algebra without
re-dimensioning of arrays e.g. <code class="reqn">b_ib_jb_k</code>,
<code class="reqn">A_{ijk}b_j</code>. The
Einstein summing rule is implemented in
<code>einstein.tensor</code> and supported by the index sequencing
functions <code>$.tensor</code> and <code>|.tensor</code>. A general
multiplication allowing to
identify and sum over any two indices is implemented in
<code>trace.tensor</code>, when the indices are in the same tensor
and in <code>mul.tensor</code>, when the indices to sum over are in
different tensors. 
<br>
Tensors with the same level and dimensions (identified by name
and dimension) can also be added like matrices according to the rule
that the values with the same combination of index values are added
(see <code>add.tensor</code>). The implementation takes care of the
sequence of the indices and rearranges them accordingly to match
dimensions with the same name. E.g. the tensor addition
</p>
<p style="text-align: center;"><code class="reqn">E_ijk=A_{ijk}+B_{kji}</code>
</p>
 
<p>has the effect, which is expressed by the same formula read in
entries, which is also true for the more surprising
</p>
<p style="text-align: center;"><code class="reqn">E_ijk=A_{ij}+B_{kj}</code>
</p>
 
<p><br>
Like a matrix a tensor can also be seen as a mapping from one tensor
space to another:
</p>
<p style="text-align: center;"><code class="reqn">A_{i_1\ldots i_d j_1 \ldots j_e}x_{j_1 \ldots j_e}=b_{i_1\ldots
      i_d}</code>
</p>

<p>In this reading all the standard matrix computations and
decompositions get a tensorial interpretation and generalization. The
package provides some of these (see <code>svd.tensor</code>). 
<br>
Another interpretation of tensors is as a sequence of tensors of lower
level. E.g. a data matrix is seen as a sequence of vectors in
multivariate dataset. The tensorA library provides means to do
computation on these in parallel on these sequences of tensors like we
can do parallel computation on sequences of numbers. This is typically
done by the <code>by=</code> argument present in most functions and giving
the index enumerating the elements of the sequence.<br>
E.g. If we have
sequence <code class="reqn">V_{ijd}</code> of variance matrices <code class="reqn">V_{ij}</code> of some sequence
<code class="reqn">v_{id}</code> of vectors and we would like to transform the vectors
with some Matrix <code class="reqn">M_{i'i}</code> we would get the sequence of
transformed variances
by <code class="reqn">V_{ijd} M_{i'i}M_{j'j}</code>.
However if the <code class="reqn">M_{ki}</code> are different for each of the elements
in sequence we would have stored them in a tensor <code class="reqn">M_{kid}</code> and
would have to replace  <code class="reqn">M_{kid}</code> with <code class="reqn">M_{kidd'}=M_{kid}</code> if
<code class="reqn">d=d'</code> and zero otherwise. We can than get our result by
</p>
<p style="text-align: center;"><code class="reqn">V_{ijd}M_{i'id'd}M_{j'jd'd''}</code>
</p>
<p> and we would have a by dimension
of
<code>by="d"</code>. These operations are not strictly
mathematical tensor operation, but generalizations of the
vectorization approach of R. This is also closely related to
<code>diagmul.tensor</code> or <code>diag.tensor</code>.   
<br>
To complicate things the Einstein rule is only valid in case of
tensors represented with respect to a orthogonal basis. Otherwise
tensors get lower and upper indices like
</p>
<p style="text-align: center;"><code class="reqn">A_{i\cdot k}^{\cdot j \cdot}</code>
</p>

<p>for representation in the covariate and contravariate form of the
basis. In this case the Riemann summing rule applies which only sums
over pairs of the same index, where one is in the lower and one is in
the upper position. The contravariate form is represented with indices
prefixed by <code>^</code>. 
<br> 
The state of being covariate or contravariate can be changed by the
dragging rule, which allows to switch between both state through the
multiplication with the geometry tensors <code class="reqn">g_i^{\;j}</code>. This
can be done through <code>drag.tensor</code>.
</p>


<h3>Author(s)</h3>

<p>K.Gerald van den Boogaart &lt;boogaart@uni-greifswald.de</p>


<h3>See Also</h3>

<p><code>to.tensor</code>, <code>mul.tensor</code> ,
<code>einstein.tensor</code>, <code>add.tensor</code>,
<code>[[.tensor</code>, <code>|.tensor</code> 
</p>


<h3>Examples</h3>

<pre><code class="language-R">A &lt;- to.tensor( 1:20, c(a=2,b=2,c=5) )
A
ftable(A)
B &lt;- to.tensor( c(0,1,1,0) , c(a=2,"a'"=2))
A %e% B
drag.tensor( A , B, c("a","b"))
A %e% one.tensor(c(c=5))/5     # a mean of matrices
reorder.tensor(A,c("c","b","a"))
A -  reorder.tensor(A,c("c","b","a"))  # =0 since sequence is irrelevant
inv.tensor(A,"a",by="c")  

</code></pre>


</div>