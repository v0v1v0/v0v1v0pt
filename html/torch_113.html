<div class="container">

<table style="width: 100%;"><tr>
<td>lr_step</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Step learning rate decay</h2>

<h3>Description</h3>

<p>Decays the learning rate of each parameter group by gamma every
step_size epochs. Notice that such decay can happen simultaneously with
other changes to the learning rate from outside this scheduler. When
last_epoch=-1, sets initial lr as lr.
</p>


<h3>Usage</h3>

<pre><code class="language-R">lr_step(optimizer, step_size, gamma = 0.1, last_epoch = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>optimizer</code></td>
<td>
<p>(Optimizer): Wrapped optimizer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>step_size</code></td>
<td>
<p>(int): Period of learning rate decay.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>(float): Multiplicative factor of learning rate decay.
Default: 0.1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>last_epoch</code></td>
<td>
<p>(int): The index of last epoch. Default: -1.</p>
</td>
</tr>
</table>
<h3>Examples</h3>

<pre><code class="language-R">if (torch_is_installed()) {
## Not run: 
# Assuming optimizer uses lr = 0.05 for all groups
# lr = 0.05     if epoch &lt; 30
# lr = 0.005    if 30 &lt;= epoch &lt; 60
# lr = 0.0005   if 60 &lt;= epoch &lt; 90
# ...
scheduler &lt;- lr_step(optimizer, step_size = 30, gamma = 0.1)
for (epoch in 1:100) {
  train(...)
  validate(...)
  scheduler$step()
}

## End(Not run)

}
</code></pre>


</div>