<div class="container">

<table style="width: 100%;"><tr>
<td>textEmbedLayerAggregation</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Select and aggregate layers of hidden states to form a word embedding.</h2>

<h3>Description</h3>

<p>Select and aggregate layers of hidden states to form a word embedding.
</p>


<h3>Usage</h3>

<pre><code class="language-R">textEmbedLayerAggregation(
  word_embeddings_layers,
  layers = "all",
  aggregation_from_layers_to_tokens = "concatenate",
  aggregation_from_tokens_to_texts = "mean",
  return_tokens = FALSE,
  tokens_select = NULL,
  tokens_deselect = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>word_embeddings_layers</code></td>
<td>
<p>Layers returned by the textEmbedRawLayers function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>layers</code></td>
<td>
<p>(character or numeric) The numbers of the layers to be aggregated
(e.g., c(11:12) to aggregate the eleventh and twelfth).
Note that layer 0 is the input embedding to the transformer, and should normally not be used.
Selecting 'all' thus removes layer 0 (default = "all")</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>aggregation_from_layers_to_tokens</code></td>
<td>
<p>(character) Method to carry out the aggregation among
the layers for each word/token, including "min", "max" and "mean" which takes the minimum,
maximum or mean across each column; or "concatenate", which links together each layer of the
word embedding to one long row (default = "concatenate").</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>aggregation_from_tokens_to_texts</code></td>
<td>
<p>(character) Method to carry out the aggregation among the word embeddings
for the words/tokens, including "min", "max" and "mean" which takes the minimum, maximum or mean across each column;
or "concatenate", which links together each layer of the word embedding to one long row (default = "mean").</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>return_tokens</code></td>
<td>
<p>(boolean) If TRUE, provide the tokens used in the specified transformer model (default = FALSE).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tokens_select</code></td>
<td>
<p>(character) Option to only select embeddings linked to specific tokens
in the textEmbedLayerAggregation() phase such as "[CLS]" and "[SEP]" (default NULL).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tokens_deselect</code></td>
<td>
<p>(character) Option to deselect embeddings linked to specific tokens in
the textEmbedLayerAggregation() phase such as "[CLS]" and "[SEP]" (default NULL).</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A tibble with word embeddings. Note that layer 0 is the input embedding to
the transformer, which is normally not used.
</p>


<h3>See Also</h3>

<p>See <code>textEmbedRawLayers</code> and <code>textEmbed</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Aggregate the hidden states from textEmbedRawLayers
# to create a word embedding representing the entire text.
# This is achieved by concatenating layer 11 and 12.
## Not run: 
word_embedding &lt;- textEmbedLayerAggregation(
  imf_embeddings_11_12$context_tokens,
  layers = 11:12,
  aggregation_from_layers_to_tokens = "concatenate",
  aggregation_from_tokens_to_texts = "mean"
)

# Examine word_embedding
word_embedding

## End(Not run)
</code></pre>


</div>