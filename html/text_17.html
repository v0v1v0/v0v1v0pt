<div class="container">

<table style="width: 100%;"><tr>
<td>textEmbed</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Extract layers and aggregate them to word embeddings, for all character variables in a given dataframe.</h2>

<h3>Description</h3>

<p>Extract layers and aggregate them to word embeddings, for all character variables in a given dataframe.
</p>


<h3>Usage</h3>

<pre><code class="language-R">textEmbed(
  texts,
  model = "bert-base-uncased",
  layers = -2,
  dim_name = TRUE,
  aggregation_from_layers_to_tokens = "concatenate",
  aggregation_from_tokens_to_texts = "mean",
  aggregation_from_tokens_to_word_types = NULL,
  keep_token_embeddings = TRUE,
  tokens_select = NULL,
  tokens_deselect = NULL,
  decontextualize = FALSE,
  model_max_length = NULL,
  max_token_to_sentence = 4,
  tokenizer_parallelism = FALSE,
  device = "cpu",
  hg_gated = FALSE,
  hg_token = Sys.getenv("HUGGINGFACE_TOKEN", unset = ""),
  logging_level = "error",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>texts</code></td>
<td>
<p>A character variable or a tibble/dataframe with at least one character variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>Character string specifying pre-trained language model (default 'bert-base-uncased').
For full list of options see pretrained models at
<a href="https://huggingface.co/transformers/pretrained_models.html">HuggingFace</a>.
For example use "bert-base-multilingual-cased", "openai-gpt",
"gpt2", "ctrl", "transfo-xl-wt103", "xlnet-base-cased", "xlm-mlm-enfr-1024", "distilbert-base-cased",
"roberta-base", or "xlm-roberta-base". Only load models that you trust from HuggingFace; loading a
malicious model can execute arbitrary code on your computer).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>layers</code></td>
<td>
<p>(string or numeric) Specify the layers that should be extracted
(default -2 which give the second to last layer). It is more efficient to only extract the layers
that you need (e.g., 11). You can also extract several (e.g., 11:12), or all by setting this parameter
to "all". Layer 0 is the decontextualized input layer (i.e., not comprising hidden states) and
thus should normally not be used. These layers can then be aggregated in the textEmbedLayerAggregation
function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dim_name</code></td>
<td>
<p>(boolean) If TRUE append the variable name after all variable-names in the output.
(This differentiates between word embedding dimension names; e.g., Dim1_text_variable_name).
see <code>textDimName</code> to change names back and forth.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>aggregation_from_layers_to_tokens</code></td>
<td>
<p>(string) Aggregated layers of each token. Method to aggregate the
contextualized layers (e.g., "mean", "min" or "max, which takes the minimum, maximum or mean, respectively,
across each column; or "concatenate", which links  together each word embedding layer to one long row.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>aggregation_from_tokens_to_texts</code></td>
<td>
<p>(string) Method to carry out the aggregation among the word embeddings
for the words/tokens, including "min", "max" and "mean" which takes the minimum, maximum or mean across each column;
or "concatenate", which links together each layer of the word embedding to one long row (default = "mean"). If set to NULL, embeddings are not
aggregated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>aggregation_from_tokens_to_word_types</code></td>
<td>
<p>(string) Aggregates to the word type (i.e., the individual words)
rather than texts. If set to "individually", then duplicate words are not aggregated, (i.e, the context of individual
is preserved). (default = NULL).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keep_token_embeddings</code></td>
<td>
<p>(boolean) Whether to also keep token embeddings when using texts or word
types aggregation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tokens_select</code></td>
<td>
<p>Option to select word embeddings linked to specific tokens
such as [CLS] and [SEP] for the context embeddings.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tokens_deselect</code></td>
<td>
<p>Option to deselect embeddings linked to specific tokens
such as [CLS] and [SEP] for the context embeddings.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>decontextualize</code></td>
<td>
<p>(boolean) Provide word embeddings of single words as input to the model
(these embeddings are, e.g., used for plotting; default is to use ). If using this, then set
single_context_embeddings to FALSE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model_max_length</code></td>
<td>
<p>The maximum length (in number of tokens) for the inputs to the transformer model
(default the value stored for the associated model).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_token_to_sentence</code></td>
<td>
<p>(numeric) Maximum number of tokens in a string to handle before
switching to embedding text sentence by sentence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tokenizer_parallelism</code></td>
<td>
<p>(boolean) If TRUE this will turn on tokenizer parallelism. Default FALSE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>device</code></td>
<td>
<p>Name of device to use: 'cpu', 'gpu', 'gpu:k' or 'mps'/'mps:k' for MacOS, where k is a
specific device number such as 'mps:1'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hg_gated</code></td>
<td>
<p>Set to TRUE if the accessed model is gated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hg_token</code></td>
<td>
<p>The token needed to access the gated model.
Create a token from the ['Settings' page](https://huggingface.co/settings/tokens) of
the Hugging Face website. An an environment variable HUGGINGFACE_TOKEN can
be set to avoid the need to enter the token each time.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>logging_level</code></td>
<td>
<p>Set the logging level. Default: "warning".
Options (ordered from less logging to more logging): critical, error, warning, info, debug</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>settings from textEmbedRawLayers().</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A tibble with tokens, a column for layer identifier and word embeddings.
Note that layer 0 is the input embedding to the transformer.
</p>


<h3>See Also</h3>

<p>See <code>textEmbedLayerAggregation</code>, <code>textEmbedRawLayers</code> and
<code>textDimName</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Automatically transforms the characters in the example dataset:
# Language_based_assessment_data_8 (included in text-package), to embeddings.
## Not run: 
word_embeddings &lt;- textEmbed(Language_based_assessment_data_8[1:2, 1:2],
  layers = 10:11,
  aggregation_from_layers_to_tokens = "concatenate",
  aggregation_from_tokens_to_texts = "mean",
  aggregation_from_tokens_to_word_types = "mean"
)

# Show information about how the embeddings were constructed.
comment(word_embeddings$texts$satisfactiontexts)
comment(word_embeddings$word_types)
comment(word_embeddings$tokens$satisfactiontexts)

# See how the word embeddings are structured.
word_embeddings

# Save the word embeddings to avoid having to embed the text again.
saveRDS(word_embeddings, "word_embeddings.rds")

# Retrieve the saved word embeddings.
word_embeddings &lt;- readRDS("word_embeddings.rds")

## End(Not run)

</code></pre>


</div>