<div class="container">

<table style="width: 100%;"><tr>
<td>tfd_quantized</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Distribution representing the quantization <code>Y = ceiling(X)</code>
</h2>

<h3>Description</h3>

<p>Definition in Terms of Sampling
</p>


<h3>Usage</h3>

<pre><code class="language-R">tfd_quantized(
  distribution,
  low = NULL,
  high = NULL,
  validate_args = FALSE,
  name = "QuantizedDistribution"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>distribution</code></td>
<td>
<p>The base distribution class to transform. Typically an
instance of <code>Distribution</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>low</code></td>
<td>
<p><code>Tensor</code> with same <code>dtype</code> as this distribution and shape
able to be added to samples. Should be a whole number. Default <code>NULL</code>.
If provided, base distribution's <code>prob</code> should be defined at <code>low</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>high</code></td>
<td>
<p><code>Tensor</code> with same <code>dtype</code> as this distribution and shape
able to be added to samples. Should be a whole number. Default <code>NULL</code>.
If provided, base distribution's <code>prob</code> should be defined at <code>high - 1</code>.
<code>high</code> must be strictly greater than <code>low</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>validate_args</code></td>
<td>
<p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>name</code></td>
<td>
<p>name prefixed to Ops created by this class.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<div class="sourceCode"><pre>1. Draw X
2. Set Y &lt;-- ceiling(X)
3. If Y &lt; low, reset Y &lt;-- low
4. If Y &gt; high, reset Y &lt;-- high
5. Return Y
</pre></div>
<p>Definition in Terms of the Probability Mass Function
</p>
<p>Given scalar random variable <code>X</code>, we define a discrete random variable <code>Y</code>
supported on the integers as follows:
</p>
<div class="sourceCode"><pre>P[Y = j] := P[X &lt;= low],  if j == low,
         := P[X &gt; high - 1],  j == high,
         := 0, if j &lt; low or j &gt; high,
         := P[j - 1 &lt; X &lt;= j],  all other j.
</pre></div>
<p>Conceptually, without cutoffs, the quantization process partitions the real
line <code>R</code> into half open intervals, and identifies an integer <code>j</code> with the
right endpoints:
</p>
<div class="sourceCode"><pre>R = ... (-2, -1](-1, 0](0, 1](1, 2](2, 3](3, 4] ...
j = ...      -1      0     1     2     3     4  ...
</pre></div>
<p><code>P[Y = j]</code> is the mass of <code>X</code> within the <code>jth</code> interval.
If <code>low = 0</code>, and <code>high = 2</code>, then the intervals are redrawn
and <code>j</code> is re-assigned:
</p>
<div class="sourceCode"><pre>R = (-infty, 0](0, 1](1, infty)
j =          0     1     2
</pre></div>
<p><code>P[Y = j]</code> is still the mass of <code>X</code> within the <code>jth</code> interval.
</p>
<p>@section References:
</p>

<ul>
<li> <p><a href="https://arxiv.org/abs/1701.05517">Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications. International Conference on Learning Representations_, 2017.</a>
</p>
</li>
<li> <p><a href="https://arxiv.org/abs/1711.10433">Aaron van den Oord et al. Parallel WaveNet: Fast High-Fidelity Speech Synthesis. <em>arXiv preprint arXiv:1711.10433</em>, 2017.</a>
</p>
</li>
</ul>
<h3>Value</h3>

<p>a distribution instance.
</p>


<h3>See Also</h3>

<p>For usage examples see e.g. <code>tfd_sample()</code>, <code>tfd_log_prob()</code>, <code>tfd_mean()</code>.
</p>
<p>Other distributions: 
<code>tfd_autoregressive()</code>,
<code>tfd_batch_reshape()</code>,
<code>tfd_bates()</code>,
<code>tfd_bernoulli()</code>,
<code>tfd_beta_binomial()</code>,
<code>tfd_beta()</code>,
<code>tfd_binomial()</code>,
<code>tfd_categorical()</code>,
<code>tfd_cauchy()</code>,
<code>tfd_chi2()</code>,
<code>tfd_chi()</code>,
<code>tfd_cholesky_lkj()</code>,
<code>tfd_continuous_bernoulli()</code>,
<code>tfd_deterministic()</code>,
<code>tfd_dirichlet_multinomial()</code>,
<code>tfd_dirichlet()</code>,
<code>tfd_empirical()</code>,
<code>tfd_exp_gamma()</code>,
<code>tfd_exp_inverse_gamma()</code>,
<code>tfd_exponential()</code>,
<code>tfd_gamma_gamma()</code>,
<code>tfd_gamma()</code>,
<code>tfd_gaussian_process_regression_model()</code>,
<code>tfd_gaussian_process()</code>,
<code>tfd_generalized_normal()</code>,
<code>tfd_geometric()</code>,
<code>tfd_gumbel()</code>,
<code>tfd_half_cauchy()</code>,
<code>tfd_half_normal()</code>,
<code>tfd_hidden_markov_model()</code>,
<code>tfd_horseshoe()</code>,
<code>tfd_independent()</code>,
<code>tfd_inverse_gamma()</code>,
<code>tfd_inverse_gaussian()</code>,
<code>tfd_johnson_s_u()</code>,
<code>tfd_joint_distribution_named_auto_batched()</code>,
<code>tfd_joint_distribution_named()</code>,
<code>tfd_joint_distribution_sequential_auto_batched()</code>,
<code>tfd_joint_distribution_sequential()</code>,
<code>tfd_kumaraswamy()</code>,
<code>tfd_laplace()</code>,
<code>tfd_linear_gaussian_state_space_model()</code>,
<code>tfd_lkj()</code>,
<code>tfd_log_logistic()</code>,
<code>tfd_log_normal()</code>,
<code>tfd_logistic()</code>,
<code>tfd_mixture_same_family()</code>,
<code>tfd_mixture()</code>,
<code>tfd_multinomial()</code>,
<code>tfd_multivariate_normal_diag_plus_low_rank()</code>,
<code>tfd_multivariate_normal_diag()</code>,
<code>tfd_multivariate_normal_full_covariance()</code>,
<code>tfd_multivariate_normal_linear_operator()</code>,
<code>tfd_multivariate_normal_tri_l()</code>,
<code>tfd_multivariate_student_t_linear_operator()</code>,
<code>tfd_negative_binomial()</code>,
<code>tfd_normal()</code>,
<code>tfd_one_hot_categorical()</code>,
<code>tfd_pareto()</code>,
<code>tfd_pixel_cnn()</code>,
<code>tfd_poisson_log_normal_quadrature_compound()</code>,
<code>tfd_poisson()</code>,
<code>tfd_power_spherical()</code>,
<code>tfd_probit_bernoulli()</code>,
<code>tfd_relaxed_bernoulli()</code>,
<code>tfd_relaxed_one_hot_categorical()</code>,
<code>tfd_sample_distribution()</code>,
<code>tfd_sinh_arcsinh()</code>,
<code>tfd_skellam()</code>,
<code>tfd_spherical_uniform()</code>,
<code>tfd_student_t_process()</code>,
<code>tfd_student_t()</code>,
<code>tfd_transformed_distribution()</code>,
<code>tfd_triangular()</code>,
<code>tfd_truncated_cauchy()</code>,
<code>tfd_truncated_normal()</code>,
<code>tfd_uniform()</code>,
<code>tfd_variational_gaussian_process()</code>,
<code>tfd_vector_diffeomixture()</code>,
<code>tfd_vector_exponential_diag()</code>,
<code>tfd_vector_exponential_linear_operator()</code>,
<code>tfd_vector_laplace_diag()</code>,
<code>tfd_vector_laplace_linear_operator()</code>,
<code>tfd_vector_sinh_arcsinh_diag()</code>,
<code>tfd_von_mises_fisher()</code>,
<code>tfd_von_mises()</code>,
<code>tfd_weibull()</code>,
<code>tfd_wishart_linear_operator()</code>,
<code>tfd_wishart_tri_l()</code>,
<code>tfd_wishart()</code>,
<code>tfd_zipf()</code>
</p>


</div>