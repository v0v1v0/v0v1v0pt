<div class="container">

<table style="width: 100%;"><tr>
<td>rlg</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Robust Linear Grouping</h2>

<h3>Description</h3>

<p>The function <code>rlg()</code> searches for clusters around affine subspaces of dimensions given by 
vector <code>d</code> (the length of that vector is the number of clusters). For instance <code>d=c(1,2)</code> 
means that we are clustering around a line and a plane. For robustifying the estimation, 
a proportion <code>alpha</code> of observations is trimmed. In particular, the trimmed k-means 
method is represented by the rlg method, if <code>d=c(0,0,..0)</code> (a vector of length 
<code>k</code> with zeroes).
</p>


<h3>Usage</h3>

<pre><code class="language-R">rlg(
  x,
  d,
  alpha = 0.05,
  nstart = 500,
  niter1 = 3,
  niter2 = 20,
  nkeep = 5,
  scale = FALSE,
  parallel = FALSE,
  n.cores = -1,
  trace = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A matrix or data.frame of dimension n x p, containing the observations (rowwise).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>d</code></td>
<td>
<p>A numeric vector of length equal to the number of clusters to be detected. 
Each component of vector <code>d</code> indicates the intrinsic dimension of the affine subspace 
where observations on that cluster are going to be clustered. All the elements 
of vector <code>d</code> should be smaller than the problem dimension minus 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>The proportion of observations to be trimmed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nstart</code></td>
<td>
<p>The number of random initializations to be performed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>niter1</code></td>
<td>
<p>The number of concentration steps to be performed for the nstart initializations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>niter2</code></td>
<td>
<p>The maximum number of concentration steps to be performed for 
the nkeep solutions kept for further iteration. The concentration steps 
are stopped, whenever two consecutive steps lead to the same data partition.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nkeep</code></td>
<td>
<p>The number of iterated initializations (after niter1 concentration 
steps) with the best values in the target function that are kept for further iterations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale</code></td>
<td>
<p>A robust centering and scaling (using the median and MAD) is done if TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parallel</code></td>
<td>
<p>A logical value, specifying whether the nstart initializations should be done in parallel.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.cores</code></td>
<td>
<p>The number of cores to use when paralellizing, only taken into account if parallel=T.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace</code></td>
<td>
<p>Defines the tracing level, which is set to 0 by default. Tracing level 1 gives additional information on the stage of the iterative process.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The procedure allows to deal with robust clustering around affine subspaces 
with an alpha proportion of trimming level by minimizing the trimmed sums of squared 
orthogonal residuals. Each component of vector <code>d</code> indicates the intrinsic dimension of 
the affine subspace where observations on that cluster are going to be clustered. 
Therefore a component equal to 0 on that vector implies clustering around centres, 
equal to 1 around lines, equal to 2 around planes and so on. The procedure so 
allows simultaneous clustering and dimensionality reduction. 
</p>
<p>This iterative algorithm performs "concentration steps" to improve the current 
cluster assignments. For approximately obtaining the global optimum, the procedure 
is randomly initialized <code>nstart</code> times and <code>niter1</code> concentration steps are performed 
for them. The <code>nkeep</code> most “promising” iterations, i.e. the <code>nkeep</code> iterated solutions 
with the initial best values for the target function, are then iterated until 
convergence or until <code>niter2</code> concentration steps are done.
</p>


<h3>Value</h3>

<p>Returns an object of class <code>rlg</code> which is basically a list with the following elements:
</p>

<ul>
<li>
<p> centers - A matrix of size p x k containing the location vectors (column-wise) of each cluster. 
</p>
</li>
<li>
<p> U - A list with k elements where each element is p x d_j matrix whose d_j columns are unitary and orthogonal vectors generating the affine subspace (after subtracting the corresponding cluster’s location parameter in centers). d_j is the intrinsic dimension of the affine subspace approximation in the j-th cluster, i.e., the elements of vector d.
</p>
</li>
<li>
<p> cluster - A numerical vector of size n containing the cluster assignment 
for each observation. Cluster names are integer numbers from 1 to k, 
0 indicates trimmed observations.
</p>
</li>
<li>
<p> obj - The value of the objective function of the best (returned) solution.
</p>
</li>
<li>
<p> cluster.ini - A matrix with nstart rows and number of columns equal to the number of observations and where each row shows the final clustering assignments (0 for trimmed observations) obtained after the niter1 iteration of the nstart random initializations.
</p>
</li>
<li>
<p> obj.ini -A numerical vector of length nstart containing the values of the target function obtained after the niter1 iteration of the nstart random initializations.
</p>
</li>
<li>
<p> x - The input data set. 
</p>
</li>
<li>
<p> dimensions - The input d vector with the intrinsic dimensions. The number of clusters is the length of that vector. 
</p>
</li>
<li>
<p> alpha - The input trimming level.
</p>
</li>
</ul>
<h3>Author(s)</h3>

<p>Javier Crespo Guerrero, Jesús Fernández Iglesias, Luis Angel Garcia Escudero, Agustin Mayo Iscar.
</p>


<h3>References</h3>

<p>García‐Escudero, L. A., Gordaliza, A., San Martin, R., Van Aelst, S., &amp; Zamar, R. (2009). 
Robust linear clustering. Journal of the Royal Statistical Society: 
Series B (Statistical Methodology), 71, 301-318.
</p>


<h3>Examples</h3>

<pre><code class="language-R">##--- EXAMPLE 1 ------------------------------------------
data (LG5data)
x &lt;- LG5data[, 1:10]
clus &lt;- rlg(x, d = c(2,2,2), alpha=0.1)
plot(x, col=clus$cluster+1)
plot(clus, which="eigenvalues") 
plot(clus, which="scores") 

##--- EXAMPLE 2 ------------------------------------------
 data (pine) 
 clus &lt;- rlg(pine, d = c(1,1,1), alpha=0.035)
 plot(pine, col=clus$cluster+1)
 
</code></pre>


</div>