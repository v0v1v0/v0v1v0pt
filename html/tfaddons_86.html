<div class="container">

<table style="width: 100%;"><tr>
<td>layer_weight_normalization</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Weight Normalization layer</h2>

<h3>Description</h3>

<p>Weight Normalization layer
</p>


<h3>Usage</h3>

<pre><code class="language-R">layer_weight_normalization(object, layer, data_init = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Model or layer object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>layer</code></td>
<td>
<p>a layer instance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data_init</code></td>
<td>
<p>If 'TRUE' use data dependent variable initialization</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional parameters to pass</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This wrapper reparameterizes a layer by decoupling the weight's magnitude and
direction.
This speeds up convergence by improving the conditioning of the optimization problem.
Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural
Networks: https://arxiv.org/abs/1602.07868 Tim Salimans, Diederik P. Kingma (2016)
WeightNormalization wrapper works for keras and tf layers.
</p>


<h3>Value</h3>

<p>A tensor
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## Not run: 

model= keras_model_sequential() %&gt;%
layer_weight_normalization(
layer_conv_2d(filters = 2, kernel_size = 2, activation = 'relu'),
input_shape = c(32L, 32L, 3L))
model



## End(Not run)

</code></pre>


</div>