<div class="container">

<table style="width: 100%;"><tr>
<td>last_fit</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fit the final best model to the training set and evaluate the test set</h2>

<h3>Description</h3>

<p><code>last_fit()</code> emulates the process where, after determining the best model,
the final fit on the entire training set is needed and is then evaluated on
the test set.
</p>


<h3>Usage</h3>

<pre><code class="language-R">last_fit(object, ...)

## S3 method for class 'model_spec'
last_fit(
  object,
  preprocessor,
  split,
  ...,
  metrics = NULL,
  eval_time = NULL,
  control = control_last_fit(),
  add_validation_set = FALSE
)

## S3 method for class 'workflow'
last_fit(
  object,
  split,
  ...,
  metrics = NULL,
  eval_time = NULL,
  control = control_last_fit(),
  add_validation_set = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>A <code>parsnip</code> model specification or an unfitted
workflow(). No tuning parameters are allowed; if arguments
have been marked with tune(), their values must be
finalized.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Currently unused.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>preprocessor</code></td>
<td>
<p>A traditional model formula or a recipe created using
<code>recipes::recipe()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>split</code></td>
<td>
<p>An <code>rsplit</code> object created from <code>rsample::initial_split()</code> or
<code>rsample::initial_validation_split()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metrics</code></td>
<td>
<p>A <code>yardstick::metric_set()</code>, or <code>NULL</code> to compute a standard
set of metrics.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eval_time</code></td>
<td>
<p>A numeric vector of time points where dynamic event time
metrics should be computed (e.g. the time-dependent ROC curve, etc). The
values must be non-negative and should probably be no greater than the
largest event time in the training set (See Details below).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p>A <code>control_last_fit()</code> object used to fine tune the last fit
process.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>add_validation_set</code></td>
<td>
<p>For 3-way splits into training, validation, and test
set via <code>rsample::initial_validation_split()</code>, should the validation set be
included in the data set used to train the model. If not, only the training
set is used.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function is intended to be used after fitting a <em>variety of models</em>
and the final tuning parameters (if any) have been finalized. The next step
would be to fit using the entire training set and verify performance using
the test data.
</p>


<h3>Value</h3>

<p>A single row tibble that emulates the structure of <code>fit_resamples()</code>.
However, a list column called <code>.workflow</code> is also attached with the fitted
model (and recipe, if any) that used the training set. Helper functions
for formatting tuning results like <code>collect_metrics()</code> and
<code>collect_predictions()</code> can be used with <code>last_fit()</code> output.
</p>


<h3>Case Weights</h3>

<p>Some models can utilize case weights during training. tidymodels currently
supports two types of case weights: importance weights (doubles) and
frequency weights (integers). Frequency weights are used during model
fitting and evaluation, whereas importance weights are only used during
fitting.
</p>
<p>To know if your model is capable of using case weights, create a model spec
and test it using <code>parsnip::case_weights_allowed()</code>.
</p>
<p>To use them, you will need a numeric column in your data set that has been
passed through either <code>hardhat:: importance_weights()</code> or
<code>hardhat::frequency_weights()</code>.
</p>
<p>For functions such as <code>fit_resamples()</code> and the <code style="white-space: pre;">⁠tune_*()⁠</code> functions, the
model must be contained inside of a <code>workflows::workflow()</code>. To declare that
case weights are used, invoke <code>workflows::add_case_weights()</code> with the
corresponding (unquoted) column name.
</p>
<p>From there, the packages will appropriately handle the weights during model
fitting and (if appropriate) performance estimation.
</p>


<h3>Censored Regression Models</h3>

<p>Three types of metrics can be used to assess the quality of censored
regression models:
</p>

<ul>
<li>
<p> static: the prediction is independent of time.
</p>
</li>
<li>
<p> dynamic: the prediction is a time-specific probability (e.g., survival
probability) and is measured at one or more particular times.
</p>
</li>
<li>
<p> integrated: same as the dynamic metric but returns the integral of the
different metrics from each time point.
</p>
</li>
</ul>
<p>Which metrics are chosen by the user affects how many evaluation times
should be specified. For example:
</p>
<div class="sourceCode"><pre># Needs no `eval_time` value
metric_set(concordance_survival)

# Needs at least one `eval_time`
metric_set(brier_survival)
metric_set(brier_survival, concordance_survival)

# Needs at least two eval_time` values
metric_set(brier_survival_integrated, concordance_survival)
metric_set(brier_survival_integrated, concordance_survival)
metric_set(brier_survival_integrated, concordance_survival, brier_survival)
</pre></div>
<p>Values of <code>eval_time</code> should be less than the largest observed event
time in the training data. For many non-parametric models, the results beyond
the largest time corresponding to an event are constant (or <code>NA</code>).
</p>


<h3>See also</h3>

<p><code>last_fit()</code> is closely related to <code>fit_best()</code>. They both
give you access to a workflow fitted on the training data but are situated
somewhat differently in the modeling workflow. <code>fit_best()</code> picks up
after a tuning function like <code>tune_grid()</code> to take you from tuning results
to fitted workflow, ready for you to predict and assess further. <code>last_fit()</code>
assumes you have made your choice of hyperparameters and finalized your
workflow to then take you from finalized workflow to fitted workflow and
further to performance assessment on the test data. While <code>fit_best()</code> gives
a fitted workflow, <code>last_fit()</code> gives you the performance results. If you
want the fitted workflow, you can extract it from the result of <code>last_fit()</code>
via extract_workflow().
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(recipes)
library(rsample)
library(parsnip)

set.seed(6735)
tr_te_split &lt;- initial_split(mtcars)

spline_rec &lt;- recipe(mpg ~ ., data = mtcars) %&gt;%
  step_ns(disp)

lin_mod &lt;- linear_reg() %&gt;%
  set_engine("lm")

spline_res &lt;- last_fit(lin_mod, spline_rec, split = tr_te_split)
spline_res

# test set metrics
collect_metrics(spline_res)

# test set predictions
collect_predictions(spline_res)

# or use a workflow

library(workflows)
spline_wfl &lt;-
  workflow() %&gt;%
  add_recipe(spline_rec) %&gt;%
  add_model(lin_mod)

last_fit(spline_wfl, split = tr_te_split)

</code></pre>


</div>