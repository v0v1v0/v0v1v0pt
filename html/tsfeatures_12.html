<div class="container">

<table style="width: 100%;"><tr>
<td>entropy</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Spectral entropy of a time series</h2>

<h3>Description</h3>

<p>Computes spectral entropy from a univariate normalized
spectral density, estimated using an AR model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">entropy(x)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a univariate time series</p>
</td>
</tr></table>
<h3>Details</h3>

<p>The <em>spectral entropy</em> equals the Shannon entropy of the spectral density
<code class="reqn">f_x(\lambda)</code> of a stationary process <code class="reqn">x_t</code>:
</p>
<p style="text-align: center;"><code class="reqn">
H_s(x_t) = - \int_{-\pi}^{\pi} f_x(\lambda) \log f_x(\lambda) d \lambda,
</code>
</p>

<p>where the density is normalized such that
<code class="reqn">\int_{-\pi}^{\pi} f_x(\lambda) d \lambda = 1</code>.
An estimate of <code class="reqn">f(\lambda)</code> can be obtained using <code>spec.ar</code> with
the <code>burg</code> method.
</p>


<h3>Value</h3>

<p>A non-negative real value for the spectral entropy <code class="reqn">H_s(x_t)</code>.
</p>


<h3>Author(s)</h3>

<p>Rob J Hyndman
</p>


<h3>References</h3>

<p>Jerry D. Gibson and Jaewoo Jung (2006). “The
Interpretation of Spectral Entropy Based Upon Rate Distortion Functions”.
IEEE International Symposium on Information Theory, pp. 277-281.
</p>
<p>Goerg, G. M. (2013). “Forecastable Component Analysis”.
Proceedings of the 30th International Conference on Machine Learning (PMLR) 28 (2): 64-72, 2013.
Available at <a href="https://proceedings.mlr.press/v28/goerg13.html">https://proceedings.mlr.press/v28/goerg13.html</a>.
</p>


<h3>See Also</h3>

<p><code>spec.ar</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">entropy(rnorm(1000))
entropy(lynx)
entropy(sin(1:20))
</code></pre>


</div>