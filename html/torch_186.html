<div class="container">

<table style="width: 100%;"><tr>
<td>nnf_multi_head_attention_forward</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Multi head attention forward</h2>

<h3>Description</h3>

<p>Allows the model to jointly attend to information from different representation
subspaces. See reference: Attention Is All You Need
</p>


<h3>Usage</h3>

<pre><code class="language-R">nnf_multi_head_attention_forward(
  query,
  key,
  value,
  embed_dim_to_check,
  num_heads,
  in_proj_weight,
  in_proj_bias,
  bias_k,
  bias_v,
  add_zero_attn,
  dropout_p,
  out_proj_weight,
  out_proj_bias,
  training = TRUE,
  key_padding_mask = NULL,
  need_weights = TRUE,
  attn_mask = NULL,
  avg_weights = TRUE,
  use_separate_proj_weight = FALSE,
  q_proj_weight = NULL,
  k_proj_weight = NULL,
  v_proj_weight = NULL,
  static_k = NULL,
  static_v = NULL,
  batch_first = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>query</code></td>
<td>
<p><code class="reqn">(L, N, E)</code> where L is the target sequence length, N is the batch size, E is
the embedding dimension. If batch_first is TRUE, the first two dimensions are transposed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>key</code></td>
<td>
<p><code class="reqn">(S, N, E)</code>, where S is the source sequence length, N is the batch size, E is
the embedding dimension. If batch_first is TRUE, the first two dimensions are transposed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>value</code></td>
<td>
<p><code class="reqn">(S, N, E)</code> where S is the source sequence length, N is the batch size, E is
the embedding dimension. If batch_first is TRUE, the first two dimensions are transposed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>embed_dim_to_check</code></td>
<td>
<p>total dimension of the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_heads</code></td>
<td>
<p>parallel attention heads.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>in_proj_weight</code></td>
<td>
<p>input projection weight.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>in_proj_bias</code></td>
<td>
<p>input projection bias.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias_k</code></td>
<td>
<p>bias of the key and value sequences to be added at dim=0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias_v</code></td>
<td>
<p>currently undocumented.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>add_zero_attn</code></td>
<td>
<p>add a new batch of zeros to the key and
value sequences at dim=1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dropout_p</code></td>
<td>
<p>probability of an element to be zeroed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>out_proj_weight</code></td>
<td>
<p>the output projection weight.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>out_proj_bias</code></td>
<td>
<p>output projection bias.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>training</code></td>
<td>
<p>apply dropout if is <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>key_padding_mask</code></td>
<td>
<p><code class="reqn">(N, S)</code> where N is the batch size, S is the source sequence length.
If a ByteTensor is provided, the non-zero positions will be ignored while the position
with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the
value of <code>True</code> will be ignored while the position with the value of <code>False</code> will be unchanged.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>need_weights</code></td>
<td>
<p>output attn_output_weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>attn_mask</code></td>
<td>
<p>2D mask <code class="reqn">(L, S)</code> where L is the target sequence length, S is the source sequence length.
3D mask <code class="reqn">(N*num_heads, L, S)</code> where N is the batch size, L is the target sequence length,
S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked
positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend
while the zero positions will be unchanged. If a BoolTensor is provided, positions with <code>True</code>
is not allowed to attend while <code>False</code> values will be unchanged. If a FloatTensor
is provided, it will be added to the attention weight.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>avg_weights</code></td>
<td>
<p>Logical; whether to average attn_output_weights over the
attention heads before outputting them. This doesn't change the returned
value of attn_output; it only affects the returned attention weight matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_separate_proj_weight</code></td>
<td>
<p>the function accept the proj. weights for
query, key, and value in different forms. If false, in_proj_weight will be used,
which is a combination of q_proj_weight, k_proj_weight, v_proj_weight.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>q_proj_weight</code></td>
<td>
<p>input projection weight and bias.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k_proj_weight</code></td>
<td>
<p>currently undocumented.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>v_proj_weight</code></td>
<td>
<p>currently undocumented.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>static_k</code></td>
<td>
<p>static key and value used for attention operators.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>static_v</code></td>
<td>
<p>currently undocumented.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch_first</code></td>
<td>
<p>Logical; whether to expect query, key, and value to have
batch as their first parameter, and to return output with batch first.</p>
</td>
</tr>
</table>
</div>