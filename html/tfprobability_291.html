<div class="container">

<table style="width: 100%;"><tr>
<td>vi_jensen_shannon</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>The Jensen-Shannon Csiszar-function in log-space</h2>

<h3>Description</h3>

<p>A Csiszar-function is a member of <code style="white-space: pre;">⁠F = { f:R_+ to R : f convex }⁠</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">vi_jensen_shannon(logu, self_normalized = FALSE, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>logu</code></td>
<td>
<p><code>float</code>-like <code>Tensor</code> representing <code>log(u)</code> from above.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>self_normalized</code></td>
<td>
<p><code>logical</code> indicating whether <code style="white-space: pre;">⁠f'(u=1)=0⁠</code>. When
<code style="white-space: pre;">⁠f'(u=1)=0⁠</code> the implied Csiszar f-Divergence remains non-negative even
when <code style="white-space: pre;">⁠p, q⁠</code> are unnormalized measures.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>name</code></td>
<td>
<p>name prefixed to Ops created by this function.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>When <code>self_normalized = True</code>, the Jensen-Shannon Csiszar-function is:
</p>
<div class="sourceCode"><pre>f(u) = u log(u) - (1 + u) log(1 + u) + (u + 1) log(2)
</pre></div>
<p>When <code>self_normalized = False</code> the <code style="white-space: pre;">⁠(u + 1) log(2)⁠</code> term is omitted.
</p>
<p>Observe that as an f-Divergence, this Csiszar-function implies:
</p>
<div class="sourceCode"><pre>D_f[p, q] = KL[p, m] + KL[q, m]
m(x) = 0.5 p(x) + 0.5 q(x)
</pre></div>
<p>In a sense, this divergence is the "reverse" of the Arithmetic-Geometric
f-Divergence.
</p>
<p>This Csiszar-function induces a symmetric f-Divergence, i.e.,
<code>D_f[p, q] = D_f[q, p]</code>.
</p>
<p>Warning: this function makes non-log-space calculations and may therefore be
numerically unstable for <code style="white-space: pre;">⁠|logu| &gt;&gt; 0⁠</code>.
</p>


<h3>Value</h3>

<p>jensen_shannon_of_u, <code>float</code>-like <code>Tensor</code> of the Csiszar-function
evaluated at <code>u = exp(logu)</code>.
</p>


<h3>References</h3>


<ul><li>
<p> Lin, J. "Divergence measures based on the Shannon entropy." IEEE Trans.
Inf. Th., 37, 145-151, 1991.
</p>
</li></ul>
<h3>See Also</h3>

<p>Other vi-functions: 
<code>vi_amari_alpha()</code>,
<code>vi_arithmetic_geometric()</code>,
<code>vi_chi_square()</code>,
<code>vi_csiszar_vimco()</code>,
<code>vi_dual_csiszar_function()</code>,
<code>vi_fit_surrogate_posterior()</code>,
<code>vi_jeffreys()</code>,
<code>vi_kl_forward()</code>,
<code>vi_kl_reverse()</code>,
<code>vi_log1p_abs()</code>,
<code>vi_modified_gan()</code>,
<code>vi_monte_carlo_variational_loss()</code>,
<code>vi_pearson()</code>,
<code>vi_squared_hellinger()</code>,
<code>vi_symmetrized_csiszar_function()</code>
</p>


</div>