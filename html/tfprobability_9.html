<div class="container">

<table style="width: 100%;"><tr>
<td>layer_autoregressive</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Masked Autoencoder for Distribution Estimation</h2>

<h3>Description</h3>

<p><code>layer_autoregressive</code> takes as input a Tensor of shape <code style="white-space: pre;">⁠[..., event_size]⁠</code>
and returns a Tensor of shape <code style="white-space: pre;">⁠[..., event_size, params]⁠</code>.
The output satisfies the autoregressive property.  That is, the layer is
configured with some permutation <code>ord</code> of <code style="white-space: pre;">⁠{0, ..., event_size-1}⁠</code> (i.e., an
ordering of the input dimensions), and the output <code>output[batch_idx, i, ...]</code>
for input dimension <code>i</code> depends only on inputs <code>x[batch_idx, j]</code> where
<code>ord(j) &lt; ord(i)</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">layer_autoregressive(
  object,
  params,
  event_shape = NULL,
  hidden_units = NULL,
  input_order = "left-to-right",
  hidden_degrees = "equal",
  activation = NULL,
  use_bias = TRUE,
  kernel_initializer = "glorot_uniform",
  validate_args = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>What to compose the new <code>Layer</code> instance with. Typically a
Sequential model or a Tensor (e.g., as returned by <code>layer_input()</code>).
The return value depends on <code>object</code>. If <code>object</code> is:
</p>

<ul>
<li>
<p> missing or <code>NULL</code>, the <code>Layer</code> instance is returned.
</p>
</li>
<li>
<p> a <code>Sequential</code> model, the model with an additional layer is returned.
</p>
</li>
<li>
<p> a Tensor, the output tensor from <code>layer_instance(object)</code> is returned.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>params</code></td>
<td>
<p>integer specifying the number of parameters to output per input.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>event_shape</code></td>
<td>
<p><code>list</code>-like of positive integers (or a single int),
specifying the shape of the input to this layer, which is also the
event_shape of the distribution parameterized by this layer.  Currently
only rank-1 shapes are supported.  That is, event_shape must be a single
integer.  If not specified, the event shape is inferred when this layer
is first called or built.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hidden_units</code></td>
<td>
<p><code>list</code>-like of non-negative integers, specifying
the number of units in each hidden layer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>input_order</code></td>
<td>
<p>Order of degrees to the input units: 'random',
'left-to-right', 'right-to-left', or an array of an explicit order. For
example, 'left-to-right' builds an autoregressive model:
<code style="white-space: pre;">⁠p(x) = p(x1) p(x2 | x1) ... p(xD | x&lt;D)⁠</code>.  Default: 'left-to-right'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hidden_degrees</code></td>
<td>
<p>Method for assigning degrees to the hidden units:
'equal', 'random'.  If 'equal', hidden units in each layer are allocated
equally (up to a remainder term) to each degree.  Default: 'equal'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>activation</code></td>
<td>
<p>An activation function.  See <code>keras::layer_dense</code>. Default: <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_bias</code></td>
<td>
<p>Whether or not the dense layers constructed in this layer
should have a bias term.  See <code>keras::layer_dense</code>.  Default: <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_initializer</code></td>
<td>
<p>Initializer for the kernel weights matrix.  Default: 'glorot_uniform'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>validate_args</code></td>
<td>
<p><code>logical</code>, default <code>FALSE</code>. When <code>TRUE</code>, layer
parameters are checked for validity despite possibly degrading runtime
performance. When <code>FALSE</code> invalid inputs may silently render incorrect outputs.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional keyword arguments passed to the <code>keras::layer_dense</code> constructed by this layer.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The autoregressive property allows us to use
<code>output[batch_idx, i]</code> to parameterize conditional distributions:
<code style="white-space: pre;">⁠p(x[batch_idx, i] | x[batch_idx, ] for ord(j) &lt; ord(i))⁠</code>
which give us a tractable distribution over input <code>x[batch_idx]</code>:
</p>
<p><code style="white-space: pre;">⁠p(x[batch_idx]) = prod_i p(x[batch_idx, ord(i)] | x[batch_idx, ord(0:i)])⁠</code>
</p>
<p>For example, when <code>params</code> is 2, the output of the layer can parameterize
the location and log-scale of an autoregressive Gaussian distribution.
</p>


<h3>Value</h3>

<p>a Keras layer
</p>


<h3>See Also</h3>

<p>Other layers: 
<code>layer_conv_1d_flipout()</code>,
<code>layer_conv_1d_reparameterization()</code>,
<code>layer_conv_2d_flipout()</code>,
<code>layer_conv_2d_reparameterization()</code>,
<code>layer_conv_3d_flipout()</code>,
<code>layer_conv_3d_reparameterization()</code>,
<code>layer_dense_flipout()</code>,
<code>layer_dense_local_reparameterization()</code>,
<code>layer_dense_reparameterization()</code>,
<code>layer_dense_variational()</code>,
<code>layer_variable()</code>
</p>


</div>