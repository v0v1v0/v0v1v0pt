<div class="container">

<table style="width: 100%;"><tr>
<td>mcmc_no_u_turn_sampler</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Runs one step of the No U-Turn Sampler</h2>

<h3>Description</h3>

<p>The No U-Turn Sampler (NUTS) is an adaptive variant of the Hamiltonian Monte
Carlo (HMC) method for MCMC.  NUTS adapts the distance traveled in response to
the curvature of the target density.  Conceptually, one proposal consists of
reversibly evolving a trajectory through the sample space, continuing until
that trajectory turns back on itself (hence the name, 'No U-Turn').
This class implements one random NUTS step from a given
<code>current_state</code>.  Mathematical details and derivations can be found in
Hoffman &amp; Gelman (2011).
</p>


<h3>Usage</h3>

<pre><code class="language-R">mcmc_no_u_turn_sampler(
  target_log_prob_fn,
  step_size,
  max_tree_depth = 10,
  max_energy_diff = 1000,
  unrolled_leapfrog_steps = 1,
  seed = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>target_log_prob_fn</code></td>
<td>
<p>function which takes an argument like
<code>current_state</code> and returns its (possibly unnormalized) log-density under the target
distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>step_size</code></td>
<td>
<p><code>Tensor</code> or <code>list</code> of <code>Tensor</code>s representing the step
size for the leapfrog integrator. Must broadcast with the shape of
<code>current_state</code>. Larger step sizes lead to faster progress, but
too-large step sizes make rejection exponentially more likely. When
possible, it's often helpful to match per-variable step sizes to the
standard deviations of the target distribution in each variable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_tree_depth</code></td>
<td>
<p>Maximum depth of the tree implicitly built by NUTS. The
maximum number of leapfrog steps is bounded by <code>2**max_tree_depth</code> i.e.
the number of nodes in a binary tree <code>max_tree_depth</code> nodes deep. The
default setting of 10 takes up to 1024 leapfrog steps.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_energy_diff</code></td>
<td>
<p>Scaler threshold of energy differences at each leapfrog,
divergence samples are defined as leapfrog steps that exceed this
threshold. Default to 1000.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unrolled_leapfrog_steps</code></td>
<td>
<p>The number of leapfrogs to unroll per tree
expansion step. Applies a direct linear multipler to the maximum
trajectory length implied by max_tree_depth. Defaults to 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>integer to seed the random number generator.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>name</code></td>
<td>
<p>name prefixed to Ops created by this function.
Default value: <code>NULL</code> (i.e., 'nuts_kernel').</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The <code>one_step</code> function can update multiple chains in parallel. It assumes
that a prefix of leftmost dimensions of <code>current_state</code> index independent
chain states (and are therefore updated independently).  The output of
<code>target_log_prob_fn(current_state)</code> should sum log-probabilities across all
event dimensions.  Slices along the rightmost dimensions may have different
target distributions; for example, <code>current_state[0][0, ...]</code> could have a
different target distribution from <code>current_state[0][1, ...]</code>.  These
semantics are governed by <code style="white-space: pre;">⁠target_log_prob_fn(*current_state)⁠</code>.
(The number of independent chains is <code>tf$size(target_log_prob_fn(current_state))</code>.)
</p>


<h3>Value</h3>

<p>a Monte Carlo sampling kernel
</p>


<h3>References</h3>


<ul><li> <p><a href="https://arxiv.org/pdf/1111.4246.pdf">Matthew D. Hoffman, Andrew Gelman.  The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.  2011.</a>
</p>
</li></ul>
<h3>See Also</h3>

<p>Other mcmc_kernels: 
<code>mcmc_dual_averaging_step_size_adaptation()</code>,
<code>mcmc_hamiltonian_monte_carlo()</code>,
<code>mcmc_metropolis_adjusted_langevin_algorithm()</code>,
<code>mcmc_metropolis_hastings()</code>,
<code>mcmc_random_walk_metropolis()</code>,
<code>mcmc_replica_exchange_mc()</code>,
<code>mcmc_simple_step_size_adaptation()</code>,
<code>mcmc_slice_sampler()</code>,
<code>mcmc_transformed_transition_kernel()</code>,
<code>mcmc_uncalibrated_hamiltonian_monte_carlo()</code>,
<code>mcmc_uncalibrated_langevin()</code>,
<code>mcmc_uncalibrated_random_walk()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
predictors &lt;- tf$cast( c(201,244, 47,287,203,58,210,202,198,158,165,201,157,
  131,166,160,186,125,218,146),tf$float32)
obs &lt;- tf$cast(c(592,401,583,402,495,173,479,504,510,416,393,442,317,311,400,
  337,423,334,533,344),tf$float32)
y_sigma &lt;- tf$cast(c(61,25,38,15,21,15,27,14,30,16,14,25,52,16,34,31,42,26,
  16,22),tf$float32)

# Robust linear regression model
robust_lm &lt;- tfd_joint_distribution_sequential(
 list(
   tfd_normal(loc = 0, scale = 1, name = "b0"),
   tfd_normal(loc = 0, scale = 1, name = "b1"),
   tfd_half_normal(5, name = "df"),
   function(df, b1, b0)
     tfd_independent(
       tfd_student_t(
         # Likelihood
           df = tf$expand_dims(df, axis = -1L),
           loc = tf$expand_dims(b0, axis = -1L) +
                 tf$expand_dims(b1, axis = -1L) * predictors[tf$newaxis, ],
           scale = y_sigma,
           name = "st"
           ), name = "ind")), validate_args = TRUE)

 log_prob &lt;-function(b0, b1, df) {robust_lm %&gt;%
   tfd_log_prob(list(b0, b1, df, obs))}
 step_size0 &lt;- Map(function(x) tf$cast(x, tf$float32), c(1, .2, .5))

 number_of_steps &lt;- 10
 burnin &lt;- 5
 nchain &lt;- 50

 run_chain &lt;- function() {
 # random initialization of the starting postion of each chain
 samples &lt;- robust_lm %&gt;% tfd_sample(nchain)
 b0 &lt;- samples[[1]]
 b1 &lt;- samples[[2]]
 df &lt;- samples[[3]]

 # bijector to map constrained parameters to real
 unconstraining_bijectors &lt;- list(
   tfb_identity(), tfb_identity(), tfb_exp())

 trace_fn &lt;- function(x, pkr) {
   list(pkr$inner_results$inner_results$step_size,
     pkr$inner_results$inner_results$log_accept_ratio)
 }

 nuts &lt;- mcmc_no_u_turn_sampler(
   target_log_prob_fn = log_prob,
   step_size = step_size0
   ) %&gt;%
   mcmc_transformed_transition_kernel(bijector = unconstraining_bijectors) %&gt;%
   mcmc_dual_averaging_step_size_adaptation(
     num_adaptation_steps = burnin,
     step_size_setter_fn = function(pkr, new_step_size)
       pkr$`_replace`(
         inner_results = pkr$inner_results$`_replace`(step_size = new_step_size)),
     step_size_getter_fn = function(pkr) pkr$inner_results$step_size,
     log_accept_prob_getter_fn = function(pkr) pkr$inner_results$log_accept_ratio
     )

   nuts %&gt;% mcmc_sample_chain(
     num_results = number_of_steps,
     num_burnin_steps = burnin,
     current_state = list(b0, b1, df),
     trace_fn = trace_fn)
   }

   run_chain &lt;- tensorflow::tf_function(run_chain)
   res &lt;- run_chain()

</code></pre>


</div>