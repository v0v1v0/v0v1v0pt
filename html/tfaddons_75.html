<div class="container">

<table style="width: 100%;"><tr>
<td>layer_activation_gelu</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Gaussian Error Linear Unit</h2>

<h3>Description</h3>

<p>Gaussian Error Linear Unit
</p>


<h3>Usage</h3>

<pre><code class="language-R">layer_activation_gelu(object, approximate = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Model or layer object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>approximate</code></td>
<td>
<p>(bool) Whether to apply approximation</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional parameters to pass</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>A smoother version of ReLU generally used in the BERT or BERT architecture based
models. Original paper: https://arxiv.org/abs/1606.08415
</p>


<h3>Value</h3>

<p>A tensor
</p>


<h3>Note</h3>

<p>Input shape: Arbitrary. Use the keyword argument 'input_shape' (tuple of integers, d
oes not include the samples axis) when using this layer as the first layer in a model.
</p>
<p>Output shape: Same shape as the input.
</p>


</div>