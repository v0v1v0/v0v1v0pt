<div class="container">

<table style="width: 100%;"><tr>
<td>featureContribTree</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Feature Contribution</h2>

<h3>Description</h3>

<p>Contribution of each feature to the prediction.
</p>


<h3>Usage</h3>

<pre><code class="language-R">featureContribTree(tidy.RF, tree, X)

featureContrib(tidy.RF, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>tidy.RF</code></td>
<td>
<p>A tidy random forest. The random forest to make predictions
with.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tree</code></td>
<td>
<p>An integer. The index of the tree to look at.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>A data frame. Features of samples to be predicted.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Recall that each node in a decision tree has a prediction associated with
it. For regression trees, it's the average response in that node, whereas
in classification trees, it's the frequency of each response class, or the
most frequent response class in that node.
</p>
<p>For a tree in the forest, the contribution of each feature to the prediction
of a sample is the sum of differences between the predictions of nodes which
split on the feature and those of their children, i.e. the sum of changes in
node prediction caused by spliting on the feature. This is the calculated by
<code>featureContribTree</code>.
</p>
<p>For a forest, the contribution of each feature to the prediction if a sample
is the average contribution across all trees in the forest. This is because
the prediction of a forest is the average of the predictions of its trees.
This is calculated by <code>featureContrib</code>.
</p>
<p>Together with <code>trainsetBias(Tree)</code>, they can decompose the prediction
by feature importance:
</p>
<p style="text-align: center;"><code class="reqn">prediction(MODEL, X) =
    trainsetBias(MODEL) +
    featureContrib_1(MODEL, X) + ... + featureContrib_p(MODEL, X),</code>
</p>

<p>where MODEL can be either a tree or a forest.
</p>


<h3>Value</h3>

<p>A cube (3D array). The content depends on the type of the response.
</p>

<ul>
<li>
<p> Regression: A P-by-1-by-N array, where P is the number of features
in <code>X</code>, and N the number of samples in <code>X</code>. The pth row of
the nth slice stands for the contribution of feature p to the
prediction for response n.
</p>
</li>
<li>
<p> Classification: A P-by-D-by-N array, where P is the number of
features in <code>X</code>, D is the number of response classes, and N is
the number of samples in <code>X</code>. The pth row of the nth slice stands
for the contribution of feature p to the prediction of each response
class for response n.
</p>
</li>
</ul>
<h3>Functions</h3>


<ul>
<li> <p><code>featureContribTree</code>: Feature contribution to prediction within a
single tree
</p>
</li>
<li> <p><code>featureContrib</code>: Feature contribution to prediction within the
whole forest
</p>
</li>
</ul>
<h3>References</h3>

<p>Interpreting random forests
<a href="http://blog.datadive.net/interpreting-random-forests/">http://blog.datadive.net/interpreting-random-forests/</a>
</p>
<p>Random forest interpretation with scikit-learn
<a href="http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/">http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/</a>
</p>


<h3>See Also</h3>

<p><code>trainsetBias</code>
</p>
<p><code>MDI</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(ranger)
test.id &lt;- 50 * seq(3)
rfobj &lt;- ranger(Species ~ ., iris[-test.id, ], keep.inbag=TRUE)
tidy.RF &lt;- tidyRF(rfobj, iris[-test.id, -5], iris[-test.id, 5])
featureContribTree(tidy.RF, 1, iris[test.id, -5])
featureContrib(tidy.RF, iris[test.id, -5])

</code></pre>


</div>