<div class="container">

<table style="width: 100%;"><tr>
<td>get_regions</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Word embedding semantic region extractor</h2>

<h3>Description</h3>

<p>Given a set of word embeddings of <code class="reqn">d</code> dimensions and <code class="reqn">v</code> vocabulary,
<code>get_regions()</code> finds <code class="reqn">k</code> semantic regions in <code class="reqn">d</code> dimensions.
This, in effect, learns latent topics from an embedding space (a.k.a.
topic modeling), which are directly comparable to both terms (with
cosine similarity) and documents (with Concept Mover's distance
using <code>CMDist()</code>).
</p>


<h3>Usage</h3>

<pre><code class="language-R">get_regions(wv, k_regions = 5L, max_iter = 20L, seed = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>wv</code></td>
<td>
<p>Matrix of word embedding vectors (a.k.a embedding model)
with rows as words.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k_regions</code></td>
<td>
<p>Integer indicating the k number of regions to return</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter</code></td>
<td>
<p>Integer indicating the maximum number of iterations
before k-means terminates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>Integer indicating a random seed. Default is 0, which calls
'std::time(NULL)'.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>To group words into more encompassing "semantic regions" we use <code class="reqn">k</code>-means
clustering. We choose <code class="reqn">k</code>-means primarily for it's ubiquity and the wide
range of available diagnostic tools for <code class="reqn">k</code>-means cluster.
</p>
<p>A word embedding matrix of <code class="reqn">d</code> dimensions and <code class="reqn">v</code> vocabulary is
"clustered" into <code class="reqn">k</code> semantic regions which have <code class="reqn">d</code> dimensions.
Each region is represented by a single point defined by the <code class="reqn">d</code>
dimensional vector. The process discretely assigns all word vectors are
assigned to a given region so as to minimize some error function, however
as the resulting regions are in the same dimensions as the word embeddings,
we can measure each terms similarity to each region. This, in effect,
is a mixed membership topic model similar to topic modeling by Latent
Dirichlet Allocation.
</p>
<p>We use the <code>KMeans_arma</code> function from the <code>ClusterR</code> package which
uses the Armadillo library.
</p>


<h3>Value</h3>

<p>returns a matrix of class "dgCMatrix" with k rows and d dimensions
</p>


<h3>Author(s)</h3>

<p>Dustin Stoltz
</p>


<h3>References</h3>

<p>Butnaru, Andrei M., and Radu Tudor Ionescu.  (2017)
'From image to text classification: A novel approach
based on clustering word embeddings.'
<em>Procedia computer science</em>. 112:1783-1792.
<a href="https://doi.org/10.1016/j.procs.2017.08.211">doi:10.1016/j.procs.2017.08.211</a>.<br>
Zhang, Yi, Jie Lu, Feng Liu, Qian Liu, Alan Porter,
Hongshu Chen, and Guangquan Zhang. (2018).
'Does Deep Learning Help Topic Extraction? A Kernel
K-Means Clustering Method with Word Embedding.'
<em>Journal of Informetrics</em>. 12(4):1099-1117.
<a href="https://doi.org/10.1016/j.joi.2018.09.004">doi:10.1016/j.joi.2018.09.004</a>.<br>
Arseniev-Koehler, Alina and Cochran, Susan D and
Mays, Vickie M and Chang, Kai-Wei and Foster,
Jacob Gates (2021) 'Integrating topic modeling
and word embedding to characterize violent deaths'
<a href="https://doi.org/10.31235/osf.io/nkyaq">doi:10.31235/osf.io/nkyaq</a><br></p>


<h3>Examples</h3>

<pre><code class="language-R">
# load example word embeddings
data(ft_wv_sample)

my.regions &lt;- get_regions(
  wv = ft_wv_sample,
  k_regions = 10L,
  max_iter = 10L,
  seed = 01984
)
</code></pre>


</div>