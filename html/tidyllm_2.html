<div class="container">

<table style="width: 100%;"><tr>
<td>chatgpt</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Call the OpenAI API to interact with ChatGPT or o-reasoning models</h2>

<h3>Description</h3>

<p>Call the OpenAI API to interact with ChatGPT or o-reasoning models
</p>


<h3>Usage</h3>

<pre><code class="language-R">chatgpt(
  .llm,
  .model = "gpt-4",
  .max_tokens = 1024,
  .temperature = NULL,
  .top_p = NULL,
  .top_k = NULL,
  .frequency_penalty = NULL,
  .presence_penalty = NULL,
  .api_url = "https://api.openai.com/",
  .timeout = 60,
  .verbose = FALSE,
  .wait = TRUE,
  .min_tokens_reset = 0L,
  .stream = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>.llm</code></td>
<td>
<p>An existing LLMMessage object or an initial text prompt.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.model</code></td>
<td>
<p>The model identifier (default: "gpt-4o").</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.max_tokens</code></td>
<td>
<p>The maximum number of tokens to generate (default: 1024).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.temperature</code></td>
<td>
<p>Control for randomness in response generation (optional).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.top_p</code></td>
<td>
<p>Nucleus sampling parameter (optional).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.top_k</code></td>
<td>
<p>Top k sampling parameter (optional).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.frequency_penalty</code></td>
<td>
<p>Controls repetition frequency (optional).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.presence_penalty</code></td>
<td>
<p>Controls how much to penalize repeating content (optional)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.api_url</code></td>
<td>
<p>Base URL for the API (default: https://api.openai.com/v1/completions).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.timeout</code></td>
<td>
<p>Request timeout in seconds (default: 60).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.verbose</code></td>
<td>
<p>Should additional information be shown after the API call</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.wait</code></td>
<td>
<p>Should we wait for rate limits if necessary?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.min_tokens_reset</code></td>
<td>
<p>How many tokens should be remaining to wait until we wait for token reset?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>.stream</code></td>
<td>
<p>Stream back the response piece by piece (default: FALSE).</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Returns an updated LLMMessage object.
</p>


</div>