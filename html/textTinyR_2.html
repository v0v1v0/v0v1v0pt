<div class="container">

<table style="width: 100%;"><tr>
<td>big_tokenize_transform</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>String tokenization and transformation for big data sets</h2>

<h3>Description</h3>

<p>String tokenization and transformation for big data sets
</p>
<p>String tokenization and transformation for big data sets
</p>


<h3>Usage</h3>

<pre><code class="language-R"># utl &lt;- big_tokenize_transform$new(verbose = FALSE)
</code></pre>


<h3>Details</h3>

<p>the <em>big_text_splitter</em> function splits a text file into sub-text-files using either the batches parameter (big-text-splitter-bytes) or both the batches and the end_query parameter (big-text-splitter-query). The end_query parameter (if not NULL) should be a character string specifying a word that appears repeatedly at the end of each line in the text file.
</p>
<p>the <em>big_text_parser</em> function parses text files from an input folder and saves those processed files to an output folder. The <em>big_text_parser</em> is appropriate for files with a structure using the start- and end- query parameters.
</p>
<p>the <em>big_text_tokenizer</em> function tokenizes and transforms the text files of a folder and saves those files to either a folder or a single file. There is also the option to save a frequency vocabulary of those transformed tokens to a file.
</p>
<p>the <em>vocabulary_accumulator</em> function takes the resulted vocabulary files of the <em>big_text_tokenizer</em> and returns the vocabulary sums sorted in decreasing order. The parameter <em>max_num_chars</em> limits the number of the corpus using the number of characters of each word.
</p>
<p>The <em>ngram_sequential</em> or <em>ngram_overlap</em> stemming method applies to each single batch and not to the whole corpus of the text file. Thus, it is possible that the stems of the same words for randomly selected batches might differ.
</p>


<h3>Methods</h3>


<dl>
<dt><code>big_tokenize_transform$new(verbose = FALSE)</code></dt>
<dd></dd>
<dt><code>--------------</code></dt>
<dd></dd>
<dt><code>big_text_splitter(input_path_file = NULL, output_path_folder = NULL, end_query = NULL, batches = NULL, trimmed_line = FALSE)</code></dt>
<dd></dd>
<dt><code>--------------</code></dt>
<dd></dd>
<dt><code>big_text_parser(input_path_folder = NULL, output_path_folder = NULL, start_query = NULL, end_query = NULL, min_lines = 1, trimmed_line = FALSE)</code></dt>
<dd></dd>
<dt><code>--------------</code></dt>
<dd></dd>
<dt><code>big_text_tokenizer(input_path_folder = NULL, batches = NULL, read_file_delimiter = " ", to_lower = FALSE, to_upper = FALSE, utf_locale = "", remove_char = "", remove_punctuation_string = FALSE, remove_punctuation_vector = FALSE, remove_numbers = FALSE, trim_token = FALSE, split_string = FALSE, split_separator = " .,;:()?!", remove_stopwords = FALSE, language = "english", min_num_char = 1, max_num_char = Inf, stemmer = NULL, min_n_gram = 1, max_n_gram = 1, skip_n_gram = 1, skip_distance = 0, n_gram_delimiter = " ", concat_delimiter = NULL, path_2folder = "", stemmer_ngram = 4, stemmer_gamma = 0.0, stemmer_truncate = 3, stemmer_batches = 1, threads = 1, save_2single_file = FALSE, increment_batch_nr = 1, vocabulary_path_folder = NULL)</code></dt>
<dd></dd>
<dt><code>--------------</code></dt>
<dd></dd>
<dt><code>vocabulary_accumulator(input_path_folder = NULL, vocabulary_path_file = NULL, max_num_chars = 100)</code></dt>
<dd></dd>
</dl>
<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-big_tokenize_transform-new"><code>big_tokenize_transform$new()</code></a>
</p>
</li>
<li> <p><a href="#method-big_tokenize_transform-big_text_splitter"><code>big_tokenize_transform$big_text_splitter()</code></a>
</p>
</li>
<li> <p><a href="#method-big_tokenize_transform-big_text_parser"><code>big_tokenize_transform$big_text_parser()</code></a>
</p>
</li>
<li> <p><a href="#method-big_tokenize_transform-big_text_tokenizer"><code>big_tokenize_transform$big_text_tokenizer()</code></a>
</p>
</li>
<li> <p><a href="#method-big_tokenize_transform-vocabulary_accumulator"><code>big_tokenize_transform$vocabulary_accumulator()</code></a>
</p>
</li>
<li> <p><a href="#method-big_tokenize_transform-clone"><code>big_tokenize_transform$clone()</code></a>
</p>
</li>
</ul>
<hr>
<a id="method-big_tokenize_transform-new"></a>



<h4>Method <code>new()</code>
</h4>



<h5>Usage</h5>

<div class="r"><pre>big_tokenize_transform$new(verbose = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>verbose</code></dt>
<dd>
<p>either TRUE or FALSE. If TRUE then information will be printed in the console</p>
</dd>
</dl>
</div>


<hr>
<a id="method-big_tokenize_transform-big_text_splitter"></a>



<h4>Method <code>big_text_splitter()</code>
</h4>



<h5>Usage</h5>

<div class="r"><pre>big_tokenize_transform$big_text_splitter(
  input_path_file = NULL,
  output_path_folder = NULL,
  end_query = NULL,
  batches = NULL,
  trimmed_line = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>input_path_file</code></dt>
<dd>
<p>a character string specifying the path to the input file</p>
</dd>
<dt><code>output_path_folder</code></dt>
<dd>
<p>a character string specifying the folder where the output files should be saved</p>
</dd>
<dt><code>end_query</code></dt>
<dd>
<p>a character string. The <em>end_query</em> is the last word of the subset of the data and should appear frequently at the end of each line in the text file.</p>
</dd>
<dt><code>batches</code></dt>
<dd>
<p>a numeric value specifying the number of batches to use. The batches will be used to split the initial data into subsets. Those subsets will be either saved in files (<em>big_text_splitter</em> function) or will be used internally for low memory processing (<em>big_text_tokenizer</em> function).</p>
</dd>
<dt><code>trimmed_line</code></dt>
<dd>
<p>either TRUE or FALSE. If FALSE then each line of the text file will be trimmed both sides before applying the start_query and end_query</p>
</dd>
</dl>
</div>


<hr>
<a id="method-big_tokenize_transform-big_text_parser"></a>



<h4>Method <code>big_text_parser()</code>
</h4>



<h5>Usage</h5>

<div class="r"><pre>big_tokenize_transform$big_text_parser(
  input_path_folder = NULL,
  output_path_folder = NULL,
  start_query = NULL,
  end_query = NULL,
  min_lines = 1,
  trimmed_line = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>input_path_folder</code></dt>
<dd>
<p>a character string specifying the folder where the input files are saved</p>
</dd>
<dt><code>output_path_folder</code></dt>
<dd>
<p>a character string specifying the folder where the output files should be saved</p>
</dd>
<dt><code>start_query</code></dt>
<dd>
<p>a character string. The <em>start_query</em> is the first word of the subset of the data and should appear frequently at the beginning of each line int the text file.</p>
</dd>
<dt><code>end_query</code></dt>
<dd>
<p>a character string. The <em>end_query</em> is the last word of the subset of the data and should appear frequently at the end of each line in the text file.</p>
</dd>
<dt><code>min_lines</code></dt>
<dd>
<p>a numeric value specifying the minimum number of lines. For instance if min_lines = 2, then only subsets of text with more than 1 lines will be kept.</p>
</dd>
<dt><code>trimmed_line</code></dt>
<dd>
<p>either TRUE or FALSE. If FALSE then each line of the text file will be trimmed both sides before applying the start_query and end_query</p>
</dd>
</dl>
</div>


<hr>
<a id="method-big_tokenize_transform-big_text_tokenizer"></a>



<h4>Method <code>big_text_tokenizer()</code>
</h4>



<h5>Usage</h5>

<div class="r"><pre>big_tokenize_transform$big_text_tokenizer(
  input_path_folder = NULL,
  batches = NULL,
  read_file_delimiter = "\n",
  to_lower = FALSE,
  to_upper = FALSE,
  utf_locale = "",
  remove_char = "",
  remove_punctuation_string = FALSE,
  remove_punctuation_vector = FALSE,
  remove_numbers = FALSE,
  trim_token = FALSE,
  split_string = FALSE,
  split_separator = " \r\n\t.,;:()?!//",
  remove_stopwords = FALSE,
  language = "english",
  min_num_char = 1,
  max_num_char = Inf,
  stemmer = NULL,
  min_n_gram = 1,
  max_n_gram = 1,
  skip_n_gram = 1,
  skip_distance = 0,
  n_gram_delimiter = " ",
  concat_delimiter = NULL,
  path_2folder = "",
  stemmer_ngram = 4,
  stemmer_gamma = 0,
  stemmer_truncate = 3,
  stemmer_batches = 1,
  threads = 1,
  save_2single_file = FALSE,
  increment_batch_nr = 1,
  vocabulary_path_folder = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>input_path_folder</code></dt>
<dd>
<p>a character string specifying the folder where the input files are saved</p>
</dd>
<dt><code>batches</code></dt>
<dd>
<p>a numeric value specifying the number of batches to use. The batches will be used to split the initial data into subsets. Those subsets will be either saved in files (<em>big_text_splitter</em> function) or will be used internally for low memory processing (<em>big_text_tokenizer</em> function).</p>
</dd>
<dt><code>read_file_delimiter</code></dt>
<dd>
<p>the delimiter to use when the input file will be red (for instance a tab-delimiter or a new-line delimiter).</p>
</dd>
<dt><code>to_lower</code></dt>
<dd>
<p>either TRUE or FALSE. If TRUE the character string will be converted to lower case</p>
</dd>
<dt><code>to_upper</code></dt>
<dd>
<p>either TRUE or FALSE. If TRUE the character string will be converted to upper case</p>
</dd>
<dt><code>utf_locale</code></dt>
<dd>
<p>the language specific locale to use in case that either the <em>to_lower</em> or the <em>to_upper</em> parameter is TRUE and the text file language is other than english. For instance if the language of a text file is greek then the <em>utf_locale</em> parameter should be <em>'el_GR.UTF-8'</em> ( <em>language_country.encoding</em> ). A wrong utf-locale does not raise an error, however the runtime of the function increases.</p>
</dd>
<dt><code>remove_char</code></dt>
<dd>
<p>a character string with specific characters that should be removed from the text file. If the <em>remove_char</em> is "" then no removal of characters take place</p>
</dd>
<dt><code>remove_punctuation_string</code></dt>
<dd>
<p>either TRUE or FALSE. If TRUE then the punctuation of the character string will be removed (applies before the split function)</p>
</dd>
<dt><code>remove_punctuation_vector</code></dt>
<dd>
<p>either TRUE or FALSE. If TRUE then the punctuation of the vector of the character strings will be removed  (after the string split has taken place)</p>
</dd>
<dt><code>remove_numbers</code></dt>
<dd>
<p>either TRUE or FALSE. If TRUE then any numbers in the character string will be removed</p>
</dd>
<dt><code>trim_token</code></dt>
<dd>
<p>either TRUE or FALSE. If TRUE then the string will be trimmed (left and/or right)</p>
</dd>
<dt><code>split_string</code></dt>
<dd>
<p>either TRUE or FALSE. If TRUE then the character string will be split using the <em>split_separator</em> as delimiter. The user can also specify multiple delimiters.</p>
</dd>
<dt><code>split_separator</code></dt>
<dd>
<p>a character string specifying the character delimiter(s)</p>
</dd>
<dt><code>remove_stopwords</code></dt>
<dd>
<p>either TRUE, FALSE or a character vector of user defined stop words. If TRUE then by using the <em>language</em> parameter the corresponding stop words vector will be uploaded.</p>
</dd>
<dt><code>language</code></dt>
<dd>
<p>a character string which defaults to english. If the <em>remove_stopwords</em> parameter is TRUE then the corresponding stop words vector will be uploaded. Available languages
are <em>afrikaans</em>, <em>arabic</em>, <em>armenian</em>, <em>basque</em>, <em>bengali</em>, <em>breton</em>, <em>bulgarian</em>, <em>catalan</em>,
<em>croatian</em>, <em>czech</em>, <em>danish</em>, <em>dutch</em>, <em>english</em>, <em>estonian</em>,
<em>finnish</em>, <em>french</em>, <em>galician</em>, <em>german</em>, <em>greek</em>, <em>hausa</em>, <em>hebrew</em>, <em>hindi</em>, <em>hungarian</em>,
<em>indonesian</em>, <em>irish</em>, <em>italian</em>, <em>latvian</em>, <em>marathi</em>,
<em>norwegian</em>, <em>persian</em>, <em>polish</em>, <em>portuguese</em>, <em>romanian</em>, <em>russian</em>, <em>slovak</em>, <em>slovenian</em>,
<em>somalia</em>, <em>spanish</em>, <em>swahili</em>, <em>swedish</em>, <em>turkish</em>, <em>yoruba</em>, <em>zulu</em></p>
</dd>
<dt><code>min_num_char</code></dt>
<dd>
<p>an integer specifying the minimum number of characters to keep. If the <em>min_num_char</em> is greater than 1 then character strings with more than 1 characters will be returned</p>
</dd>
<dt><code>max_num_char</code></dt>
<dd>
<p>an integer specifying the maximum number of characters to keep. The <em>max_num_char</em> should be less than or equal to <em>Inf</em> (in this function the Inf value translates to a word-length of 1000000000)</p>
</dd>
<dt><code>stemmer</code></dt>
<dd>
<p>a character string specifying the stemming method. One of the following <em>porter2_stemmer</em>, <em>ngram_sequential</em>, <em>ngram_overlap</em>. See details for more information.</p>
</dd>
<dt><code>min_n_gram</code></dt>
<dd>
<p>an integer specifying the minimum number of n-grams. The minimum number of min_n_gram is 1.</p>
</dd>
<dt><code>max_n_gram</code></dt>
<dd>
<p>an integer specifying the maximum number of n-grams. The minimum number of max_n_gram is 1.</p>
</dd>
<dt><code>skip_n_gram</code></dt>
<dd>
<p>an integer specifying the number of skip-n-grams. The minimum number of skip_n_gram is 1. The skip_n_gram gives the (max.) n-grams using the <em>skip_distance</em> parameter. If <em>skip_n_gram</em> is greater than 1 then both <em>min_n_gram</em> and <em>max_n_gram</em> should be set to 1.</p>
</dd>
<dt><code>skip_distance</code></dt>
<dd>
<p>an integer specifying the skip distance between the words. The minimum value for the skip distance is 0, in which case simple n-grams will be returned.</p>
</dd>
<dt><code>n_gram_delimiter</code></dt>
<dd>
<p>a character string specifying the n-gram delimiter (applies to both n-gram and skip-n-gram cases)</p>
</dd>
<dt><code>concat_delimiter</code></dt>
<dd>
<p>either NULL or a character string specifying the delimiter to use in order to concatenate the end-vector of character strings to a single character string (recommended in case that the end-vector should be saved to a file)</p>
</dd>
<dt><code>path_2folder</code></dt>
<dd>
<p>a character string specifying the path to the folder where the file(s) will be saved</p>
</dd>
<dt><code>stemmer_ngram</code></dt>
<dd>
<p>a numeric value greater than 1. Applies to both <em>ngram_sequential</em> and <em>ngram_overlap</em> methods. In case of <em>ngram_sequential</em> the first stemmer_ngram characters will be picked, whereas in the case of <em>ngram_overlap</em> the overlapping stemmer_ngram characters will be build.</p>
</dd>
<dt><code>stemmer_gamma</code></dt>
<dd>
<p>a float number greater or equal to 0.0. Applies only to <em>ngram_sequential</em>. Is a threshold value, which defines how much frequency deviation of two N-grams is acceptable. It is kept either zero or to a minimum value.</p>
</dd>
<dt><code>stemmer_truncate</code></dt>
<dd>
<p>a numeric value greater than 0. Applies only to <em>ngram_sequential</em>. The ngram_sequential is modified to use relative frequencies (float numbers between 0.0 and 1.0 for the ngrams of a specific word in the corpus) and the stemmer_truncate parameter controls the number of rounding digits for the ngrams of the word. The main purpose was to give the same relative frequency to words appearing approximately the same on the corpus.</p>
</dd>
<dt><code>stemmer_batches</code></dt>
<dd>
<p>a numeric value greater than 0. Applies only to <em>ngram_sequential</em>. Splits the corpus into batches with the option to run the batches in multiple threads.</p>
</dd>
<dt><code>threads</code></dt>
<dd>
<p>an integer specifying the number of cores to run in parallel</p>
</dd>
<dt><code>save_2single_file</code></dt>
<dd>
<p>either TRUE or FALSE. If TRUE then the output data will be saved in a single file. Otherwise the data will be saved in multiple files with incremented enumeration</p>
</dd>
<dt><code>increment_batch_nr</code></dt>
<dd>
<p>a numeric value. The enumeration of the output files will start from the <em>increment_batch_nr</em>. If the <em>save_2single_file</em> parameter is TRUE then the <em>increment_batch_nr</em> parameter won't be taken into consideration.</p>
</dd>
<dt><code>vocabulary_path_folder</code></dt>
<dd>
<p>either NULL or a character string specifying the output folder where the vocabulary batches should be saved (after tokenization and transformation is applied). Applies to the <em>big_text_tokenizer</em> method.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-big_tokenize_transform-vocabulary_accumulator"></a>



<h4>Method <code>vocabulary_accumulator()</code>
</h4>



<h5>Usage</h5>

<div class="r"><pre>big_tokenize_transform$vocabulary_accumulator(
  input_path_folder = NULL,
  vocabulary_path_file = NULL,
  max_num_chars = 100
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>input_path_folder</code></dt>
<dd>
<p>a character string specifying the folder where the input files are saved</p>
</dd>
<dt><code>vocabulary_path_file</code></dt>
<dd>
<p>either NULL or a character string specifying the output file where the vocabulary should be saved (after tokenization and transformation is applied). Applies to the <em>vocabulary_accumulator</em> method.</p>
</dd>
<dt><code>max_num_chars</code></dt>
<dd>
<p>a numeric value to limit the words of the output vocabulary to a maximum number of characters (applies to the <em>vocabulary_accumulator</em> function)</p>
</dd>
</dl>
</div>


<hr>
<a id="method-big_tokenize_transform-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>big_tokenize_transform$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>Examples</h3>

<pre><code class="language-R">
## Not run: 

library(textTinyR)

fs &lt;- big_tokenize_transform$new(verbose = FALSE)

#---------------
# file splitter:
#---------------

fs$big_text_splitter(input_path_file = "input.txt",
                     output_path_folder = "/folder/output/",
                     end_query = "endword", batches = 5,
                     trimmed_line = FALSE)


#-------------
# file parser:
#-------------

fs$big_text_parser(input_path_folder = "/folder/output/",
                    output_path_folder = "/folder/parser/",
                    start_query = "startword", end_query = "endword",
                    min_lines = 1, trimmed_line = TRUE)


#----------------
# file tokenizer:
#----------------


 fs$big_text_tokenizer(input_path_folder = "/folder/parser/",
                       batches = 5, split_string=TRUE,
                       to_lower = TRUE, trim_token = TRUE,
                       max_num_char = 100, remove_stopwords = TRUE,
                       stemmer = "porter2_stemmer", threads = 1,
                       path_2folder="/folder/output_token/",
                       vocabulary_path_folder="/folder/VOCAB/")

#-------------------
# vocabulary counts:
#-------------------


fs$vocabulary_accumulator(input_path_folder = "/folder/VOCAB/",
                           vocabulary_path_file = "/folder/vocab.txt",
                           max_num_chars = 50)


## End(Not run)
</code></pre>


</div>