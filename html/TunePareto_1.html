<div class="container">

<table style="width: 100%;"><tr>
<td>TunePareto-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Multi-objective parameter tuning for classifiers
</h2>

<h3>Description</h3>

<p>Generic methods for parameter tuning of classification algorithms using multiple scoring functions
</p>


<h3>Details</h3>

<p>The methods of this package allow to assess the performance of classifiers with respect to certain parameter values and multiple scoring functions, such as the cross-validation error or the sensitivity. It provides the <code>tunePareto</code> function which can be configured to run most common classification methods implemented in R. Several sampling strategies for parameters are supplied, including Latin Hypercube sampling, quasi-random sequences, and evolutionary algorithms.
</p>
<p>Classifiers are wrapped in generic <code>TuneParetoClassifier</code> objects which can be created using <code>tuneParetoClassifier</code>. For state-of-the-art classifiers, the package includes the corresponding wrapper objects (see <code>tunePareto.knn</code>, <code>tunePareto.tree</code>, <code>tunePareto.randomForest</code>, <code>tunePareto.svm</code>, <code>tunePareto.NaiveBayes</code>). 
</p>
<p>The method tests combinations of the supplied classifier parameters according to the supplied scoring functions and calculates the Pareto front of optimal parameter configurations. The Pareto fronts can be visualized using <code>plotDominationGraph</code>, <code>plotParetoFronts2D</code> and <code>plotObjectivePairs</code>.
</p>
<p>A number of predefined scoring functions are provided (see <code>predefinedObjectiveFunctions</code>), but the user is free to implement own scores (see <code>createObjective</code>).
</p>


<h3>Author(s)</h3>

<p>Christoph Müssel, Ludwig Lausser, Hans Kestler
</p>
<p>Maintainer: Hans Kestler &lt;hans.kestler@uni-ulm.de&gt;
</p>


<h3>References</h3>

<p>Christoph Müssel, Ludwig Lausser, Markus Maucher, Hans A. Kestler (2012). Multi-Objective Parameter Selection for Classifiers. Journal of Statistical Software, 46(5), 1-27. DOI https://doi.org/10.18637/jss.v046.i05.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# optimize the 'cost' and 'kernel' parameters of an SVM according
# to CV error and CV Specificity on the 'iris' data set
# using several predefined values for the cost
r &lt;- tunePareto(data = iris[, -ncol(iris)], 
                labels = iris[, ncol(iris)],
                classifier=tunePareto.svm(),
                cost=c(0.001,0.01,0.1,1,10), 
                kernel=c("linear", "polynomial", 
                         "radial", "sigmoid"),
                objectiveFunctions=list(cvError(10, 10), 
                                        cvSpecificity(10, 10, caseClass="setosa")))

# print Pareto-optimal solutions
print(r)

# use a continuous interval for the 'cost' parameter 
# and optimize it using evolutionary algorithms and
# parallel execution with snowfall
library(snowfall)
sfInit(parallel=TRUE, cpus=2, type="SOCK")
r &lt;- tunePareto(data = iris[, -ncol(iris)], 
                 labels = iris[, ncol(iris)],
                 classifier = tunePareto.svm(), 
                 cost = as.interval(0.001,10), 
                 kernel = c("linear", "polynomial",
                            "radial", "sigmoid"),
                 sampleType="evolution",
                 numCombinations=20,
                 numIterations=20,                      
                 objectiveFunctions = list(cvError(10, 10),
                                           cvSensitivity(10, 10, caseClass="setosa"),
                                           cvSpecificity(10, 10, caseClass="setosa")),
                useSnowfall=TRUE)
sfStop()

# print Pareto-optimal solutions
print(r)

# plot the Pareto fronts
plotDominationGraph(r, legend.x="topright")
</code></pre>


</div>