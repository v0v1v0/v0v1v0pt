<div class="container">

<table style="width: 100%;"><tr>
<td>lambdaseq</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Generate sequence of lambda for LASSO regression</h2>

<h3>Description</h3>

<p>Calculates the <code>lambdaMax</code> value, which is the penalty term (lambda) beyond which coefficients are guaranteed to be all zero and provides a sequence of <code>nLambda</code> values to <code>lambdaMin</code> in logarithmic descent.
</p>


<h3>Usage</h3>

<pre><code class="language-R">lambdaseq(
  x,
  y,
  weight = NA,
  alpha = 1,
  standardise = TRUE,
  lambdaRatio = 1e-04,
  nLambda = 100,
  addZeroLambda = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>matrix of regressors. See <code>glmnet</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>response variable. See <code>glmnet</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight</code></td>
<td>
<p>vector of <code>length(nrow(y))</code> for weighted LASSO estimation. See <code>glmnet</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>elastic net mixing value. See <code>glmnet</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardise</code></td>
<td>
<p>if <code>TRUE</code>, then variables are standardised.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambdaRatio</code></td>
<td>
<p>ratio between <code>lambdaMax</code> and <code>lambdaMin</code>. That is, <code>lambdaMin &lt;- lambdaMax * lambdaRatio</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nLambda</code></td>
<td>
<p>length of the lambda sequence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>addZeroLambda</code></td>
<td>
<p>if <code>TRUE</code>, then set the last value in the lambda sequence to 0, which is the OLS solution.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list that contains:
</p>

<ul>
<li> <p><code>lambda</code>: sequence of lambda values, from <code>lambdaMax</code> to <code>lambdaMin</code>.
</p>
</li>
<li> <p><code>lambdaMin</code>: minimal lambda value.
</p>
</li>
<li> <p><code>lambdaMax</code>: maximal lambda value.
</p>
</li>
<li> <p><code>nullMSE</code>: MSE of the fit using just a constant term.
</p>
</li>
</ul>
<h3>Author(s)</h3>

<p>Oliver Schaer, <a href="mailto:info@oliverschaer.ch">info@oliverschaer.ch</a>,
</p>
<p>Nikolaos Kourentzes, <a href="mailto:nikolaos@kourentzes.com">nikolaos@kourentzes.com</a>.
</p>


<h3>References</h3>

<p>Hastie, T., Tibshirani, R., &amp; Wainwright, M. (2015). Statistical learning with sparsity: the lasso and generalizations. CRC press.
</p>


<h3>Examples</h3>

<pre><code class="language-R">y &lt;- mtcars[,1]
x &lt;- as.matrix(mtcars[,2:11])
lambda &lt;- lambdaseq(x, y)$lambda

## Not run: 
  library(glmnet)
  fit.lasso &lt;- cv.glmnet(x, y, lambda = lambda)
  coef.lasso &lt;- coef(fit.lasso, s = "lambda.1se")

## End(Not run)

</code></pre>


</div>