<div class="container">

<table style="width: 100%;"><tr>
<td>vectorizers</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Vocabulary and hash vectorizers</h2>

<h3>Description</h3>

<p>This function creates an object (closure) which defines on how to
transform list of tokens into vector space - i.e. how to map words to indices.
It supposed to be used only as argument to create_dtm, create_tcm,
create_vocabulary.
</p>


<h3>Usage</h3>

<pre><code class="language-R">vocab_vectorizer(vocabulary)

hash_vectorizer(hash_size = 2^18, ngram = c(1L, 1L),
  signed_hash = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>vocabulary</code></td>
<td>
<p><code>text2vec_vocabulary</code> object, see create_vocabulary.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hash_size</code></td>
<td>
<p><code>integer</code> The number of of hash-buckets for the feature
hashing trick. The number must be greater than 0, and preferably it will be
a power of 2.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ngram</code></td>
<td>
<p><code>integer</code> vector. The lower and upper boundary of the range
of n-values for different n-grams to be extracted. All values of <code>n</code>
such that ngram_min &lt;= n &lt;= ngram_max will be used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>signed_hash</code></td>
<td>
<p><code>logical</code>,  indicating whether to use a signed
hash-function to reduce collisions when hashing.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A vectorizer <code>object</code> (closure).
</p>


<h3>See Also</h3>

<p>create_dtm create_tcm create_vocabulary
</p>


<h3>Examples</h3>

<pre><code class="language-R">data("movie_review")
N = 100
vectorizer = hash_vectorizer(2 ^ 18, c(1L, 2L))
it = itoken(movie_review$review[1:N], preprocess_function = tolower,
             tokenizer = word_tokenizer, n_chunks = 10)
hash_dtm = create_dtm(it, vectorizer)

it = itoken(movie_review$review[1:N], preprocess_function = tolower,
             tokenizer = word_tokenizer, n_chunks = 10)
v = create_vocabulary(it, c(1L, 1L) )

vectorizer = vocab_vectorizer(v)

it = itoken(movie_review$review[1:N], preprocess_function = tolower,
             tokenizer = word_tokenizer, n_chunks = 10)

dtm = create_dtm(it, vectorizer)
</code></pre>


</div>