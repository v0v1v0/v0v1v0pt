<div class="container">

<table style="width: 100%;"><tr>
<td>autograd_grad</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Computes and returns the sum of gradients of outputs w.r.t. the inputs.</h2>

<h3>Description</h3>

<p><code>grad_outputs</code> should be a list of length matching output containing the “vector”
in Jacobian-vector product, usually the pre-computed gradients w.r.t. each of
the outputs. If an output doesn’t require_grad, then the gradient can be None).
</p>


<h3>Usage</h3>

<pre><code class="language-R">autograd_grad(
  outputs,
  inputs,
  grad_outputs = NULL,
  retain_graph = create_graph,
  create_graph = FALSE,
  allow_unused = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>outputs</code></td>
<td>
<p>(sequence of Tensor) – outputs of the differentiated function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>inputs</code></td>
<td>
<p>(sequence of Tensor) – Inputs w.r.t. which the gradient will be
returned (and not accumulated into .grad).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>grad_outputs</code></td>
<td>
<p>(sequence of Tensor) – The “vector” in the Jacobian-vector
product. Usually gradients w.r.t. each output. None values can be specified for
scalar Tensors or ones that don’t require grad. If a None value would be acceptable
for all <code>grad_tensors</code>, then this argument is optional. Default: None.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>retain_graph</code></td>
<td>
<p>(bool, optional) – If <code>FALSE</code>, the graph used to compute the
grad will be freed. Note that in nearly all cases setting this option to <code>TRUE</code> is
not needed and often can be worked around in a much more efficient way.
Defaults to the value of <code>create_graph</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>create_graph</code></td>
<td>
<p>(bool, optional) – If <code style="white-space: pre;">⁠TRUE, graph of the derivative will be constructed, allowing to compute higher order derivative products. Default: ⁠</code>FALSE'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>allow_unused</code></td>
<td>
<p>(bool, optional) – If <code>FALSE</code>, specifying inputs that were
not used when computing outputs (and therefore their grad is always zero) is an
error. Defaults to <code>FALSE</code></p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>If only_inputs is <code>TRUE</code>, the function will only return a list of gradients w.r.t
the specified inputs. If it’s <code>FALSE</code>, then gradient w.r.t. all remaining leaves
will still be computed, and will be accumulated into their <code>.grad</code> attribute.
</p>


<h3>Examples</h3>

<pre><code class="language-R">if (torch_is_installed()) {
w &lt;- torch_tensor(0.5, requires_grad = TRUE)
b &lt;- torch_tensor(0.9, requires_grad = TRUE)
x &lt;- torch_tensor(runif(100))
y &lt;- 2 * x + 1
loss &lt;- (y - (w * x + b))^2
loss &lt;- loss$mean()

o &lt;- autograd_grad(loss, list(w, b))
o
}
</code></pre>


</div>