<div class="container">

<table style="width: 100%;"><tr>
<td>chunk_text</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Chunk text into smaller segments</h2>

<h3>Description</h3>

<p>Given a text or vector/list of texts, break the texts into smaller segments
each with the same number of words. This allows you to treat a very long
document, such as a novel, as a set of smaller documents.
</p>


<h3>Usage</h3>

<pre><code class="language-R">chunk_text(x, chunk_size = 100, doc_id = names(x), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A character vector or a list of character vectors to be tokenized
into n-grams. If <code>x</code> is a character vector, it can be of any length,
and each element will be chunked separately. If <code>x</code> is a list of
character vectors, each element of the list should have a length of 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>chunk_size</code></td>
<td>
<p>The number of words in each chunk.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>doc_id</code></td>
<td>
<p>The document IDs as a character vector. This will be taken from
the names of the <code>x</code> vector if available. <code>NULL</code> is acceptable.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Arguments passed on to <code>tokenize_words</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Chunking the text passes it through <code>tokenize_words</code>,
which will strip punctuation and lowercase the text unless you provide
arguments to pass along to that function.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
chunked &lt;- chunk_text(mobydick, chunk_size = 100)
length(chunked)
chunked[1:3]

## End(Not run)
</code></pre>


</div>