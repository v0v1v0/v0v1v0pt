<div class="container">

<table style="width: 100%;"><tr>
<td>initialize_topic_counts</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Initialize topic counts for gibbs sampling</h2>

<h3>Description</h3>

<p>Implementing seeded (or guided) LDA models and transfer learning means that
we can't initialize topics with a uniform-random start. This function prepares
data and then calls a C++ function, <code>create_lexicon</code>, that runs a single
Gibbs iteration to populate topic counts (and other objects) used during the
main Gibbs sampling run of <code>fit_lda_c</code>. In the event that
you aren't using fancy seeding or transfer learning, this makes a random
initialization by sampling from Dirichlet distributions parameterized by
priors <code>alpha</code> and <code>eta</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">initialize_topic_counts(
  dtm,
  k,
  alpha,
  eta,
  beta_initial = NULL,
  theta_initial = NULL,
  freeze_topics = FALSE,
  threads = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>dtm</code></td>
<td>
<p>a document term matrix or term co-occurrence matrix of class <code>dgCMatrix</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>the number of topics</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>the numeric vector prior for topics over documents as formatted
by <code>format_alpha</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eta</code></td>
<td>
<p>the numeric matrix prior for topics over documents as formatted
by <code>format_eta</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta_initial</code></td>
<td>
<p>if specified, a numeric matrix for the probability of tokens
in topics. Must be specified for predictions or updates as called by
<code>predict.tidylda</code> or <code>refit.tidylda</code>
respectively.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta_initial</code></td>
<td>
<p>if specified, a numeric matrix for the probability of
topics in documents. Must be specified for updates as called by
<code>refit.tidylda</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>freeze_topics</code></td>
<td>
<p>if <code>TRUE</code> does not update counts of tokens in topics.
This is <code>TRUE</code> for predictions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threads</code></td>
<td>
<p>number of parallel threads, currently unused</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments, currently unused</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>Returns a list with 5 elements: <code>docs</code>, <code>Zd</code>, <code>Cd</code>, <code>Cv</code>,
and <code>Ck</code>. All of these are used by <code>fit_lda_c</code>.
</p>
<p><code>docs</code> is a list with one element per document. Each element is a vector
of integers of length <code>sum(dtm[j,])</code> for the j-th document. The integer
entries correspond to the zero-index column of the <code>dtm</code>.
</p>
<p><code>Zd</code> is a list of similar format as <code>docs</code>. The difference is that
the integer values correspond to the zero-index for topics.
</p>
<p><code>Cd</code> is a matrix of integers denoting how many times each topic has
been sampled in each document.
</p>
<p><code>Cv</code> is similar to <code>Cd</code> but it counts how many times each topic
has been sampled for each token.
</p>
<p><code>Ck</code> is an integer vector denoting how many times each topic has been
sampled overall.
</p>


<h3>Note</h3>

<p>All of <code>Cd</code>, <code>Cv</code>, and <code>Ck</code> should be derivable by summing
over Zd in various ways.
</p>


</div>