<div class="container">

<table style="width: 100%;"><tr>
<td>tfb_masked_autoregressive_default_template</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Masked Autoregressive Density Estimator</h2>

<h3>Description</h3>

<p>This will be wrapped in a make_template to ensure the variables are only
created once. It takes the input and returns the loc ("mu" in
Germain et al. (2015)) and log_scale ("alpha" in Germain et al. (2015)) from
the MADE network.
</p>


<h3>Usage</h3>

<pre><code class="language-R">tfb_masked_autoregressive_default_template(
  hidden_layers,
  shift_only = FALSE,
  activation = tf$nn$relu,
  log_scale_min_clip = -5,
  log_scale_max_clip = 3,
  log_scale_clip_gradient = FALSE,
  name = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>hidden_layers</code></td>
<td>
<p>list-like of non-negative integer, scalars indicating the number
of units in each hidden layer. Default: <code>list(512, 512)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shift_only</code></td>
<td>
<p>logical indicating if only the shift term shall be
computed. Default: FALSE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>activation</code></td>
<td>
<p>Activation function (callable). Explicitly setting to NULL implies a linear activation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>log_scale_min_clip</code></td>
<td>
<p>float-like scalar Tensor, or a Tensor with the same shape as log_scale. The minimum value to clip by. Default: -5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>log_scale_max_clip</code></td>
<td>
<p>float-like scalar Tensor, or a Tensor with the same shape as log_scale. The maximum value to clip by. Default: 3.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>log_scale_clip_gradient</code></td>
<td>
<p>logical indicating that the gradient of tf$clip_by_value should be preserved. Default: FALSE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>name</code></td>
<td>
<p>A name for ops managed by this function. Default: "tfb_masked_autoregressive_default_template".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p><code>tf$layers$dense</code> arguments</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Warning: This function uses masked_dense to create randomly initialized
<code>tf$Variables</code>. It is presumed that these will be fit, just as you would any
other neural architecture which uses <code>tf$layers$dense</code>.
</p>
<p>About Hidden Layers
Each element of hidden_layers should be greater than the input_depth
(i.e., <code>input_depth = tf$shape(input)[-1]</code> where input is the input to the
neural network). This is necessary to ensure the autoregressivity property.
</p>
<p>About Clipping
This function also optionally clips the log_scale (but possibly not its
gradient). This is useful because if log_scale is too small/large it might
underflow/overflow making it impossible for the MaskedAutoregressiveFlow
bijector to implement a bijection. Additionally, the log_scale_clip_gradient
bool indicates whether the gradient should also be clipped. The default does
not clip the gradient; this is useful because it still provides gradient
information (for fitting) yet solves the numerical stability problem. I.e.,
log_scale_clip_gradient = FALSE means <code style="white-space: pre;">⁠grad[exp(clip(x))] = grad[x] exp(clip(x))⁠</code>
rather than the usual <code style="white-space: pre;">⁠grad[clip(x)] exp(clip(x))⁠</code>.
</p>


<h3>Value</h3>

<p>list of:
</p>

<ul>
<li>
<p> shift: <code>Float</code>-like <code>Tensor</code> of shift terms
</p>
</li>
<li>
<p> log_scale: <code>Float</code>-like <code>Tensor</code> of log(scale) terms
</p>
</li>
</ul>
<h3>References</h3>


<ul><li> <p><a href="https://arxiv.org/abs/1502.03509">Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE: Masked Autoencoder for Distribution Estimation. In <em>International Conference on Machine Learning</em>, 2015.</a>
</p>
</li></ul>
<h3>See Also</h3>

<p>For usage examples see <code>tfb_forward()</code>, <code>tfb_inverse()</code>, <code>tfb_inverse_log_det_jacobian()</code>.
</p>
<p>Other bijectors: 
<code>tfb_absolute_value()</code>,
<code>tfb_affine_linear_operator()</code>,
<code>tfb_affine_scalar()</code>,
<code>tfb_affine()</code>,
<code>tfb_ascending()</code>,
<code>tfb_batch_normalization()</code>,
<code>tfb_blockwise()</code>,
<code>tfb_chain()</code>,
<code>tfb_cholesky_outer_product()</code>,
<code>tfb_cholesky_to_inv_cholesky()</code>,
<code>tfb_correlation_cholesky()</code>,
<code>tfb_cumsum()</code>,
<code>tfb_discrete_cosine_transform()</code>,
<code>tfb_expm1()</code>,
<code>tfb_exp()</code>,
<code>tfb_ffjord()</code>,
<code>tfb_fill_scale_tri_l()</code>,
<code>tfb_fill_triangular()</code>,
<code>tfb_glow()</code>,
<code>tfb_gompertz_cdf()</code>,
<code>tfb_gumbel_cdf()</code>,
<code>tfb_gumbel()</code>,
<code>tfb_identity()</code>,
<code>tfb_inline()</code>,
<code>tfb_invert()</code>,
<code>tfb_iterated_sigmoid_centered()</code>,
<code>tfb_kumaraswamy_cdf()</code>,
<code>tfb_kumaraswamy()</code>,
<code>tfb_lambert_w_tail()</code>,
<code>tfb_masked_autoregressive_flow()</code>,
<code>tfb_masked_dense()</code>,
<code>tfb_matrix_inverse_tri_l()</code>,
<code>tfb_matvec_lu()</code>,
<code>tfb_normal_cdf()</code>,
<code>tfb_ordered()</code>,
<code>tfb_pad()</code>,
<code>tfb_permute()</code>,
<code>tfb_power_transform()</code>,
<code>tfb_rational_quadratic_spline()</code>,
<code>tfb_rayleigh_cdf()</code>,
<code>tfb_real_nvp_default_template()</code>,
<code>tfb_real_nvp()</code>,
<code>tfb_reciprocal()</code>,
<code>tfb_reshape()</code>,
<code>tfb_scale_matvec_diag()</code>,
<code>tfb_scale_matvec_linear_operator()</code>,
<code>tfb_scale_matvec_lu()</code>,
<code>tfb_scale_matvec_tri_l()</code>,
<code>tfb_scale_tri_l()</code>,
<code>tfb_scale()</code>,
<code>tfb_shifted_gompertz_cdf()</code>,
<code>tfb_shift()</code>,
<code>tfb_sigmoid()</code>,
<code>tfb_sinh_arcsinh()</code>,
<code>tfb_sinh()</code>,
<code>tfb_softmax_centered()</code>,
<code>tfb_softplus()</code>,
<code>tfb_softsign()</code>,
<code>tfb_split()</code>,
<code>tfb_square()</code>,
<code>tfb_tanh()</code>,
<code>tfb_transform_diagonal()</code>,
<code>tfb_transpose()</code>,
<code>tfb_weibull_cdf()</code>,
<code>tfb_weibull()</code>
</p>


</div>