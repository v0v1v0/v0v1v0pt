<div class="container">

<table style="width: 100%;"><tr>
<td>perf_mod</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Bayesian Analysis of Resampling Statistics</h2>

<h3>Description</h3>

<p>Bayesian analysis used here to answer the question: "when looking at
resampling results, are the differences between models 'real?'" To answer
this, a model can be created were the <em>outcome</em> is the resampling statistics
(e.g. accuracy or RMSE). These values are explained by the model types. In
doing this, we can get parameter estimates for each model's affect on
performance and make statistical (and practical) comparisons between models.
</p>


<h3>Usage</h3>

<pre><code class="language-R">perf_mod(object, ...)

## S3 method for class 'rset'
perf_mod(object, transform = no_trans, hetero_var = FALSE, formula = NULL, ...)

## S3 method for class 'resamples'
perf_mod(
  object,
  transform = no_trans,
  hetero_var = FALSE,
  metric = object$metrics[1],
  ...
)

## S3 method for class 'data.frame'
perf_mod(object, transform = no_trans, hetero_var = FALSE, formula = NULL, ...)

## S3 method for class 'tune_results'
perf_mod(
  object,
  metric = NULL,
  transform = no_trans,
  hetero_var = FALSE,
  formula = NULL,
  filter = NULL,
  ...
)

## S3 method for class 'workflow_set'
perf_mod(
  object,
  metric = NULL,
  transform = no_trans,
  hetero_var = FALSE,
  formula = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Depending on the context (see Details below):
</p>

<ul>
<li>
<p> A data frame with <code>id</code> columns for the resampling groupds and metric
results in all of the other columns..
</p>
</li>
<li>
<p> An <code>rset</code> object (such as <code>rsample::vfold_cv()</code>) containing the <code>id</code>
column(s) and at least two numeric columns of model performance
statistics (e.g. accuracy).
</p>
</li>
<li>
<p> An object from <code>caret::resamples</code>.
</p>
</li>
<li>
<p> An object with class <code>tune_results</code>, which could be produced by
<code>tune::tune_grid()</code>, <code>tune::tune_bayes()</code> or similar.
</p>
</li>
<li>
<p> A workflow set where all results contain the metric value given in the
<code>metric</code> argument value.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments to pass to <code>rstanarm::stan_glmer()</code> such as
<code>verbose</code>, <code>prior</code>, <code>seed</code>, <code>refresh</code>, <code>family</code>, etc.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>transform</code></td>
<td>
<p>An named list of transformation and inverse
transformation functions. See <code>logit_trans()</code> as an example.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hetero_var</code></td>
<td>
<p>A logical; if <code>TRUE</code>, then different
variances are estimated for each model group. Otherwise, the
same variance is used for each group. Estimating heterogeneous
variances may slow or prevent convergence.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>An optional model formula to use for the Bayesian hierarchical model
(see Details below).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>metric</code></td>
<td>
<p>A single character value for the statistic from
the <code>resamples</code> object that should be analyzed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>filter</code></td>
<td>
<p>A conditional logic statement that can be used to filter the
statistics generated by <code>tune_results</code> using the tuning parameter values or
the <code>.config</code> column.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>These functions can be used to process and analyze matched
resampling statistics from different models using a Bayesian generalized
linear model with effects for the model and the resamples.
</p>


<h4>Bayesian Model formula</h4>

<p>By default, a generalized linear model with Gaussian error and an identity
link is fit to the data and has terms for the predictive model grouping
variable. In this way, the performance metrics can be compared between
models.
</p>
<p>Additionally, random effect terms are also used. For most resampling
methods (except repeated <em>V</em>-fold cross-validation), a simple random
intercept model its used with an exchangeable (i.e. compound-symmetric)
variance structure. In the case of repeated cross-validation, two random
intercept terms are used; one for the repeat and another for the fold within
repeat. These also have exchangeable correlation structures.
</p>
<p>The above model specification assumes that the variance in the performance
metrics is the same across models. However, this is unlikely to be true in
some cases. For example, for simple binomial accuracy, it well know that the
variance is highest when the accuracy is near 50 percent. When the argument
<code>hetero_var = TRUE</code>, the variance structure uses random intercepts for each
model term. This may produce more realistic posterior distributions but may
take more time to converge.
</p>
<p>Examples of the default formulas are:
</p>
<pre>
   # One ID field and common variance:
     statistic ~ model + (model | id)

   # One ID field and heterogeneous variance:
     statistic ~ model + (model + 0 | id)

   # Repeated CV (id = repeat, id2 = fold within repeat)
   # with a common variance:
     statistic ~ model + (model | id2/id)

   # Repeated CV (id = repeat, id2 = fold within repeat)
   # with a heterogeneous variance:
     statistic ~ model + (model + 0| id2/id)

   # Default for unknown resampling method and
   # multiple ID fields:
     statistic ~ model + (model | idN/../id)
 </pre>
<p>Custom formulas should use <code>statistic</code> as the outcome variable and <code>model</code>
as the factor variable with the model names.
</p>
<p>Also, as shown in the package vignettes, the Gaussian assumption make be
unrealistic. In this case, there are at least two approaches that can be
used. First, the outcome statistics can be transformed prior to fitting the
model. For example, for accuracy, the logit transformation can be used to
convert the outcome values to be on the real line and a model is fit to
these data. Once the posterior distributions are computed, the inverse
transformation can be used to put them back into the original units. The
<code>transform</code> argument can be used to do this.
</p>
<p>The second approach would be to use a different error distribution from the
exponential family. For RMSE values, the Gamma distribution may produce
better results at the expense of model computational complexity. This can be
achieved by passing the <code>family</code> argument to <code>perf_mod</code> as one might with
the <code>glm</code> function.
</p>



<h4>Input formats</h4>

<p>There are several ways to give resampling results to the <code>perf_mod()</code> function. To
illustrate, here are some example objects using 10-fold cross-validation for a
simple two-class problem:
</p>
<div class="sourceCode r"><pre>   library(tidymodels)
   library(tidyposterior)
   library(workflowsets)

   data(two_class_dat, package = "modeldata")

   set.seed(100)
   folds &lt;- vfold_cv(two_class_dat)
</pre></div>
<p>We can define two different models (for simplicity, with no tuning parameters).
</p>
<div class="sourceCode r"><pre>   logistic_reg_glm_spec &lt;-
     logistic_reg() %&gt;%
     set_engine('glm')

   mars_earth_spec &lt;-
     mars(prod_degree = 1) %&gt;%
     set_engine('earth') %&gt;%
     set_mode('classification')
</pre></div>
<p>For tidymodels, the <code>tune::fit_resamples()</code> function can be used to estimate
performance for each model/resample:
</p>
<div class="sourceCode r"><pre>   rs_ctrl &lt;- control_resamples(save_workflow = TRUE)

   logistic_reg_glm_res &lt;-
     logistic_reg_glm_spec %&gt;%
     fit_resamples(Class ~ ., resamples = folds, control = rs_ctrl)

   mars_earth_res &lt;-
     mars_earth_spec %&gt;%
     fit_resamples(Class ~ ., resamples = folds, control = rs_ctrl)
</pre></div>
<p>From these, there are several ways to pass the results to <code>perf_mod()</code>.
</p>


<h5>Data Frame as Input</h5>

<p>The most general approach is to have a data frame with the resampling labels (i.e.,
one or more id columns) as well as columns for each model that you would like to
compare.
</p>
<p>For the model results above, <code>tune::collect_metrics()</code> can be used along with some
basic data manipulation steps:
</p>
<div class="sourceCode r"><pre>   logistic_roc &lt;-
     collect_metrics(logistic_reg_glm_res, summarize = FALSE) %&gt;%
     dplyr::filter(.metric == "roc_auc") %&gt;%
     dplyr::select(id, logistic = .estimate)

   mars_roc &lt;-
     collect_metrics(mars_earth_res, summarize = FALSE) %&gt;%
     dplyr::filter(.metric == "roc_auc") %&gt;%
     dplyr::select(id, mars = .estimate)

   resamples_df &lt;- full_join(logistic_roc, mars_roc, by = "id")
   resamples_df
</pre></div>
<div class="sourceCode"><pre>   ## # A tibble: 10 x 3
   ##   id     logistic  mars
   ##   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;
   ## 1 Fold01    0.908 0.875
   ## 2 Fold02    0.904 0.917
   ## 3 Fold03    0.924 0.938
   ## 4 Fold04    0.881 0.881
   ## 5 Fold05    0.863 0.864
   ## 6 Fold06    0.893 0.889
   ## # â€¦ with 4 more rows
</pre></div>
<p>We can then give this directly to <code>perf_mod()</code>:
</p>
<div class="sourceCode r"><pre>   set.seed(101)
   roc_model_via_df &lt;- perf_mod(resamples_df, refresh = 0)
   tidy(roc_model_via_df) %&gt;% summary()
</pre></div>
<div class="sourceCode"><pre>   ## # A tibble: 2 x 4
   ##   model     mean lower upper
   ##   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
   ## 1 logistic 0.892 0.879 0.906
   ## 2 mars     0.888 0.875 0.902
</pre></div>



<h5>rsample Object as Input</h5>

<p>Alternatively, the result columns can be merged back into the original <code>rsample</code>
object. The up-side to using this method is that <code>perf_mod()</code> will know exactly
which model formula to use for the Bayesian model:
</p>
<div class="sourceCode r"><pre>   resamples_rset &lt;-
     full_join(folds, logistic_roc, by = "id") %&gt;%
     full_join(mars_roc, by = "id")

   set.seed(101)
   roc_model_via_rset &lt;- perf_mod(resamples_rset, refresh = 0)
   tidy(roc_model_via_rset) %&gt;% summary()
</pre></div>
<div class="sourceCode"><pre>   ## # A tibble: 2 x 4
   ##   model     mean lower upper
   ##   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
   ## 1 logistic 0.892 0.879 0.906
   ## 2 mars     0.888 0.875 0.902
</pre></div>



<h5>Workflow Set Object as Input</h5>

<p>Finally, for tidymodels, a workflow set object can be used. This is a collection of
models/preprocessing combinations in one object. We can emulate a workflow set using
the existing example results then pass that to <code>perf_mod()</code>:
</p>
<div class="sourceCode r"><pre>   example_wset &lt;-
     as_workflow_set(logistic = logistic_reg_glm_res, mars = mars_earth_res)

   set.seed(101)
   roc_model_via_wflowset &lt;- perf_mod(example_wset, refresh = 0)
   tidy(roc_model_via_rset) %&gt;% summary()
</pre></div>
<div class="sourceCode"><pre>   ## # A tibble: 2 x 4
   ##   model     mean lower upper
   ##   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
   ## 1 logistic 0.892 0.879 0.906
   ## 2 mars     0.888 0.875 0.902
</pre></div>



<h5>caret resamples object</h5>

<p>The <code>caret</code> package can also be used. An equivalent set of models are created:
</p>
<div class="sourceCode r"><pre>   library(caret)

   set.seed(102)
   logistic_caret &lt;- train(Class ~ ., data = two_class_dat, method = "glm",
                           trControl = trainControl(method = "cv"))

   set.seed(102)
   mars_caret &lt;- train(Class ~ ., data = two_class_dat, method = "gcvEarth",
                       tuneGrid = data.frame(degree = 1),
                       trControl = trainControl(method = "cv"))
</pre></div>
<p>Note that these two models use the same resamples as one another due to setting the
seed prior to calling <code>train()</code>. However, these are different from the tidymodels
results used above (so the final results will be different).
</p>
<p><code>caret</code> has a <code>resamples()</code> function that can collect and collate the resamples.
This can also be given to <code>perf_mod()</code>:
</p>
<div class="sourceCode r"><pre>   caret_resamples &lt;- resamples(list(logistic = logistic_caret, mars = mars_caret))

   set.seed(101)
   roc_model_via_caret &lt;- perf_mod(caret_resamples, refresh = 0)
   tidy(roc_model_via_caret) %&gt;% summary()
</pre></div>
<div class="sourceCode"><pre>   ## # A tibble: 2 x 4
   ##   model     mean lower upper
   ##   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
   ## 1 logistic 0.821 0.801 0.842
   ## 2 mars     0.822 0.802 0.842
</pre></div>




<h3>Value</h3>

<p>An object of class <code>perf_mod</code>. If a workfkow set is given in
<code>object</code>, there is an extra class of <code>"perf_mod_workflow_set"</code>.
</p>


<h3>References</h3>

<p>Kuhn and Silge (2021) <em>Tidy Models with R</em>, Chapter 11,
<a href="https://www.tmwr.org/compare.html">https://www.tmwr.org/compare.html</a>
</p>


<h3>See Also</h3>

<p><code>tidy.perf_mod()</code>, <code>contrast_models()</code>
</p>


</div>