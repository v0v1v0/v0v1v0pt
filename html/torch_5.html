<div class="container">

<table style="width: 100%;"><tr>
<td>autograd_backward</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Computes the sum of gradients of given tensors w.r.t. graph leaves.</h2>

<h3>Description</h3>

<p>The graph is differentiated using the chain rule. If any of tensors are
non-scalar (i.e. their data has more than one element) and require gradient,
then the Jacobian-vector product would be computed, in this case the function
additionally requires specifying <code>grad_tensors</code>. It should be a sequence of
matching length, that contains the “vector” in the Jacobian-vector product,
usually the gradient of the differentiated function w.r.t. corresponding
tensors (None is an acceptable value for all tensors that don’t need gradient
tensors).
</p>


<h3>Usage</h3>

<pre><code class="language-R">autograd_backward(
  tensors,
  grad_tensors = NULL,
  retain_graph = create_graph,
  create_graph = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>tensors</code></td>
<td>
<p>(list of Tensor) – Tensors of which the derivative will
be computed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>grad_tensors</code></td>
<td>
<p>(list of (Tensor or <code style="white-space: pre;">⁠NULL)) – The “vector” in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors. ⁠</code>NULL<code style="white-space: pre;">⁠values can be specified for scalar Tensors or ones that don’t require grad. If a⁠</code>NULL' value would be acceptable for all
grad_tensors, then this argument is optional.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>retain_graph</code></td>
<td>
<p>(bool, optional) – If <code>FALSE</code>, the graph used to compute
the grad will be freed. Note that in nearly all cases setting this option to
<code>TRUE</code> is not needed and often can be worked around in a much more efficient
way. Defaults to the value of <code>create_graph</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>create_graph</code></td>
<td>
<p>(bool, optional) – If <code>TRUE</code>, graph of the derivative will
be constructed, allowing to compute higher order derivative products.
Defaults to <code>FALSE</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function accumulates gradients in the leaves - you might need to zero
them before calling it.
</p>


<h3>Examples</h3>

<pre><code class="language-R">if (torch_is_installed()) {
x &lt;- torch_tensor(1, requires_grad = TRUE)
y &lt;- 2 * x

a &lt;- torch_tensor(1, requires_grad = TRUE)
b &lt;- 3 * a

autograd_backward(list(y, b))
}
</code></pre>


</div>