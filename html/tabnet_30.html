<div class="container">

<table style="width: 100%;"><tr>
<td>tabnet_nn</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>TabNet Model Architecture</h2>

<h3>Description</h3>

<p>This is a <code>nn_module</code> representing the TabNet architecture from
<a href="https://arxiv.org/abs/1908.07442">Attentive Interpretable Tabular Deep Learning</a>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">tabnet_nn(
  input_dim,
  output_dim,
  n_d = 8,
  n_a = 8,
  n_steps = 3,
  gamma = 1.3,
  cat_idxs = c(),
  cat_dims = c(),
  cat_emb_dim = 1,
  n_independent = 2,
  n_shared = 2,
  epsilon = 1e-15,
  virtual_batch_size = 128,
  momentum = 0.02,
  mask_type = "sparsemax"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>input_dim</code></td>
<td>
<p>Initial number of features.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>output_dim</code></td>
<td>
<p>Dimension of network output examples : one for regression, 2 for
binary classification etc.. Vector of those dimensions in case of multi-output.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_d</code></td>
<td>
<p>Dimension of the prediction  layer (usually between 4 and 64).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_a</code></td>
<td>
<p>Dimension of the attention  layer (usually between 4 and 64).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_steps</code></td>
<td>
<p>Number of successive steps in the network (usually between 3 and 10).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>Float above 1, scaling factor for attention updates (usually between 1 and 2).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cat_idxs</code></td>
<td>
<p>Index of each categorical column in the dataset.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cat_dims</code></td>
<td>
<p>Number of categories in each categorical column.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cat_emb_dim</code></td>
<td>
<p>Size of the embedding of categorical features if int, all categorical
features will have same embedding size if list of int, every corresponding feature will have
specific size.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_independent</code></td>
<td>
<p>Number of independent GLU layer in each GLU block of the encoder.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_shared</code></td>
<td>
<p>Number of independent GLU layer in each GLU block of the encoder.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>Avoid log(0), this should be kept very low.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>virtual_batch_size</code></td>
<td>
<p>Batch size for Ghost Batch Normalization.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>momentum</code></td>
<td>
<p>Float value between 0 and 1 which will be used for momentum in all batch norm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mask_type</code></td>
<td>
<p>Either "sparsemax" or "entmax" : this is the masking function to use.</p>
</td>
</tr>
</table>
</div>