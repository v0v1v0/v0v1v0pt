<div class="container">

<table style="width: 100%;"><tr>
<td>backward</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Backward pass of the Neural ODE</h2>

<h3>Description</h3>

<p>Backward pass of the Neural ODE
</p>


<h3>Usage</h3>

<pre><code class="language-R">backward(model, tsteps, outputs, output_gradients = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>A keras neural network that defines the Neural ODE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tsteps</code></td>
<td>
<p>A vector of each time step upon which the Neural ODE is solved to get to the final solution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>outputs</code></td>
<td>
<p>The tensor outputs of the forward pass of the Neural ODE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>output_gradients</code></td>
<td>
<p>The tensor gradients of the loss function.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>The model input at the last time step.
</p>
<p>The gradient of loss with respect to the inputs for use with the Adjoint Method.
</p>
<p>The gradients of loss the neural ODE.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
reticulate::py_module_available("tensorflow")

# example code
# single training example
OdeModel(keras$Model) %py_class% {
 initialize &lt;- function() {
   super$initialize()
   self$block_1 &lt;- layer_dense(units = 50, activation = 'tanh')
   self$block_2 &lt;- layer_dense(units = 2, activation = 'linear')
 }

 call &lt;- function(inputs) {
   x&lt;- inputs ^ 3
   x &lt;- self$block_1(x)
   self$block_2(x)
 }
}
tsteps &lt;- seq(0, 2.5, by = 2.5/10)
true_y0 = t(c(2., 0.))
model&lt;- OdeModel()
optimizer = tf$keras$optimizers$legacy$Adam(learning_rate = 1e-3)
# single training iteration
pred = forward(model, true_y0, tsteps)
with(tf$GradientTape() %as% tape, {
  tape$watch(pred)
  loss = tf$reduce_mean(tf$abs(pred - inp[[2]]))
})
dLoss = tape$gradient(loss, pred)
list_w = backward(model, tsteps[1:batch_time], pred, output_gradients = dLoss)
optimizer$apply_gradients(zip_lists(list_w[[3]], model$trainable_variables))

</code></pre>


</div>