<div class="container">

<table style="width: 100%;"><tr>
<td>find_transformation</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Find a specified matrix transformation</h2>

<h3>Description</h3>

<p>Given a matrix, <code class="reqn">B</code>, of word embedding vectors (source) with
terms as rows, this function finds a transformed matrix following a
specified operation. These include: centering (i.e.
translation) and normalization (i.e. scaling). In the first, <code class="reqn">B</code> is
centered by subtracting column means. In the second, <code class="reqn">B</code> is
normalized by the L2 norm. Both have been found to improve
word embedding representations. The function also finds a transformed
matrix that approximately aligns <code class="reqn">B</code>, with another matrix,
<code class="reqn">A</code>, of word embedding vectors (reference), using Procrustes
transformation (see details). Finally, given a term-co-occurrence matrix
built on a local corpus, the function can "retrofit" pretrained
embeddings to better match the local corpus.
</p>


<h3>Usage</h3>

<pre><code class="language-R">find_transformation(
  wv,
  ref = NULL,
  method = c("align", "norm", "center", "retrofit")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>wv</code></td>
<td>
<p>Matrix of word embedding vectors (a.k.a embedding model)
with rows as terms (the source matrix to be transformed).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ref</code></td>
<td>
<p>If <code>method = "align"</code>, this is the reference matrix
toward which the source matrix is to be aligned.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>Character vector indicating the method to use for
the transformation. Current methods include: "align",
"norm", "center", and "refrofit" â€“ see details.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Aligning a source matrix of word embedding vectors, <code class="reqn">B</code>, to a
reference matrix, <code class="reqn">A</code>, has primarily been used as a post-processing step
for embeddings trained on longitudinal corpora for diachronic analysis
or for cross-lingual embeddings. Aligning preserves internal (cosine)
distances, while orient the source embeddings to minimize the sum of squared
distances (and is therefore a Least Squares problem).
Alignment is accomplished with the following steps:
</p>

<ul>
<li>
<p> translation: centering by column means
</p>
</li>
<li>
<p> scaling: scale (normalizes) by the L2 Norm
</p>
</li>
<li>
<p> rotation/reflection: rotates and a reflects to minimize
sum of squared differences, using singular value decomposition
</p>
</li>
</ul>
<p>Alignment is asymmetrical, and only outputs the transformed source matrix,
<code class="reqn">B</code>. Therefore, it is typically recommended to align <code class="reqn">B</code> to <code class="reqn">A</code>,
and then <code class="reqn">A</code> to <code class="reqn">B</code>. However, simplying centering and norming
<code class="reqn">A</code> after may be sufficient.
</p>


<h3>Value</h3>

<p>A new word embedding matrix,
transformed using the specified method.
</p>


<h3>References</h3>

<p>Mikel Artetxe, Gorka Labaka, and Eneko Agirre. (2018).
'A robust self-learning method for fully unsupervised
cross-lingual mappings of word embeddings.' <em>In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics</em>. 789-798<br>
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2019.
'An effective approach to unsupervised machine translation.'
<em>In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>. 194-203<br>
Hamilton, William L., Jure Leskovec, and Dan Jurafsky. (2018).
'Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change.'
<a href="https://arxiv.org/abs/1605.09096v6">https://arxiv.org/abs/1605.09096v6</a>.<br>
Lin, Zefeng, Xiaojun Wan, and Zongming Guo. (2019).
'Learning Diachronic Word Embeddings with Iterative Stable
Information Alignment.' <em>Natural Language Processing and
Chinese Computing</em>. 749-60. <a href="https://doi.org/10.1007/978-3-030-32233-5_58">doi:10.1007/978-3-030-32233-5_58</a>.<br>
Schlechtweg et al. (2019). 'A Wind of Change: Detecting and
Evaluating Lexical Semantic Change across Times and Domains.'
<a href="https://arxiv.org/abs/1906.02979v1">https://arxiv.org/abs/1906.02979v1</a>.
Shoemark et a. (2019). 'Room to Glo: A Systematic Comparison
of Semantic Change Detection Approaches with Word Embeddings.'
<em>Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing</em>. 66-76. <a href="https://doi.org/10.18653/v1/D19-1007">doi:10.18653/v1/D19-1007</a>
Borg and Groenen. (1997). <em>Modern Multidimensional Scaling</em>.
New York: Springer. 340-342
</p>


</div>