<div class="container">

<table style="width: 100%;"><tr>
<td>nn_cross_entropy_loss</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>CrossEntropyLoss module</h2>

<h3>Description</h3>

<p>This criterion combines <code>nn_log_softmax()</code> and <code>nn_nll_loss()</code> in one single class.
It is useful when training a classification problem with <code>C</code> classes.
</p>


<h3>Usage</h3>

<pre><code class="language-R">nn_cross_entropy_loss(weight = NULL, ignore_index = -100, reduction = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>weight</code></td>
<td>
<p>(Tensor, optional): a manual rescaling weight given to each class.
If given, has to be a Tensor of size <code>C</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ignore_index</code></td>
<td>
<p>(int, optional): Specifies a target value that is ignored
and does not contribute to the input gradient. When <code>size_average</code> is
<code>TRUE</code>, the loss is averaged over non-ignored targets.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reduction</code></td>
<td>
<p>(string, optional): Specifies the reduction to apply to the output:
<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied,
<code>'mean'</code>: the sum of the output will be divided by the number of
elements in the output, <code>'sum'</code>: the output will be summed.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>If provided, the optional argument <code>weight</code> should be a 1D <code>Tensor</code>
assigning weight to each of the classes.
</p>
<p>This is particularly useful when you have an unbalanced training set.
The <code>input</code> is expected to contain raw, unnormalized scores for each class.
<code>input</code> has to be a Tensor of size either <code class="reqn">(minibatch, C)</code> or
<code class="reqn">(minibatch, C, d_1, d_2, ..., d_K)</code>
with <code class="reqn">K \geq 1</code> for the <code>K</code>-dimensional case (described later).
</p>
<p>This criterion expects a class index in the range <code class="reqn">[0, C-1]</code> as the
<code>target</code> for each value of a 1D tensor of size <code>minibatch</code>; if <code>ignore_index</code>
is specified, this criterion also accepts this class index (this index may not
necessarily be in the class range).
</p>
<p>The loss can be described as:
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)
= -x[class] + \log\left(\sum_j \exp(x[j])\right)
</code>
</p>

<p>or in the case of the <code>weight</code> argument being specified:
</p>
<p style="text-align: center;"><code class="reqn">
  \mbox{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)
</code>
</p>

<p>The losses are averaged across observations for each minibatch.
Can also be used for higher dimension inputs, such as 2D images, by providing
an input of size <code class="reqn">(minibatch, C, d_1, d_2, ..., d_K)</code> with <code class="reqn">K \geq 1</code>,
where <code class="reqn">K</code> is the number of dimensions, and a target of appropriate shape
(see below).
</p>


<h3>Shape</h3>


<ul>
<li>
<p> Input: <code class="reqn">(N, C)</code> where <code style="white-space: pre;">⁠C = number of classes⁠</code>, or
<code class="reqn">(N, C, d_1, d_2, ..., d_K)</code> with <code class="reqn">K \geq 1</code>
in the case of <code>K</code>-dimensional loss.
</p>
</li>
<li>
<p> Target: <code class="reqn">(N)</code> where each value is <code class="reqn">0 \leq \mbox{targets}[i] \leq C-1</code>, or
<code class="reqn">(N, d_1, d_2, ..., d_K)</code> with <code class="reqn">K \geq 1</code> in the case of
K-dimensional loss.
</p>
</li>
<li>
<p> Output: scalar.
If <code>reduction</code> is <code>'none'</code>, then the same size as the target:
<code class="reqn">(N)</code>, or
<code class="reqn">(N, d_1, d_2, ..., d_K)</code> with <code class="reqn">K \geq 1</code> in the case
of K-dimensional loss.
</p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R">if (torch_is_installed()) {
loss &lt;- nn_cross_entropy_loss()
input &lt;- torch_randn(3, 5, requires_grad = TRUE)
target &lt;- torch_randint(low = 1, high = 5, size = 3, dtype = torch_long())
output &lt;- loss(input, target)
output$backward()
}
</code></pre>


</div>