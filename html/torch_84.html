<div class="container">

<table style="width: 100%;"><tr>
<td>linalg_lstsq</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Computes a solution to the least squares problem of a system of linear equations.</h2>

<h3>Description</h3>

<p>Letting <code class="reqn">\mathbb{K}</code> be <code class="reqn">\mathbb{R}</code> or <code class="reqn">\mathbb{C}</code>,
the <strong>least squares problem</strong> for a linear system <code class="reqn">AX = B</code> with
<code class="reqn">A \in \mathbb{K}^{m \times n}, B \in \mathbb{K}^{m \times k}</code> is defined as
</p>


<h3>Usage</h3>

<pre><code class="language-R">linalg_lstsq(A, B, rcond = NULL, ..., driver = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>A</code></td>
<td>
<p>(Tensor): lhs tensor of shape <code style="white-space: pre;">⁠(*, m, n)⁠</code> where <code>*</code> is zero or more batch dimensions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>B</code></td>
<td>
<p>(Tensor): rhs tensor of shape <code style="white-space: pre;">⁠(*, m, k)⁠</code> where <code>*</code> is zero or more batch dimensions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rcond</code></td>
<td>
<p>(float, optional): used to determine the effective rank of <code>A</code>.
If <code>rcond = NULL</code>, <code>rcond</code> is set to the machine
precision of the dtype of <code>A</code> times <code>max(m, n)</code>. Default: <code>NULL</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>currently unused.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>driver</code></td>
<td>
<p>(str, optional): name of the LAPACK/MAGMA method to be used.
If <code>NULL</code>, <code>'gelsy'</code> is used for CPU inputs and <code>'gels'</code> for CUDA inputs.
Default: <code>NULL</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>


<p>Math could not be displayed. Please visit the package website.</p>

<p>where <code class="reqn">\|-\|_F</code> denotes the Frobenius norm.
Supports inputs of float, double, cfloat and cdouble dtypes.
</p>
<p>Also supports batches of matrices, and if the inputs are batches of matrices then
the output has the same batch dimensions.
<code>driver</code> chooses the LAPACK/MAGMA function that will be used.
</p>
<p>For CPU inputs the valid values are <code>'gels'</code>, <code>'gelsy'</code>, <code style="white-space: pre;">⁠'gelsd⁠</code>, <code>'gelss'</code>.
For CUDA input, the only valid driver is <code>'gels'</code>, which assumes that <code>A</code> is full-rank.
</p>
<p>To choose the best driver on CPU consider:
</p>

<ul>
<li>
<p> If <code>A</code> is well-conditioned (its <a href="https://pytorch.org/docs/master/linalg.html#torch.linalg.cond">condition number</a> is not too large), or you do not mind some precision loss.
</p>
</li>
<li>
<p> For a general matrix: <code>'gelsy'</code> (QR with pivoting) (default)
</p>
</li>
<li>
<p> If <code>A</code> is full-rank: <code>'gels'</code> (QR)
</p>
</li>
<li>
<p> If <code>A</code> is not well-conditioned.
</p>
</li>
<li> <p><code>'gelsd'</code> (tridiagonal reduction and SVD)
</p>
</li>
<li>
<p> But if you run into memory issues: <code>'gelss'</code> (full SVD).
</p>
</li>
</ul>
<p>See also the <a href="https://netlib.org/lapack/lug/node27.html">full description of these drivers</a>
</p>
<p><code>rcond</code> is used to determine the effective rank of the matrices in <code>A</code>
when <code>driver</code> is one of (<code>'gelsy'</code>, <code>'gelsd'</code>, <code>'gelss'</code>).
In this case, if <code class="reqn">\sigma_i</code> are the singular values of <code>A</code> in decreasing order,
<code class="reqn">\sigma_i</code> will be rounded down to zero if <code class="reqn">\sigma_i \leq rcond \cdot \sigma_1</code>.
If <code>rcond = NULL</code> (default), <code>rcond</code> is set to the machine precision of the dtype of <code>A</code>.
</p>
<p>This function returns the solution to the problem and some extra information in a list of
four tensors <code style="white-space: pre;">⁠(solution, residuals, rank, singular_values)⁠</code>. For inputs <code>A</code>, <code>B</code>
of shape <code style="white-space: pre;">⁠(*, m, n)⁠</code>, <code style="white-space: pre;">⁠(*, m, k)⁠</code> respectively, it cointains
</p>

<ul>
<li> <p><code>solution</code>: the least squares solution. It has shape <code style="white-space: pre;">⁠(*, n, k)⁠</code>.
</p>
</li>
<li> <p><code>residuals</code>: the squared residuals of the solutions, that is, <code class="reqn">\|AX - B\|_F^2</code>.
It has shape equal to the batch dimensions of <code>A</code>.
It is computed when <code>m &gt; n</code> and every matrix in <code>A</code> is full-rank,
otherwise, it is an empty tensor.
If <code>A</code> is a batch of matrices and any matrix in the batch is not full rank,
then an empty tensor is returned. This behavior may change in a future PyTorch release.
</p>
</li>
<li> <p><code>rank</code>: tensor of ranks of the matrices in <code>A</code>.
It has shape equal to the batch dimensions of <code>A</code>.
It is computed when <code>driver</code> is one of (<code>'gelsy'</code>, <code>'gelsd'</code>, <code>'gelss'</code>),
otherwise it is an empty tensor.
</p>
</li>
<li> <p><code>singular_values</code>: tensor of singular values of the matrices in <code>A</code>.
It has shape <code style="white-space: pre;">⁠(*, min(m, n))⁠</code>.
It is computed when <code>driver</code> is one of (<code>'gelsd'</code>, <code>'gelss'</code>),
otherwise it is an empty tensor.
</p>
</li>
</ul>
<h3>Value</h3>

<p>A list <code style="white-space: pre;">⁠(solution, residuals, rank, singular_values)⁠</code>.
</p>


<h3>Warning</h3>

<p>The default value of <code>rcond</code> may change in a future PyTorch release.
It is therefore recommended to use a fixed value to avoid potential
breaking changes.
</p>


<h3>Note</h3>

<p>This function computes <code>X = A$pinverse() %*% B</code> in a faster and
more numerically stable way than performing the computations separately.
</p>


<h3>See Also</h3>

<p>Other linalg: 
<code>linalg_cholesky_ex()</code>,
<code>linalg_cholesky()</code>,
<code>linalg_det()</code>,
<code>linalg_eigh()</code>,
<code>linalg_eigvalsh()</code>,
<code>linalg_eigvals()</code>,
<code>linalg_eig()</code>,
<code>linalg_householder_product()</code>,
<code>linalg_inv_ex()</code>,
<code>linalg_inv()</code>,
<code>linalg_matrix_norm()</code>,
<code>linalg_matrix_power()</code>,
<code>linalg_matrix_rank()</code>,
<code>linalg_multi_dot()</code>,
<code>linalg_norm()</code>,
<code>linalg_pinv()</code>,
<code>linalg_qr()</code>,
<code>linalg_slogdet()</code>,
<code>linalg_solve_triangular()</code>,
<code>linalg_solve()</code>,
<code>linalg_svdvals()</code>,
<code>linalg_svd()</code>,
<code>linalg_tensorinv()</code>,
<code>linalg_tensorsolve()</code>,
<code>linalg_vector_norm()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">if (torch_is_installed()) {
A &lt;- torch_tensor(rbind(c(10, 2, 3), c(3, 10, 5), c(5, 6, 12)))$unsqueeze(1) # shape (1, 3, 3)
B &lt;- torch_stack(list(
  rbind(c(2, 5, 1), c(3, 2, 1), c(5, 1, 9)),
  rbind(c(4, 2, 9), c(2, 0, 3), c(2, 5, 3))
), dim = 1) # shape (2, 3, 3)
X &lt;- linalg_lstsq(A, B)$solution # A is broadcasted to shape (2, 3, 3)
}
</code></pre>


</div>