<div class="container">

<table style="width: 100%;"><tr>
<td>activation_gelu</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Gelu</h2>

<h3>Description</h3>

<p>Gaussian Error Linear Unit.
</p>


<h3>Usage</h3>

<pre><code class="language-R">activation_gelu(x, approximate = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A 'Tensor'. Must be one of the following types: 'float16', 'float32', 'float64'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>approximate</code></td>
<td>
<p>bool, whether to enable approximation. Returns: A 'Tensor'. Has the same type as 'x'.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Computes gaussian error linear:
'0.5 * x * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x^3)))' or
'x * P(X &lt;= x) = 0.5 * x * (1 + erf(x / sqrt(2)))', where P(X) ~ N(0, 1),
depending on whether approximation is enabled.
See [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)
and [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805).
</p>


<h3>Value</h3>

<p>A 'Tensor'. Has the same type as 'x'.
</p>


<h3>Computes gaussian error linear</h3>

<p>'0.5 * x * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x^3)))' or 'x * P(X &lt;= x) = 0.5 * x * (1 + erf(x / sqrt(2)))',
where P(X) ~ N(0, 1), depending on whether approximation is enabled.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## Not run: 
library(keras)
library(tfaddons)
model = keras_model_sequential() %&gt;%
layer_conv_2d(filters = 10, kernel_size = c(3,3),input_shape = c(28,28,1),
              activation = activation_gelu)

## End(Not run)


</code></pre>


</div>