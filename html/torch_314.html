<div class="container">

<table style="width: 100%;"><tr>
<td>nn_rnn</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>RNN module</h2>

<h3>Description</h3>

<p>Applies a multi-layer Elman RNN with <code class="reqn">\tanh</code> or <code class="reqn">\mbox{ReLU}</code> non-linearity
to an input sequence.
</p>


<h3>Usage</h3>

<pre><code class="language-R">nn_rnn(
  input_size,
  hidden_size,
  num_layers = 1,
  nonlinearity = NULL,
  bias = TRUE,
  batch_first = FALSE,
  dropout = 0,
  bidirectional = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>input_size</code></td>
<td>
<p>The number of expected features in the input <code>x</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hidden_size</code></td>
<td>
<p>The number of features in the hidden state <code>h</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_layers</code></td>
<td>
<p>Number of recurrent layers. E.g., setting <code>num_layers=2</code>
would mean stacking two RNNs together to form a <code style="white-space: pre;">⁠stacked RNN⁠</code>,
with the second RNN taking in outputs of the first RNN and
computing the final results. Default: 1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nonlinearity</code></td>
<td>
<p>The non-linearity to use. Can be either <code>'tanh'</code> or
<code>'relu'</code>. Default: <code>'tanh'</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias</code></td>
<td>
<p>If <code>FALSE</code>, then the layer does not use bias weights <code>b_ih</code> and
<code>b_hh</code>. Default: <code>TRUE</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch_first</code></td>
<td>
<p>If <code>TRUE</code>, then the input and output tensors are provided
as <code style="white-space: pre;">⁠(batch, seq, feature)⁠</code>. Default: <code>FALSE</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dropout</code></td>
<td>
<p>If non-zero, introduces a <code>Dropout</code> layer on the outputs of each
RNN layer except the last layer, with dropout probability equal to
<code>dropout</code>. Default: 0</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bidirectional</code></td>
<td>
<p>If <code>TRUE</code>, becomes a bidirectional RNN. Default: <code>FALSE</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>other arguments that can be passed to the super class.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For each element in the input sequence, each layer computes the following
function:
</p>
<p style="text-align: center;"><code class="reqn">
h_t = \tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})
</code>
</p>

<p>where <code class="reqn">h_t</code> is the hidden state at time <code>t</code>, <code class="reqn">x_t</code> is
the input at time <code>t</code>, and <code class="reqn">h_{(t-1)}</code> is the hidden state of the
previous layer at time <code>t-1</code> or the initial hidden state at time <code>0</code>.
If <code>nonlinearity</code> is <code>'relu'</code>, then <code class="reqn">\mbox{ReLU}</code> is used instead of
<code class="reqn">\tanh</code>.
</p>


<h3>Inputs</h3>


<ul>
<li> <p><strong>input</strong> of shape <code style="white-space: pre;">⁠(seq_len, batch, input_size)⁠</code>: tensor containing the features
of the input sequence. The input can also be a packed variable length
sequence.
</p>
</li>
<li> <p><strong>h_0</strong> of shape <code style="white-space: pre;">⁠(num_layers * num_directions, batch, hidden_size)⁠</code>: tensor
containing the initial hidden state for each element in the batch.
Defaults to zero if not provided. If the RNN is bidirectional,
num_directions should be 2, else it should be 1.
</p>
</li>
</ul>
<h3>Outputs</h3>


<ul>
<li> <p><strong>output</strong> of shape <code style="white-space: pre;">⁠(seq_len, batch, num_directions * hidden_size)⁠</code>: tensor
containing the output features (<code>h_t</code>) from the last layer of the RNN,
for each <code>t</code>.  If a :class:<code>nn_packed_sequence</code> has
been given as the input, the output will also be a packed sequence.
For the unpacked case, the directions can be separated
using <code>output$view(seq_len, batch, num_directions, hidden_size)</code>,
with forward and backward being direction <code>0</code> and <code>1</code> respectively.
Similarly, the directions can be separated in the packed case.
</p>
</li>
<li> <p><strong>h_n</strong> of shape <code style="white-space: pre;">⁠(num_layers * num_directions, batch, hidden_size)⁠</code>: tensor
containing the hidden state for <code>t = seq_len</code>.
Like <em>output</em>, the layers can be separated using
<code>h_n$view(num_layers, num_directions, batch, hidden_size)</code>.
</p>
</li>
</ul>
<h3>Shape</h3>


<ul>
<li>
<p> Input1: <code class="reqn">(L, N, H_{in})</code> tensor containing input features where
<code class="reqn">H_{in}=\mbox{input\_size}</code> and <code>L</code> represents a sequence length.
</p>
</li>
<li>
<p> Input2: <code class="reqn">(S, N, H_{out})</code> tensor
containing the initial hidden state for each element in the batch.
<code class="reqn">H_{out}=\mbox{hidden\_size}</code>
Defaults to zero if not provided. where <code class="reqn">S=\mbox{num\_layers} * \mbox{num\_directions}</code>
If the RNN is bidirectional, num_directions should be 2, else it should be 1.
</p>
</li>
<li>
<p> Output1: <code class="reqn">(L, N, H_{all})</code> where <code class="reqn">H_{all}=\mbox{num\_directions} * \mbox{hidden\_size}</code>
</p>
</li>
<li>
<p> Output2: <code class="reqn">(S, N, H_{out})</code> tensor containing the next hidden state
for each element in the batch
</p>
</li>
</ul>
<h3>Attributes</h3>


<ul>
<li> <p><code>weight_ih_l[k]</code>: the learnable input-hidden weights of the k-th layer,
of shape <code style="white-space: pre;">⁠(hidden_size, input_size)⁠</code> for <code>k = 0</code>. Otherwise, the shape is
<code style="white-space: pre;">⁠(hidden_size, num_directions * hidden_size)⁠</code>
</p>
</li>
<li> <p><code>weight_hh_l[k]</code>: the learnable hidden-hidden weights of the k-th layer,
of shape <code style="white-space: pre;">⁠(hidden_size, hidden_size)⁠</code>
</p>
</li>
<li> <p><code>bias_ih_l[k]</code>: the learnable input-hidden bias of the k-th layer,
of shape <code>(hidden_size)</code>
</p>
</li>
<li> <p><code>bias_hh_l[k]</code>: the learnable hidden-hidden bias of the k-th layer,
of shape <code>(hidden_size)</code>
</p>
</li>
</ul>
<h3>Note</h3>

<p>All the weights and biases are initialized from <code class="reqn">\mathcal{U}(-\sqrt{k}, \sqrt{k})</code>
where <code class="reqn">k = \frac{1}{\mbox{hidden\_size}}</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">if (torch_is_installed()) {
rnn &lt;- nn_rnn(10, 20, 2)
input &lt;- torch_randn(5, 3, 10)
h0 &lt;- torch_randn(2, 3, 20)
rnn(input, h0)
}
</code></pre>


</div>