<div class="container">

<table style="width: 100%;"><tr>
<td>layer_norm_lstm_cell</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>LSTM cell with layer normalization and recurrent dropout.</h2>

<h3>Description</h3>

<p>LSTM cell with layer normalization and recurrent dropout.
</p>


<h3>Usage</h3>

<pre><code class="language-R">layer_norm_lstm_cell(
  object,
  units,
  activation = "tanh",
  recurrent_activation = "sigmoid",
  use_bias = TRUE,
  kernel_initializer = "glorot_uniform",
  recurrent_initializer = "orthogonal",
  bias_initializer = "zeros",
  unit_forget_bias = TRUE,
  kernel_regularizer = NULL,
  recurrent_regularizer = NULL,
  bias_regularizer = NULL,
  kernel_constraint = NULL,
  recurrent_constraint = NULL,
  bias_constraint = NULL,
  dropout = 0,
  recurrent_dropout = 0,
  norm_gamma_initializer = "ones",
  norm_beta_initializer = "zeros",
  norm_epsilon = 0.001,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>Model or layer object</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>units</code></td>
<td>
<p>Positive integer, dimensionality of the output space.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>activation</code></td>
<td>
<p>Activation function to use. Default: hyperbolic tangent ('tanh'). If
you pass 'NULL', no activation is applied (ie. "linear" activation: 'a(x) = x').</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>recurrent_activation</code></td>
<td>
<p>Activation function to use for the recurrent step.
Default: sigmoid ('sigmoid'). If you pass 'NULL', no activation is applied
(ie. "linear" activation: 'a(x) = x').</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>use_bias</code></td>
<td>
<p>Boolean, whether the layer uses a bias vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_initializer</code></td>
<td>
<p>Initializer for the 'kernel' weights matrix, used for the
linear transformation of the inputs.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>recurrent_initializer</code></td>
<td>
<p>Initializer for the 'recurrent_kernel' weights matrix,
used for the linear transformation of the recurrent state.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias_initializer</code></td>
<td>
<p>Initializer for the bias vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unit_forget_bias</code></td>
<td>
<p>Boolean. If True, add 1 to the bias of the forget gate at initialization.
Setting it to true will also force 'bias_initializer="zeros"'. This is
recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_regularizer</code></td>
<td>
<p>Regularizer function applied to the 'kernel' weights matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>recurrent_regularizer</code></td>
<td>
<p>Regularizer function applied to the 'recurrent_kernel' weights matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias_regularizer</code></td>
<td>
<p>Regularizer function applied to the bias vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kernel_constraint</code></td>
<td>
<p>Constraint function applied to the 'kernel' weights matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>recurrent_constraint</code></td>
<td>
<p>Constraint function applied to the 'recurrent_kernel' weights matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bias_constraint</code></td>
<td>
<p>Constraint function applied to the bias vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dropout</code></td>
<td>
<p>Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>recurrent_dropout</code></td>
<td>
<p>Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>norm_gamma_initializer</code></td>
<td>
<p>Initializer for the layer normalization gain initial value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>norm_beta_initializer</code></td>
<td>
<p>Initializer for the layer normalization shift initial value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>norm_epsilon</code></td>
<td>
<p>Float, the epsilon value for normalization layers.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>List, the other keyword arguments for layer creation.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This class adds layer normalization and recurrent dropout to a LSTM unit. Layer
normalization implementation is based on: https://arxiv.org/abs/1607.06450.
"Layer Normalization" Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton and is
applied before the internal nonlinearities.
Recurrent dropout is based on: https://arxiv.org/abs/1603.05118
"Recurrent Dropout without Memory Loss" Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth.
</p>


<h3>Value</h3>

<p>A tensor
</p>


</div>