<div class="container">

<table style="width: 100%;"><tr>
<td>next_batch</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Tensor(s) for retrieving the next batch from a dataset</h2>

<h3>Description</h3>

<p>Tensor(s) for retrieving the next batch from a dataset
</p>


<h3>Usage</h3>

<pre><code class="language-R">next_batch(dataset)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>dataset</code></td>
<td>
<p>A dataset</p>
</td>
</tr></table>
<h3>Details</h3>

<p>To access the underlying data within the dataset you iteratively evaluate the
tensor(s) to read batches of data.
</p>
<p>Note that in many cases you won't need to explicitly evaluate the tensors.
Rather, you will pass the tensors to another function that will perform
the evaluation (e.g. the Keras layer_input() and
compile() functions).
</p>
<p>If you do need to perform iteration manually by evaluating the tensors, there
are a couple of possible approaches to controlling/detecting when iteration should
end.
</p>
<p>One approach is to create a dataset that yields batches infinitely (traversing
the dataset multiple times with different batches randomly drawn). In this case you'd
use another mechanism like a global step counter or detecting a learning plateau.
</p>
<p>Another approach is to detect when all batches have been yielded
from the dataset. When the tensor reaches the end of iteration a runtime
error will occur. You can catch and ignore the error when it occurs by wrapping
your iteration code in the <code>with_dataset()</code> function.
</p>
<p>See the examples below for a demonstration of each of these methods of iteration.
</p>


<h3>Value</h3>

<p>Tensor(s) that can be evaluated to yield the next batch of training data.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 

# iteration with 'infinite' dataset and explicit step counter

library(tfdatasets)
dataset &lt;- text_line_dataset("mtcars.csv", record_spec = mtcars_spec) %&gt;%
  dataset_prepare(x = c(mpg, disp), y = cyl) %&gt;%
  dataset_shuffle(5000) %&gt;%
  dataset_batch(128) %&gt;%
  dataset_repeat() # repeat infinitely
batch &lt;- next_batch(dataset)
steps &lt;- 200
for (i in 1:steps) {
  # use batch$x and batch$y tensors
}

# iteration that detects and ignores end of iteration error

library(tfdatasets)
dataset &lt;- text_line_dataset("mtcars.csv", record_spec = mtcars_spec) %&gt;%
  dataset_prepare(x = c(mpg, disp), y = cyl) %&gt;%
  dataset_batch(128) %&gt;%
  dataset_repeat(10)
batch &lt;- next_batch(dataset)
with_dataset({
  while(TRUE) {
    # use batch$x and batch$y tensors
  }
})

## End(Not run)

</code></pre>


</div>